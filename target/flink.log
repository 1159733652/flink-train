2022-06-28 14:30:45,787 WARN  org.apache.flink.connector.kafka.sink.KafkaSinkBuilder       [] - Property [transaction.timeout.ms] not specified. Setting it to PT1H
2022-06-28 14:30:46,078 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:30:46,078 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:30:46,078 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:30:46,079 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 14:30:46,079 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 14:30:46,079 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2022-06-28 14:30:46,083 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Flink Mini Cluster
2022-06-28 14:30:46,444 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Metrics Registry
2022-06-28 14:30:46,485 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
2022-06-28 14:30:46,485 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting RPC Service(s)
2022-06-28 14:30:46,494 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 14:30:46,962 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 14:30:47,042 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink
2022-06-28 14:30:47,053 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 14:30:47,060 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 14:30:47,067 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink-metrics
2022-06-28 14:30:47,075 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2022-06-28 14:30:47,087 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting high-availability services
2022-06-28 14:30:47,096 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-581d57b4-416d-43c5-8b47-c0db47ba46cb
2022-06-28 14:30:47,099 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:65453 - max concurrent requests: 50 - max backlog: 1000
2022-06-28 14:30:47,102 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-ce19da51-9a67-49f5-82f7-864ff7aabe73
2022-06-28 14:30:47,103 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-5ec17c73-7d90-438f-813d-7280d567e86d
2022-06-28 14:30:47,103 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting 1 TaskManager(s)
2022-06-28 14:30:47,106 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6
2022-06-28 14:30:47,114 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory 'C:\Users\BONC\AppData\Local\Temp': total 237 GB, usable 176 GB (74.26% usable)
2022-06-28 14:30:47,117 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-io-d9f6cdff-2d1c-48cb-94a0-8c55cd0ad80e
2022-06-28 14:30:47,123 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-netty-shuffle-fa009cc3-284e-4600-8df8-84d5c59ff4c8
2022-06-28 14:30:47,148 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2022-06-28 14:30:47,156 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
2022-06-28 14:30:47,157 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
2022-06-28 14:30:47,166 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'akka.ask.timeout' instead of key 'taskmanager.slot.timeout'
2022-06-28 14:30:47,176 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2022-06-28 14:30:47,186 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
2022-06-28 14:30:47,187 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory C:\Users\BONC\AppData\Local\Temp\flink-dist-cache-3a21691b-24b1-4550-bd15-afde5f99049a
2022-06-28 14:30:47,220 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Starting rest endpoint.
2022-06-28 14:30:47,222 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
2022-06-28 14:30:47,277 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Log file environment variable 'log.file' is not set.
2022-06-28 14:30:47,277 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
2022-06-28 14:30:47,506 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Rest endpoint listening at localhost:65504
2022-06-28 14:30:47,507 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender http://localhost:65504
2022-06-28 14:30:47,508 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - http://localhost:65504 was granted leadership with leaderSessionID=b0a16caa-d20d-426e-ab9e-5a1a91b1f4c3
2022-06-28 14:30:47,509 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader http://localhost:65504 , session=b0a16caa-d20d-426e-ab9e-5a1a91b1f4c3
2022-06-28 14:30:47,517 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2022-06-28 14:30:47,517 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
2022-06-28 14:30:47,517 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: ResourceManagerServiceImpl
2022-06-28 14:30:47,517 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id c65d9d50-a640-434d-bfba-e7598f28f1cd. Creating new DispatcherLeaderProcess.
2022-06-28 14:30:47,518 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 264e1c4e-d3f3-4280-9b84-8f659a28c4d4.
2022-06-28 14:30:47,520 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Flink Mini Cluster started successfully
2022-06-28 14:30:47,522 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
2022-06-28 14:30:47,524 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.
2022-06-28 14:30:47,524 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
2022-06-28 14:30:47,530 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
2022-06-28 14:30:47,534 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_2 .
2022-06-28 14:30:47,537 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_1 , session=c65d9d50-a640-434d-bfba-e7598f28f1cd
2022-06-28 14:30:47,539 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
2022-06-28 14:30:47,547 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_2 , session=264e1c4e-d3f3-4280-9b84-8f659a28c4d4
2022-06-28 14:30:47,548 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(9b848f659a28c4d4264e1c4ed3f34280).
2022-06-28 14:30:47,561 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
2022-06-28 14:30:47,565 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2022-06-28 14:30:47,567 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_2 under registration id 81b20da600012845fa7440bf6105dae2.
2022-06-28 14:30:47,567 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:47,568 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:47,579 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2022-06-28 14:30:47,589 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2022-06-28 14:30:47,595 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:47,610 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for insert-into_default_catalog.default_database.t_kafka_nicekcnt (ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:47,642 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job insert-into_default_catalog.default_database.t_kafka_nicekcnt (ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:47,643 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
2022-06-28 14:30:47,691 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 0 ms
2022-06-28 14:30:47,713 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@306c3896
2022-06-28 14:30:47,713 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,714 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,728 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
2022-06-28 14:30:47,734 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@44ffb9cf for insert-into_default_catalog.default_database.t_kafka_nicekcnt (ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:47,743 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=edbcadd8-452d-42c1-bd26-04517084adfb
2022-06-28 14:30:47,746 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (ac07f2553170acd984d5562405c528aa) under job master id bd2604517084adfbedbcadd8452d42c1.
2022-06-28 14:30:47,748 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise.
2022-06-28 14:30:47,750 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2022-06-28 14:30:47,751 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (ac07f2553170acd984d5562405c528aa) switched from state CREATED to RUNNING.
2022-06-28 14:30:47,754 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (46ccf8ef24e971b1c6ec7bb39cd57109) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,754 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (8b486b1680ef46d496ccb042674767c8) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,754 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (a0d87a6198b30a466a9048d12caef989) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,754 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (d785e0124113f661df296536a625fc19) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,754 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (229ba7a0bfb43739af8be73d8ae4e536) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,754 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (1408b5bd627cff5a1073fb3f31be5063) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,754 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (052f9a4f586b62d0e83c230659408f89) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (b4947ae341e8fb4f7c43819fc4e2c556) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (72861e0816fbf58cc3ce68dfcfcbc142) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (70992881caca649ee10da257c923b2d7) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (93556fa71c22a2877eee85431c15109b) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (06f98a210d800ba69278fee9949002a5) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (b0e028f1c2db7d70045b3d4792416272) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (7bf37cb80e788c5f57b2dfef985963f8) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (429b2b31a37ba529c9bdd32dedeb3bfc) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (3455990209248d35741b396d657057a0) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (40f58bb4774bf64a42e706d6a78ef93f) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (cb8967fe237a1989878133425eed4e7d) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (ab819048d0d78d9d327ba6498dd75399) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,755 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (17ffa76fbb98cc30493841b9ec0f1804) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,756 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (4790bbc5fd981aa0a072ae2f00843837) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,756 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (62309e5d04d8c9f2ca87cdff6928a0c1) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,756 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (6c94ea8490ea02062906fa5667e5ee9f) switched from CREATED to SCHEDULED.
2022-06-28 14:30:47,771 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(9b848f659a28c4d4264e1c4ed3f34280)
2022-06-28 14:30:47,773 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
2022-06-28 14:30:47,775 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager bd2604517084adfbedbcadd8452d42c1@akka://flink/user/rpc/jobmanager_3 for job ac07f2553170acd984d5562405c528aa.
2022-06-28 14:30:47,775 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 14:30:47,780 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager bd2604517084adfbedbcadd8452d42c1@akka://flink/user/rpc/jobmanager_3 for job ac07f2553170acd984d5562405c528aa.
2022-06-28 14:30:47,781 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,782 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=12}]
2022-06-28 14:30:47,785 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 16a671e2ac06b189dedcac85ec074bbf for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,790 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 16a671e2ac06b189dedcac85ec074bbf.
2022-06-28 14:30:47,791 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job ac07f2553170acd984d5562405c528aa for job leader monitoring.
2022-06-28 14:30:47,794 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 29c0bf965a70238c6a6ef8c59d3781f8 for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,793 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id edbcadd8-452d-42c1-bd26-04517084adfb.
2022-06-28 14:30:47,794 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 29c0bf965a70238c6a6ef8c59d3781f8.
2022-06-28 14:30:47,794 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request d6b116f0f895db5064416220921b974c for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,794 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for d6b116f0f895db5064416220921b974c.
2022-06-28 14:30:47,794 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 3edbbe1df6dd9b871df3d92dad6edcda for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,795 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 3edbbe1df6dd9b871df3d92dad6edcda.
2022-06-28 14:30:47,795 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2022-06-28 14:30:47,795 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request ce9dd4ec981663547c4ffd641e2ac5e4 for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,795 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for ce9dd4ec981663547c4ffd641e2ac5e4.
2022-06-28 14:30:47,795 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request c799e2d2ff587df9bfbb84bc448a609e for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,795 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for c799e2d2ff587df9bfbb84bc448a609e.
2022-06-28 14:30:47,795 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 1d30c134a0aa59d55476f993818c0848 for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,795 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 1d30c134a0aa59d55476f993818c0848.
2022-06-28 14:30:47,796 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request d72865e645a75a8c8e6ff2d60713a47e for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,796 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for d72865e645a75a8c8e6ff2d60713a47e.
2022-06-28 14:30:47,796 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request ae9651ed91851a87873c2a594e2109ce for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,796 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for ae9651ed91851a87873c2a594e2109ce.
2022-06-28 14:30:47,796 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 9e3ba88ce4a6cf9f5e45158683b34f8b for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,796 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 9e3ba88ce4a6cf9f5e45158683b34f8b.
2022-06-28 14:30:47,796 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 75dd327f26b5c7142181e43f6d2b2ee0 for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,797 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 75dd327f26b5c7142181e43f6d2b2ee0.
2022-06-28 14:30:47,797 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 7801d18002312cc3129809fab78bbf27 for job ac07f2553170acd984d5562405c528aa from resource manager with leader id 9b848f659a28c4d4264e1c4ed3f34280.
2022-06-28 14:30:47,797 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 7801d18002312cc3129809fab78bbf27.
2022-06-28 14:30:47,798 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job ac07f2553170acd984d5562405c528aa.
2022-06-28 14:30:47,799 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job ac07f2553170acd984d5562405c528aa.
2022-06-28 14:30:47,801 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job ac07f2553170acd984d5562405c528aa.
2022-06-28 14:30:47,808 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (46ccf8ef24e971b1c6ec7bb39cd57109) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,809 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (attempt #0) with attempt id 46ccf8ef24e971b1c6ec7bb39cd57109 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 75dd327f26b5c7142181e43f6d2b2ee0
2022-06-28 14:30:47,812 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (8b486b1680ef46d496ccb042674767c8) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,813 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (attempt #0) with attempt id 8b486b1680ef46d496ccb042674767c8 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 16a671e2ac06b189dedcac85ec074bbf
2022-06-28 14:30:47,813 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 75dd327f26b5c7142181e43f6d2b2ee0.
2022-06-28 14:30:47,813 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (a0d87a6198b30a466a9048d12caef989) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,813 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (attempt #0) with attempt id a0d87a6198b30a466a9048d12caef989 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 1d30c134a0aa59d55476f993818c0848
2022-06-28 14:30:47,813 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (d785e0124113f661df296536a625fc19) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,813 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (attempt #0) with attempt id d785e0124113f661df296536a625fc19 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id ce9dd4ec981663547c4ffd641e2ac5e4
2022-06-28 14:30:47,813 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (229ba7a0bfb43739af8be73d8ae4e536) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,814 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (attempt #0) with attempt id 229ba7a0bfb43739af8be73d8ae4e536 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 7801d18002312cc3129809fab78bbf27
2022-06-28 14:30:47,814 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (1408b5bd627cff5a1073fb3f31be5063) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,814 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (attempt #0) with attempt id 1408b5bd627cff5a1073fb3f31be5063 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id ae9651ed91851a87873c2a594e2109ce
2022-06-28 14:30:47,814 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,814 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (attempt #0) with attempt id cfa43b92ce02b3b32d7a0708ad6c8dfc to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 3edbbe1df6dd9b871df3d92dad6edcda
2022-06-28 14:30:47,814 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (052f9a4f586b62d0e83c230659408f89) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,814 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (attempt #0) with attempt id 052f9a4f586b62d0e83c230659408f89 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id d6b116f0f895db5064416220921b974c
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (b4947ae341e8fb4f7c43819fc4e2c556) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (attempt #0) with attempt id b4947ae341e8fb4f7c43819fc4e2c556 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id d72865e645a75a8c8e6ff2d60713a47e
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (72861e0816fbf58cc3ce68dfcfcbc142) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (attempt #0) with attempt id 72861e0816fbf58cc3ce68dfcfcbc142 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 29c0bf965a70238c6a6ef8c59d3781f8
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (70992881caca649ee10da257c923b2d7) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (attempt #0) with attempt id 70992881caca649ee10da257c923b2d7 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 9e3ba88ce4a6cf9f5e45158683b34f8b
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (93556fa71c22a2877eee85431c15109b) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (attempt #0) with attempt id 93556fa71c22a2877eee85431c15109b to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id c799e2d2ff587df9bfbb84bc448a609e
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (06f98a210d800ba69278fee9949002a5) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,815 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (attempt #0) with attempt id 06f98a210d800ba69278fee9949002a5 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 75dd327f26b5c7142181e43f6d2b2ee0
2022-06-28 14:30:47,820 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (b0e028f1c2db7d70045b3d4792416272) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,820 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (attempt #0) with attempt id b0e028f1c2db7d70045b3d4792416272 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 16a671e2ac06b189dedcac85ec074bbf
2022-06-28 14:30:47,820 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (7bf37cb80e788c5f57b2dfef985963f8) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,820 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (attempt #0) with attempt id 7bf37cb80e788c5f57b2dfef985963f8 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 1d30c134a0aa59d55476f993818c0848
2022-06-28 14:30:47,820 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (429b2b31a37ba529c9bdd32dedeb3bfc) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,820 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (attempt #0) with attempt id 429b2b31a37ba529c9bdd32dedeb3bfc to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id ce9dd4ec981663547c4ffd641e2ac5e4
2022-06-28 14:30:47,820 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (3455990209248d35741b396d657057a0) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,820 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (attempt #0) with attempt id 3455990209248d35741b396d657057a0 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 7801d18002312cc3129809fab78bbf27
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (40f58bb4774bf64a42e706d6a78ef93f) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (attempt #0) with attempt id 40f58bb4774bf64a42e706d6a78ef93f to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id ae9651ed91851a87873c2a594e2109ce
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (cb8967fe237a1989878133425eed4e7d) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (attempt #0) with attempt id cb8967fe237a1989878133425eed4e7d to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 3edbbe1df6dd9b871df3d92dad6edcda
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (ab819048d0d78d9d327ba6498dd75399) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (attempt #0) with attempt id ab819048d0d78d9d327ba6498dd75399 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id d6b116f0f895db5064416220921b974c
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (17ffa76fbb98cc30493841b9ec0f1804) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (attempt #0) with attempt id 17ffa76fbb98cc30493841b9ec0f1804 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id d72865e645a75a8c8e6ff2d60713a47e
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (4790bbc5fd981aa0a072ae2f00843837) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,821 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (attempt #0) with attempt id 4790bbc5fd981aa0a072ae2f00843837 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 29c0bf965a70238c6a6ef8c59d3781f8
2022-06-28 14:30:47,822 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (62309e5d04d8c9f2ca87cdff6928a0c1) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,822 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (attempt #0) with attempt id 62309e5d04d8c9f2ca87cdff6928a0c1 to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id 9e3ba88ce4a6cf9f5e45158683b34f8b
2022-06-28 14:30:47,822 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (6c94ea8490ea02062906fa5667e5ee9f) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:30:47,822 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (attempt #0) with attempt id 6c94ea8490ea02062906fa5667e5ee9f to 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1) with allocation id c799e2d2ff587df9bfbb84bc448a609e
2022-06-28 14:30:47,823 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory}.
2022-06-28 14:30:47,823 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2022-06-28 14:30:47,837 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:30:47,837 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:30:47,838 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109), deploy into slot with allocation id 75dd327f26b5c7142181e43f6d2b2ee0.
2022-06-28 14:30:47,838 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:47,838 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:47,838 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397847837
2022-06-28 14:30:47,838 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,840 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 16a671e2ac06b189dedcac85ec074bbf.
2022-06-28 14:30:47,840 INFO  org.apache.kafka.clients.admin.AdminClientConfig             [] - AdminClientConfig values: 
	bootstrap.servers = [192.168.136.130:9092]
	client.dns.lookup = default
	client.id = g5-enumerator-admin-client
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-06-28 14:30:47,843 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8), deploy into slot with allocation id 16a671e2ac06b189dedcac85ec074bbf.
2022-06-28 14:30:47,843 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109) [DEPLOYING].
2022-06-28 14:30:47,844 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1d30c134a0aa59d55476f993818c0848.
2022-06-28 14:30:47,844 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,844 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8) [DEPLOYING].
2022-06-28 14:30:47,846 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989), deploy into slot with allocation id 1d30c134a0aa59d55476f993818c0848.
2022-06-28 14:30:47,847 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce9dd4ec981663547c4ffd641e2ac5e4.
2022-06-28 14:30:47,847 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,847 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989) [DEPLOYING].
2022-06-28 14:30:47,849 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19), deploy into slot with allocation id ce9dd4ec981663547c4ffd641e2ac5e4.
2022-06-28 14:30:47,850 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 7801d18002312cc3129809fab78bbf27.
2022-06-28 14:30:47,850 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,850 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19) [DEPLOYING].
2022-06-28 14:30:47,851 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536), deploy into slot with allocation id 7801d18002312cc3129809fab78bbf27.
2022-06-28 14:30:47,852 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ae9651ed91851a87873c2a594e2109ce.
2022-06-28 14:30:47,852 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,852 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536) [DEPLOYING].
2022-06-28 14:30:47,854 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063), deploy into slot with allocation id ae9651ed91851a87873c2a594e2109ce.
2022-06-28 14:30:47,855 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3edbbe1df6dd9b871df3d92dad6edcda.
2022-06-28 14:30:47,855 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,855 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063) [DEPLOYING].
2022-06-28 14:30:47,857 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc), deploy into slot with allocation id 3edbbe1df6dd9b871df3d92dad6edcda.
2022-06-28 14:30:47,857 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d6b116f0f895db5064416220921b974c.
2022-06-28 14:30:47,857 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,857 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc) [DEPLOYING].
2022-06-28 14:30:47,859 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89), deploy into slot with allocation id d6b116f0f895db5064416220921b974c.
2022-06-28 14:30:47,860 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d72865e645a75a8c8e6ff2d60713a47e.
2022-06-28 14:30:47,859 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,860 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89) [DEPLOYING].
2022-06-28 14:30:47,863 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556), deploy into slot with allocation id d72865e645a75a8c8e6ff2d60713a47e.
2022-06-28 14:30:47,863 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 29c0bf965a70238c6a6ef8c59d3781f8.
2022-06-28 14:30:47,863 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,864 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556) [DEPLOYING].
2022-06-28 14:30:47,865 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142), deploy into slot with allocation id 29c0bf965a70238c6a6ef8c59d3781f8.
2022-06-28 14:30:47,866 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 9e3ba88ce4a6cf9f5e45158683b34f8b.
2022-06-28 14:30:47,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,869 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142) [DEPLOYING].
2022-06-28 14:30:47,869 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7), deploy into slot with allocation id 9e3ba88ce4a6cf9f5e45158683b34f8b.
2022-06-28 14:30:47,869 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'key.deserializer' was supplied but isn't a known config.
2022-06-28 14:30:47,869 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot c799e2d2ff587df9bfbb84bc448a609e.
2022-06-28 14:30:47,869 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'value.deserializer' was supplied but isn't a known config.
2022-06-28 14:30:47,870 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'enable.auto.commit' was supplied but isn't a known config.
2022-06-28 14:30:47,870 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'group.id' was supplied but isn't a known config.
2022-06-28 14:30:47,870 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:30:47,869 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,870 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:30:47,870 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7) [DEPLOYING].
2022-06-28 14:30:47,870 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2022-06-28 14:30:47,871 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:47,871 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:47,871 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397847871
2022-06-28 14:30:47,871 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Starting the KafkaSourceEnumerator for consumer group g5 without periodic partition discovery.
2022-06-28 14:30:47,873 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b), deploy into slot with allocation id c799e2d2ff587df9bfbb84bc448a609e.
2022-06-28 14:30:47,873 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,873 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 75dd327f26b5c7142181e43f6d2b2ee0.
2022-06-28 14:30:47,874 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b) [DEPLOYING].
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@203d4483
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4395dd33
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2cb572dd
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4ef7a779
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@431a0493
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6f055fb3
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@31810fe5
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@f17ea1
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@776822d9
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@580af2b7
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6420b939
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@56fd8482
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,879 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,886 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (8b486b1680ef46d496ccb042674767c8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,887 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5), deploy into slot with allocation id 75dd327f26b5c7142181e43f6d2b2ee0.
2022-06-28 14:30:47,887 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 16a671e2ac06b189dedcac85ec074bbf.
2022-06-28 14:30:47,887 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,887 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5) [DEPLOYING].
2022-06-28 14:30:47,888 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (a0d87a6198b30a466a9048d12caef989) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,889 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (70992881caca649ee10da257c923b2d7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,889 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (b4947ae341e8fb4f7c43819fc4e2c556) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,889 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (d785e0124113f661df296536a625fc19) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,889 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@379f9899
2022-06-28 14:30:47,889 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,889 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (72861e0816fbf58cc3ce68dfcfcbc142) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,889 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,890 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (46ccf8ef24e971b1c6ec7bb39cd57109) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,890 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (229ba7a0bfb43739af8be73d8ae4e536) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,890 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,890 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (052f9a4f586b62d0e83c230659408f89) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,890 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272), deploy into slot with allocation id 16a671e2ac06b189dedcac85ec074bbf.
2022-06-28 14:30:47,890 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (1408b5bd627cff5a1073fb3f31be5063) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,890 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,890 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1d30c134a0aa59d55476f993818c0848.
2022-06-28 14:30:47,891 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (93556fa71c22a2877eee85431c15109b) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,891 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (06f98a210d800ba69278fee9949002a5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,893 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8), deploy into slot with allocation id 1d30c134a0aa59d55476f993818c0848.
2022-06-28 14:30:47,894 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,895 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce9dd4ec981663547c4ffd641e2ac5e4.
2022-06-28 14:30:47,895 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272) [DEPLOYING].
2022-06-28 14:30:47,897 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3c43bec2
2022-06-28 14:30:47,898 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,897 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,898 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,898 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc), deploy into slot with allocation id ce9dd4ec981663547c4ffd641e2ac5e4.
2022-06-28 14:30:47,898 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8) [DEPLOYING].
2022-06-28 14:30:47,898 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,898 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 7801d18002312cc3129809fab78bbf27.
2022-06-28 14:30:47,898 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (b0e028f1c2db7d70045b3d4792416272) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,899 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4e6bd661
2022-06-28 14:30:47,899 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,899 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,899 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,899 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc) [DEPLOYING].
2022-06-28 14:30:47,899 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,900 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (7bf37cb80e788c5f57b2dfef985963f8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,900 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@64dc378d
2022-06-28 14:30:47,900 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,900 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,900 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,901 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (429b2b31a37ba529c9bdd32dedeb3bfc) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,902 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0), deploy into slot with allocation id 7801d18002312cc3129809fab78bbf27.
2022-06-28 14:30:47,903 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ae9651ed91851a87873c2a594e2109ce.
2022-06-28 14:30:47,904 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,907 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f), deploy into slot with allocation id ae9651ed91851a87873c2a594e2109ce.
2022-06-28 14:30:47,907 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0) [DEPLOYING].
2022-06-28 14:30:47,907 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3edbbe1df6dd9b871df3d92dad6edcda.
2022-06-28 14:30:47,909 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7a2fc077
2022-06-28 14:30:47,909 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,909 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f) [DEPLOYING].
2022-06-28 14:30:47,909 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,909 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,910 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,910 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@59a31cb
2022-06-28 14:30:47,910 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,910 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,910 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,911 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (3455990209248d35741b396d657057a0) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,911 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (40f58bb4774bf64a42e706d6a78ef93f) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,913 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d), deploy into slot with allocation id 3edbbe1df6dd9b871df3d92dad6edcda.
2022-06-28 14:30:47,914 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d6b116f0f895db5064416220921b974c.
2022-06-28 14:30:47,916 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399), deploy into slot with allocation id d6b116f0f895db5064416220921b974c.
2022-06-28 14:30:47,917 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d72865e645a75a8c8e6ff2d60713a47e.
2022-06-28 14:30:47,918 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,918 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399) [DEPLOYING].
2022-06-28 14:30:47,918 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804), deploy into slot with allocation id d72865e645a75a8c8e6ff2d60713a47e.
2022-06-28 14:30:47,918 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,919 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d) [DEPLOYING].
2022-06-28 14:30:47,919 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@46553d09
2022-06-28 14:30:47,919 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,919 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,919 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,921 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3741e40d
2022-06-28 14:30:47,922 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (ab819048d0d78d9d327ba6498dd75399) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,922 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,922 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,919 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,921 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 29c0bf965a70238c6a6ef8c59d3781f8.
2022-06-28 14:30:47,922 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,922 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804) [DEPLOYING].
2022-06-28 14:30:47,923 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (cb8967fe237a1989878133425eed4e7d) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,924 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@45a8e8c1
2022-06-28 14:30:47,924 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,924 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,924 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,925 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (17ffa76fbb98cc30493841b9ec0f1804) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,929 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (4790bbc5fd981aa0a072ae2f00843837), deploy into slot with allocation id 29c0bf965a70238c6a6ef8c59d3781f8.
2022-06-28 14:30:47,930 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 9e3ba88ce4a6cf9f5e45158683b34f8b.
2022-06-28 14:30:47,931 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (4790bbc5fd981aa0a072ae2f00843837) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,931 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (4790bbc5fd981aa0a072ae2f00843837) [DEPLOYING].
2022-06-28 14:30:47,932 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1), deploy into slot with allocation id 9e3ba88ce4a6cf9f5e45158683b34f8b.
2022-06-28 14:30:47,932 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot c799e2d2ff587df9bfbb84bc448a609e.
2022-06-28 14:30:47,934 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4758f4cc
2022-06-28 14:30:47,934 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,933 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,935 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,935 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1) [DEPLOYING].
2022-06-28 14:30:47,935 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (4790bbc5fd981aa0a072ae2f00843837) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,936 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (4790bbc5fd981aa0a072ae2f00843837) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,936 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@78894a87
2022-06-28 14:30:47,936 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,938 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,939 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,939 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f), deploy into slot with allocation id c799e2d2ff587df9bfbb84bc448a609e.
2022-06-28 14:30:47,939 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (62309e5d04d8c9f2ca87cdff6928a0c1) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d72865e645a75a8c8e6ff2d60713a47e.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d6b116f0f895db5064416220921b974c.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ae9651ed91851a87873c2a594e2109ce.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 9e3ba88ce4a6cf9f5e45158683b34f8b.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3edbbe1df6dd9b871df3d92dad6edcda.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ce9dd4ec981663547c4ffd641e2ac5e4.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 75dd327f26b5c7142181e43f6d2b2ee0.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot c799e2d2ff587df9bfbb84bc448a609e.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 16a671e2ac06b189dedcac85ec074bbf.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1d30c134a0aa59d55476f993818c0848.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 7801d18002312cc3129809fab78bbf27.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 29c0bf965a70238c6a6ef8c59d3781f8.
2022-06-28 14:30:47,941 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f) switched from CREATED to DEPLOYING.
2022-06-28 14:30:47,945 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f) [DEPLOYING].
2022-06-28 14:30:47,948 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@738af23f
2022-06-28 14:30:47,948 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:30:47,948 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:30:47,949 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,950 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (6c94ea8490ea02062906fa5667e5ee9f) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:30:47,953 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,954 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,954 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,954 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,955 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,954 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,954 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,954 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,954 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,955 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,954 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:47,967 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:30:48,066 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 1 @ 
2022-06-28 14:30:48,067 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 0 @ 
2022-06-28 14:30:48,067 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 7 @ 
2022-06-28 14:30:48,067 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 2 @ 
2022-06-28 14:30:48,067 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 8 @ 
2022-06-28 14:30:48,067 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 5 @ 
2022-06-28 14:30:48,067 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 11 @ 
2022-06-28 14:30:48,067 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 4 @ 
2022-06-28 14:30:48,067 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 10 @ 
2022-06-28 14:30:48,067 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,068 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,068 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 9 @ 
2022-06-28 14:30:48,068 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 6 @ 
2022-06-28 14:30:48,068 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 3 @ 
2022-06-28 14:30:48,068 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,068 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,068 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (1408b5bd627cff5a1073fb3f31be5063) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,068 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,068 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,069 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,069 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,069 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,069 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (b4947ae341e8fb4f7c43819fc4e2c556) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,069 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (052f9a4f586b62d0e83c230659408f89) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,069 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,069 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (46ccf8ef24e971b1c6ec7bb39cd57109) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,070 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,070 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (229ba7a0bfb43739af8be73d8ae4e536) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,070 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (70992881caca649ee10da257c923b2d7) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,070 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,070 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (8b486b1680ef46d496ccb042674767c8) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,070 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (93556fa71c22a2877eee85431c15109b) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,070 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (72861e0816fbf58cc3ce68dfcfcbc142) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,070 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (d785e0124113f661df296536a625fc19) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,070 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (a0d87a6198b30a466a9048d12caef989) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,071 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,081 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,081 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,081 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,081 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,081 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,081 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,082 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,082 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,081 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,081 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,082 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,082 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:30:48,124 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,125 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,125 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848123
2022-06-28 14:30:48,126 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,126 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,126 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848125
2022-06-28 14:30:48,127 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,128 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,128 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848125
2022-06-28 14:30:48,136 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,136 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,136 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848123
2022-06-28 14:30:48,138 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,139 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,139 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848138
2022-06-28 14:30:48,142 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,142 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,143 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848124
2022-06-28 14:30:48,143 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,143 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,143 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848124
2022-06-28 14:30:48,144 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,144 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,145 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848124
2022-06-28 14:30:48,145 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,145 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,146 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848123
2022-06-28 14:30:48,146 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,146 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,146 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848123
2022-06-28 14:30:48,146 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,147 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,147 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848132
2022-06-28 14:30:48,147 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,147 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,147 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848130
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,157 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,167 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:30:48,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,219 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,220 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,220 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,220 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (b0e028f1c2db7d70045b3d4792416272) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,220 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,220 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,221 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (4790bbc5fd981aa0a072ae2f00843837) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,221 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (ab819048d0d78d9d327ba6498dd75399) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,222 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (429b2b31a37ba529c9bdd32dedeb3bfc) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,222 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (3455990209248d35741b396d657057a0) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,222 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (17ffa76fbb98cc30493841b9ec0f1804) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,223 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (cb8967fe237a1989878133425eed4e7d) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,223 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (06f98a210d800ba69278fee9949002a5) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,223 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (62309e5d04d8c9f2ca87cdff6928a0c1) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,223 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (40f58bb4774bf64a42e706d6a78ef93f) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,223 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (6c94ea8490ea02062906fa5667e5ee9f) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,223 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (7bf37cb80e788c5f57b2dfef985963f8) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,224 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (4790bbc5fd981aa0a072ae2f00843837) switched from INITIALIZING to RUNNING.
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-2] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-10] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-5] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-11] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-7] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-9] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-8] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-3] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-4] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-12] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-6] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,453 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-1] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,456 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Discovered new partitions: [flink-sql-exercise-0]
2022-06-28 14:30:48,459 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Assigning splits to readers {4=[[Partition: flink-sql-exercise-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
2022-06-28 14:30:48,465 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: flink-sql-exercise-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
2022-06-28 14:30:48,468 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 14:30:48,472 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:30:48,472 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:30:48,472 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:30:48,472 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:30:48,472 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656397848472
2022-06-28 14:30:48,479 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2022-06-28 14:30:48,482 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=g5-4, groupId=g5] Subscribed to partition(s): flink-sql-exercise-0
2022-06-28 14:30:48,486 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-4, groupId=g5] Seeking to EARLIEST offset of partition flink-sql-exercise-0
2022-06-28 14:30:48,496 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=g5-4, groupId=g5] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:30:48,506 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-4, groupId=g5] Resetting offset for partition flink-sql-exercise-0 to offset 0.
2022-06-28 14:30:48,584 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,588 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (4790bbc5fd981aa0a072ae2f00843837) switched from RUNNING to FAILED with failure cause: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56)
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30)
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)

2022-06-28 14:30:48,588 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (4790bbc5fd981aa0a072ae2f00843837).
2022-06-28 14:30:48,592 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 4790bbc5fd981aa0a072ae2f00843837.
2022-06-28 14:30:48,594 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (4790bbc5fd981aa0a072ae2f00843837) switched from RUNNING to FAILED on 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 @ kubernetes.docker.internal (dataPort=-1).
org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.4.jar:1.14.4]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-06-28 14:30:48,637 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 90bea66de1c231edf33913ecd54406c1_9.
2022-06-28 14:30:48,638 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 24 tasks should be restarted to recover the failed task 90bea66de1c231edf33913ecd54406c1_9. 
2022-06-28 14:30:48,640 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (ac07f2553170acd984d5562405c528aa) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-runtime-1.14.4.jar:1.14.4]
	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[scala-library-2.12.7.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_221]
Caused by: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.4.jar:1.14.4]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-06-28 14:30:48,664 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (46ccf8ef24e971b1c6ec7bb39cd57109) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,665 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109).
2022-06-28 14:30:48,665 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,665 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109).
2022-06-28 14:30:48,666 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (8b486b1680ef46d496ccb042674767c8) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,667 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (a0d87a6198b30a466a9048d12caef989) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,667 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (d785e0124113f661df296536a625fc19) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,667 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (229ba7a0bfb43739af8be73d8ae4e536) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,667 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (1408b5bd627cff5a1073fb3f31be5063) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,667 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,667 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,668 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (052f9a4f586b62d0e83c230659408f89) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,668 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (b4947ae341e8fb4f7c43819fc4e2c556) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,668 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (72861e0816fbf58cc3ce68dfcfcbc142) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,668 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (70992881caca649ee10da257c923b2d7) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,668 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (93556fa71c22a2877eee85431c15109b) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,668 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,668 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (06f98a210d800ba69278fee9949002a5) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,668 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (46ccf8ef24e971b1c6ec7bb39cd57109).
2022-06-28 14:30:48,669 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (b0e028f1c2db7d70045b3d4792416272) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,669 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8).
2022-06-28 14:30:48,669 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (7bf37cb80e788c5f57b2dfef985963f8) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,669 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,669 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8).
2022-06-28 14:30:48,669 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (429b2b31a37ba529c9bdd32dedeb3bfc) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,669 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (3455990209248d35741b396d657057a0) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,670 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (40f58bb4774bf64a42e706d6a78ef93f) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,672 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989).
2022-06-28 14:30:48,672 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,673 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (cb8967fe237a1989878133425eed4e7d) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,673 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,673 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989).
2022-06-28 14:30:48,673 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,673 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (8b486b1680ef46d496ccb042674767c8).
2022-06-28 14:30:48,673 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (ab819048d0d78d9d327ba6498dd75399) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,675 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,675 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (17ffa76fbb98cc30493841b9ec0f1804) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,675 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19).
2022-06-28 14:30:48,675 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,676 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,676 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (a0d87a6198b30a466a9048d12caef989).
2022-06-28 14:30:48,676 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19).
2022-06-28 14:30:48,676 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (62309e5d04d8c9f2ca87cdff6928a0c1) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,677 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (6c94ea8490ea02062906fa5667e5ee9f) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,677 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536).
2022-06-28 14:30:48,677 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,677 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,678 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536).
2022-06-28 14:30:48,678 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,678 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (d785e0124113f661df296536a625fc19).
2022-06-28 14:30:48,681 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063).
2022-06-28 14:30:48,681 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,681 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,681 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2022-06-28 14:30:48,681 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063).
2022-06-28 14:30:48,682 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc).
2022-06-28 14:30:48,682 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,682 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc).
2022-06-28 14:30:48,682 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89).
2022-06-28 14:30:48,683 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,683 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89).
2022-06-28 14:30:48,683 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556).
2022-06-28 14:30:48,683 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,683 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556).
2022-06-28 14:30:48,683 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2022-06-28 14:30:48,684 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142).
2022-06-28 14:30:48,684 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,684 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142).
2022-06-28 14:30:48,684 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7).
2022-06-28 14:30:48,684 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,684 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7).
2022-06-28 14:30:48,685 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b).
2022-06-28 14:30:48,685 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,685 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b).
2022-06-28 14:30:48,685 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,685 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5).
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,686 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,686 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,686 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (1408b5bd627cff5a1073fb3f31be5063).
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (72861e0816fbf58cc3ce68dfcfcbc142).
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (93556fa71c22a2877eee85431c15109b).
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5).
2022-06-28 14:30:48,686 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (229ba7a0bfb43739af8be73d8ae4e536).
2022-06-28 14:30:48,687 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,687 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (052f9a4f586b62d0e83c230659408f89).
2022-06-28 14:30:48,687 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272).
2022-06-28 14:30:48,687 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,687 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272).
2022-06-28 14:30:48,687 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,687 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,689 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,689 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:30:48,689 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (cfa43b92ce02b3b32d7a0708ad6c8dfc).
2022-06-28 14:30:48,689 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,689 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (70992881caca649ee10da257c923b2d7).
2022-06-28 14:30:48,689 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,689 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (b4947ae341e8fb4f7c43819fc4e2c556).
2022-06-28 14:30:48,689 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,690 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8).
2022-06-28 14:30:48,690 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,690 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8).
2022-06-28 14:30:48,691 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc).
2022-06-28 14:30:48,691 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,691 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc).
2022-06-28 14:30:48,691 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,691 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (06f98a210d800ba69278fee9949002a5).
2022-06-28 14:30:48,691 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 46ccf8ef24e971b1c6ec7bb39cd57109.
2022-06-28 14:30:48,692 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0).
2022-06-28 14:30:48,692 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,692 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0).
2022-06-28 14:30:48,692 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,693 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (46ccf8ef24e971b1c6ec7bb39cd57109) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,695 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f).
2022-06-28 14:30:48,696 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,696 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f).
2022-06-28 14:30:48,696 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,696 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (b0e028f1c2db7d70045b3d4792416272).
2022-06-28 14:30:48,697 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d).
2022-06-28 14:30:48,697 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,697 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d).
2022-06-28 14:30:48,698 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 8b486b1680ef46d496ccb042674767c8.
2022-06-28 14:30:48,698 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,698 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399).
2022-06-28 14:30:48,698 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,698 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,698 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (8b486b1680ef46d496ccb042674767c8) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,698 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399).
2022-06-28 14:30:48,700 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,700 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (7bf37cb80e788c5f57b2dfef985963f8).
2022-06-28 14:30:48,700 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804).
2022-06-28 14:30:48,700 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,700 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,700 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804).
2022-06-28 14:30:48,700 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (cb8967fe237a1989878133425eed4e7d).
2022-06-28 14:30:48,701 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,701 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 a0d87a6198b30a466a9048d12caef989.
2022-06-28 14:30:48,701 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,702 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,702 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1).
2022-06-28 14:30:48,702 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,702 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1).
2022-06-28 14:30:48,702 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (a0d87a6198b30a466a9048d12caef989) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,703 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f).
2022-06-28 14:30:48,703 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f) switched from RUNNING to CANCELING.
2022-06-28 14:30:48,703 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f).
2022-06-28 14:30:48,703 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,703 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 d785e0124113f661df296536a625fc19.
2022-06-28 14:30:48,703 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,704 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 72861e0816fbf58cc3ce68dfcfcbc142.
2022-06-28 14:30:48,704 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (d785e0124113f661df296536a625fc19) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,704 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,704 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 93556fa71c22a2877eee85431c15109b.
2022-06-28 14:30:48,704 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (40f58bb4774bf64a42e706d6a78ef93f).
2022-06-28 14:30:48,704 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,704 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 229ba7a0bfb43739af8be73d8ae4e536.
2022-06-28 14:30:48,705 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:30:48,704 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 052f9a4f586b62d0e83c230659408f89.
2022-06-28 14:30:48,705 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 1408b5bd627cff5a1073fb3f31be5063.
2022-06-28 14:30:48,705 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (72861e0816fbf58cc3ce68dfcfcbc142) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,705 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 cfa43b92ce02b3b32d7a0708ad6c8dfc.
2022-06-28 14:30:48,705 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 b4947ae341e8fb4f7c43819fc4e2c556.
2022-06-28 14:30:48,705 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,706 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (429b2b31a37ba529c9bdd32dedeb3bfc).
2022-06-28 14:30:48,706 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 70992881caca649ee10da257c923b2d7.
2022-06-28 14:30:48,706 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 06f98a210d800ba69278fee9949002a5.
2022-06-28 14:30:48,706 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 b0e028f1c2db7d70045b3d4792416272.
2022-06-28 14:30:48,707 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 7bf37cb80e788c5f57b2dfef985963f8.
2022-06-28 14:30:48,707 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 cb8967fe237a1989878133425eed4e7d.
2022-06-28 14:30:48,707 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,707 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (3455990209248d35741b396d657057a0).
2022-06-28 14:30:48,707 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 40f58bb4774bf64a42e706d6a78ef93f.
2022-06-28 14:30:48,707 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,707 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (ab819048d0d78d9d327ba6498dd75399).
2022-06-28 14:30:48,707 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 429b2b31a37ba529c9bdd32dedeb3bfc.
2022-06-28 14:30:48,708 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 3455990209248d35741b396d657057a0.
2022-06-28 14:30:48,708 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 ab819048d0d78d9d327ba6498dd75399.
2022-06-28 14:30:48,708 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,708 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (6c94ea8490ea02062906fa5667e5ee9f).
2022-06-28 14:30:48,708 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,708 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (62309e5d04d8c9f2ca87cdff6928a0c1).
2022-06-28 14:30:48,709 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,709 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (17ffa76fbb98cc30493841b9ec0f1804).
2022-06-28 14:30:48,709 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 6c94ea8490ea02062906fa5667e5ee9f.
2022-06-28 14:30:48,710 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 62309e5d04d8c9f2ca87cdff6928a0c1.
2022-06-28 14:30:48,710 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=11}]
2022-06-28 14:30:48,710 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 17ffa76fbb98cc30493841b9ec0f1804.
2022-06-28 14:30:48,710 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (93556fa71c22a2877eee85431c15109b) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,712 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (229ba7a0bfb43739af8be73d8ae4e536) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,713 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (052f9a4f586b62d0e83c230659408f89) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,713 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (1408b5bd627cff5a1073fb3f31be5063) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,714 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (cfa43b92ce02b3b32d7a0708ad6c8dfc) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,714 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (b4947ae341e8fb4f7c43819fc4e2c556) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,715 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (70992881caca649ee10da257c923b2d7) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,715 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (06f98a210d800ba69278fee9949002a5) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,715 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=10}]
2022-06-28 14:30:48,715 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (b0e028f1c2db7d70045b3d4792416272) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,716 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=9}]
2022-06-28 14:30:48,716 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (7bf37cb80e788c5f57b2dfef985963f8) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,716 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=8}]
2022-06-28 14:30:48,716 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (cb8967fe237a1989878133425eed4e7d) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,717 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=7}]
2022-06-28 14:30:48,717 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (40f58bb4774bf64a42e706d6a78ef93f) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,717 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
2022-06-28 14:30:48,717 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (429b2b31a37ba529c9bdd32dedeb3bfc) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,718 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
2022-06-28 14:30:48,718 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (3455990209248d35741b396d657057a0) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,719 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
2022-06-28 14:30:48,719 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (ab819048d0d78d9d327ba6498dd75399) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,720 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
2022-06-28 14:30:48,720 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (6c94ea8490ea02062906fa5667e5ee9f) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,720 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
2022-06-28 14:30:48,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (62309e5d04d8c9f2ca87cdff6928a0c1) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,721 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job ac07f2553170acd984d5562405c528aa: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
2022-06-28 14:30:48,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (17ffa76fbb98cc30493841b9ec0f1804) switched from CANCELING to CANCELED.
2022-06-28 14:30:48,722 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job ac07f2553170acd984d5562405c528aa
2022-06-28 14:30:48,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (ac07f2553170acd984d5562405c528aa) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-runtime-1.14.4.jar:1.14.4]
	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[scala-library-2.12.7.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_221]
Caused by: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.4.jar:1.14.4]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-06-28 14:30:48,728 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job ac07f2553170acd984d5562405c528aa.
2022-06-28 14:30:48,739 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Shutting down Flink Mini Cluster
2022-06-28 14:30:48,739 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job ac07f2553170acd984d5562405c528aa reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56)
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30)
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)
2022-06-28 14:30:48,739 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Shutting down rest endpoint.
2022-06-28 14:30:48,739 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
2022-06-28 14:30:48,739 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close ResourceManager connection f5fc595e533f3335d4a37e0b1f85aca3.
2022-06-28 14:30:48,740 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Closing TaskExecutor connection 3bfa538e-58a8-46fe-b8a0-1532ba74fcc6 because: The TaskExecutor is shutting down.
2022-06-28 14:30:48,740 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job ac07f2553170acd984d5562405c528aa.
2022-06-28 14:30:48,742 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,742 INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager [] - Shutting down TaskExecutorStateChangelogStoragesManager.
2022-06-28 14:30:48,743 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:7, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: d72865e645a75a8c8e6ff2d60713a47e, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,745 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:2, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: d6b116f0f895db5064416220921b974c, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,745 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:8, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: ae9651ed91851a87873c2a594e2109ce, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,745 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise.
2022-06-28 14:30:48,745 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:9, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 9e3ba88ce4a6cf9f5e45158683b34f8b, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,746 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:3, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 3edbbe1df6dd9b871df3d92dad6edcda, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,746 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:4, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: ce9dd4ec981663547c4ffd641e2ac5e4, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,746 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:10, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 75dd327f26b5c7142181e43f6d2b2ee0, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,746 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:5, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: c799e2d2ff587df9bfbb84bc448a609e, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,746 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 16a671e2ac06b189dedcac85ec074bbf, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,746 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:6, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 1d30c134a0aa59d55476f993818c0848, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,747 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:11, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 7801d18002312cc3129809fab78bbf27, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,747 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:1, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 29c0bf965a70238c6a6ef8c59d3781f8, jobId: ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,751 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise closed.
2022-06-28 14:30:48,752 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [d72865e645a75a8c8e6ff2d60713a47e].
2022-06-28 14:30:48,755 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [d6b116f0f895db5064416220921b974c].
2022-06-28 14:30:48,756 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [ae9651ed91851a87873c2a594e2109ce].
2022-06-28 14:30:48,756 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [9e3ba88ce4a6cf9f5e45158683b34f8b].
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [3edbbe1df6dd9b871df3d92dad6edcda].
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [ce9dd4ec981663547c4ffd641e2ac5e4].
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [75dd327f26b5c7142181e43f6d2b2ee0].
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [c799e2d2ff587df9bfbb84bc448a609e].
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [16a671e2ac06b189dedcac85ec074bbf].
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [1d30c134a0aa59d55476f993818c0848].
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [7801d18002312cc3129809fab78bbf27].
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [29c0bf965a70238c6a6ef8c59d3781f8].
2022-06-28 14:30:48,757 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Removing cache directory C:\Users\BONC\AppData\Local\Temp\flink-web-ui
2022-06-28 14:30:48,758 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2022-06-28 14:30:48,758 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection f5fc595e533f3335d4a37e0b1f85aca3: Stopping JobMaster for job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (ac07f2553170acd984d5562405c528aa).
2022-06-28 14:30:48,759 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager bd2604517084adfbedbcadd8452d42c1@akka://flink/user/rpc/jobmanager_3 for job ac07f2553170acd984d5562405c528aa from the resource manager.
2022-06-28 14:30:48,763 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Shut down complete.
2022-06-28 14:30:48,765 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
2022-06-28 14:30:48,766 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.
2022-06-28 14:30:48,766 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.
2022-06-28 14:30:48,767 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping dispatcher akka://flink/user/rpc/dispatcher_1.
2022-06-28 14:30:48,767 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.
2022-06-28 14:30:48,767 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_1.
2022-06-28 14:30:48,767 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.
2022-06-28 14:30:48,767 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.
2022-06-28 14:30:48,769 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka://flink/user/rpc/dispatcher_1.
2022-06-28 14:30:48,770 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory C:\Users\BONC\AppData\Local\Temp\flink-io-d9f6cdff-2d1c-48cb-94a0-8c55cd0ad80e
2022-06-28 14:30:48,771 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Shutting down the network environment and its components.
2022-06-28 14:30:48,772 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory C:\Users\BONC\AppData\Local\Temp\flink-netty-shuffle-fa009cc3-284e-4600-8df8-84d5c59ff4c8
2022-06-28 14:30:48,772 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Shutting down the kvState service and its components.
2022-06-28 14:30:48,772 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
2022-06-28 14:30:48,773 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory C:\Users\BONC\AppData\Local\Temp\flink-dist-cache-3a21691b-24b1-4550-bd15-afde5f99049a
2022-06-28 14:30:48,773 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
2022-06-28 14:30:48,774 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2022-06-28 14:30:48,828 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2022-06-28 14:30:48,828 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2022-06-28 14:30:48,838 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2022-06-28 14:30:48,839 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2022-06-28 14:30:48,840 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:65453
2022-06-28 14:30:48,843 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2022-06-28 14:38:15,886 WARN  org.apache.flink.connector.kafka.sink.KafkaSinkBuilder       [] - Property [transaction.timeout.ms] not specified. Setting it to PT1H
2022-06-28 14:38:16,160 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:38:16,160 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:38:16,160 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:38:16,161 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 14:38:16,161 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 14:38:16,161 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2022-06-28 14:38:16,166 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Flink Mini Cluster
2022-06-28 14:38:16,487 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Metrics Registry
2022-06-28 14:38:16,528 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
2022-06-28 14:38:16,528 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting RPC Service(s)
2022-06-28 14:38:16,538 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 14:38:17,005 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 14:38:17,084 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink
2022-06-28 14:38:17,094 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 14:38:17,101 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 14:38:17,107 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink-metrics
2022-06-28 14:38:17,116 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2022-06-28 14:38:17,128 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting high-availability services
2022-06-28 14:38:17,136 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-8122d208-a65c-486e-94b3-5a57002df352
2022-06-28 14:38:17,139 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:49320 - max concurrent requests: 50 - max backlog: 1000
2022-06-28 14:38:17,141 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-0607dc07-38b3-4d0d-b853-6f15b61aa490
2022-06-28 14:38:17,142 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-b09a9b09-b209-46fd-967e-93169e313d54
2022-06-28 14:38:17,143 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting 1 TaskManager(s)
2022-06-28 14:38:17,145 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 4991636a-9fa3-4000-99eb-87c97f8a37a1
2022-06-28 14:38:17,153 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory 'C:\Users\BONC\AppData\Local\Temp': total 237 GB, usable 176 GB (74.26% usable)
2022-06-28 14:38:17,156 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-io-37bde3db-dc5c-400f-9394-f55a97f5f44a
2022-06-28 14:38:17,162 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-netty-shuffle-9c2c4326-6983-4cc6-84cf-4ee4eb3caf17
2022-06-28 14:38:17,186 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2022-06-28 14:38:17,194 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
2022-06-28 14:38:17,195 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
2022-06-28 14:38:17,203 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'akka.ask.timeout' instead of key 'taskmanager.slot.timeout'
2022-06-28 14:38:17,212 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2022-06-28 14:38:17,221 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
2022-06-28 14:38:17,222 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory C:\Users\BONC\AppData\Local\Temp\flink-dist-cache-d3d807e2-3e5e-44ad-8053-1668da8c14f5
2022-06-28 14:38:17,255 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Starting rest endpoint.
2022-06-28 14:38:17,257 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
2022-06-28 14:38:17,312 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Log file environment variable 'log.file' is not set.
2022-06-28 14:38:17,313 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
2022-06-28 14:38:17,553 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Rest endpoint listening at localhost:49371
2022-06-28 14:38:17,554 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender http://localhost:49371
2022-06-28 14:38:17,556 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - http://localhost:49371 was granted leadership with leaderSessionID=73c74dc0-1623-40ef-bf22-f016b3f4be84
2022-06-28 14:38:17,556 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader http://localhost:49371 , session=73c74dc0-1623-40ef-bf22-f016b3f4be84
2022-06-28 14:38:17,565 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2022-06-28 14:38:17,565 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
2022-06-28 14:38:17,565 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: ResourceManagerServiceImpl
2022-06-28 14:38:17,566 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id a73cd5cf-35ed-4644-851b-c2b0f4f601e2. Creating new DispatcherLeaderProcess.
2022-06-28 14:38:17,566 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 54f33db7-3599-4ce3-9a7a-ebd8465cefdf.
2022-06-28 14:38:17,568 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Flink Mini Cluster started successfully
2022-06-28 14:38:17,570 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
2022-06-28 14:38:17,571 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.
2022-06-28 14:38:17,572 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
2022-06-28 14:38:17,577 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
2022-06-28 14:38:17,582 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_2 .
2022-06-28 14:38:17,586 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_1 , session=a73cd5cf-35ed-4644-851b-c2b0f4f601e2
2022-06-28 14:38:17,587 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
2022-06-28 14:38:17,596 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_2 , session=54f33db7-3599-4ce3-9a7a-ebd8465cefdf
2022-06-28 14:38:17,597 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(9a7aebd8465cefdf54f33db735994ce3).
2022-06-28 14:38:17,610 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
2022-06-28 14:38:17,614 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 4991636a-9fa3-4000-99eb-87c97f8a37a1 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2022-06-28 14:38:17,616 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_2 under registration id 566a506024de64d5acef2e76f6d8c461.
2022-06-28 14:38:17,617 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:17,618 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:17,629 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2022-06-28 14:38:17,638 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2022-06-28 14:38:17,644 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:17,660 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for insert-into_default_catalog.default_database.t_kafka_nicekcnt (c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:17,703 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job insert-into_default_catalog.default_database.t_kafka_nicekcnt (c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:17,704 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
2022-06-28 14:38:17,737 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 0 ms
2022-06-28 14:38:17,757 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4e35104d
2022-06-28 14:38:17,757 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,758 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,771 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
2022-06-28 14:38:17,777 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@207e764c for insert-into_default_catalog.default_database.t_kafka_nicekcnt (c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:17,784 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=c06c88cd-71f9-4a72-a262-b6f568c105f8
2022-06-28 14:38:17,786 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (c0a11dc3ebc0ae5a5dfddd283134dd70) under job master id a262b6f568c105f8c06c88cd71f94a72.
2022-06-28 14:38:17,787 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise.
2022-06-28 14:38:17,789 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2022-06-28 14:38:17,789 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (c0a11dc3ebc0ae5a5dfddd283134dd70) switched from state CREATED to RUNNING.
2022-06-28 14:38:17,793 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (588b3654da4a8e24fc1761e7cd6ab5fe) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (a4da1f9c7f7ad9fbb14e998f779b0919) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (68fa23854e4c78da22e734bb0465ec70) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (8633fa4b6a858b040e007a5ae42cbe35) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (e796a06d1ccb2d0058ce85823ed8f4af) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (c3dd6af93e7d308b80433d49b1b48251) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (76ec9c0d22dff0ce2b7466f3b75be708) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (10e43b9a3d92b5d3b04e365469af82d4) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (3281386b23445b357434006aedeefd9a) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (43811564f27f2e17e926f4553282d837) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,794 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (703ca70f828036577f24fe6491a57ad8) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (dbf2a555249d5f02a7f0398ddd5e5488) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (cd83158945013209139193039556e00a) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (08cea9e6d486665867fc068e4bd073cb) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6571f96578a56cc4f18b269a570cb10d) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (b0dc8f3f9589a6016e15d7ecbd41188c) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (789c52cf12ce8fb9c8c2f061cc8877c1) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (02bf122c0090a0f74d4a6d2f24d97310) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (133d45f20274bc9005e56649640cff61) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (6866cd47c68fd6e054bf5406aada60ee) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (8490e61e0836c2b961838b054d93c35e) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,795 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (e974e5b582c9599927ec1249fc8bbf13) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,796 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (42a375658a0b731afae537ad15bd5fbf) switched from CREATED to SCHEDULED.
2022-06-28 14:38:17,811 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(9a7aebd8465cefdf54f33db735994ce3)
2022-06-28 14:38:17,813 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
2022-06-28 14:38:17,814 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager a262b6f568c105f8c06c88cd71f94a72@akka://flink/user/rpc/jobmanager_3 for job c0a11dc3ebc0ae5a5dfddd283134dd70.
2022-06-28 14:38:17,814 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 14:38:17,819 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager a262b6f568c105f8c06c88cd71f94a72@akka://flink/user/rpc/jobmanager_3 for job c0a11dc3ebc0ae5a5dfddd283134dd70.
2022-06-28 14:38:17,821 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,822 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=12}]
2022-06-28 14:38:17,825 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 2aad4cea74eb0e9ceea19ac67cbafbb8 for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,831 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 2aad4cea74eb0e9ceea19ac67cbafbb8.
2022-06-28 14:38:17,832 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job c0a11dc3ebc0ae5a5dfddd283134dd70 for job leader monitoring.
2022-06-28 14:38:17,833 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id c06c88cd-71f9-4a72-a262-b6f568c105f8.
2022-06-28 14:38:17,834 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 056e5cba4db76b1cdb94a4aa47e0d57b for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,834 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 056e5cba4db76b1cdb94a4aa47e0d57b.
2022-06-28 14:38:17,834 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 19f79675f5f8f11b63c12b27d486e370 for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,834 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 19f79675f5f8f11b63c12b27d486e370.
2022-06-28 14:38:17,834 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2022-06-28 14:38:17,834 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 3431c86c9aabd2a7356c6a9edf0dbe0e for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,835 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 3431c86c9aabd2a7356c6a9edf0dbe0e.
2022-06-28 14:38:17,835 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request b8f3efa79d8aafc31877fb817a74970f for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,835 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for b8f3efa79d8aafc31877fb817a74970f.
2022-06-28 14:38:17,835 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 05a7832a4451ba579d3559369dca8087 for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,835 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 05a7832a4451ba579d3559369dca8087.
2022-06-28 14:38:17,835 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 4dcfaf77e07ba56762b0734215849138 for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,835 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 4dcfaf77e07ba56762b0734215849138.
2022-06-28 14:38:17,836 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 16ca465ad881d3871bafaca1cc217a24 for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,836 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 16ca465ad881d3871bafaca1cc217a24.
2022-06-28 14:38:17,836 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 5a4966e2fe4c788e44a9ed79d2f86162 for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,836 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 5a4966e2fe4c788e44a9ed79d2f86162.
2022-06-28 14:38:17,836 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request b27121af6016a289cdd9aaf3e39a1635 for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,837 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for b27121af6016a289cdd9aaf3e39a1635.
2022-06-28 14:38:17,837 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 1f749d6bbcd55df2cd9e09624908f25a for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,837 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 1f749d6bbcd55df2cd9e09624908f25a.
2022-06-28 14:38:17,837 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 1a427f99ea706922fec97ca4df7cfca0 for job c0a11dc3ebc0ae5a5dfddd283134dd70 from resource manager with leader id 9a7aebd8465cefdf54f33db735994ce3.
2022-06-28 14:38:17,837 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 1a427f99ea706922fec97ca4df7cfca0.
2022-06-28 14:38:17,839 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job c0a11dc3ebc0ae5a5dfddd283134dd70.
2022-06-28 14:38:17,839 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job c0a11dc3ebc0ae5a5dfddd283134dd70.
2022-06-28 14:38:17,842 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job c0a11dc3ebc0ae5a5dfddd283134dd70.
2022-06-28 14:38:17,851 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (588b3654da4a8e24fc1761e7cd6ab5fe) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,851 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (attempt #0) with attempt id 588b3654da4a8e24fc1761e7cd6ab5fe to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 19f79675f5f8f11b63c12b27d486e370
2022-06-28 14:38:17,855 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (a4da1f9c7f7ad9fbb14e998f779b0919) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,855 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (attempt #0) with attempt id a4da1f9c7f7ad9fbb14e998f779b0919 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 16ca465ad881d3871bafaca1cc217a24
2022-06-28 14:38:17,855 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (68fa23854e4c78da22e734bb0465ec70) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,855 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (attempt #0) with attempt id 68fa23854e4c78da22e734bb0465ec70 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 4dcfaf77e07ba56762b0734215849138
2022-06-28 14:38:17,855 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 19f79675f5f8f11b63c12b27d486e370.
2022-06-28 14:38:17,856 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (8633fa4b6a858b040e007a5ae42cbe35) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,856 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (attempt #0) with attempt id 8633fa4b6a858b040e007a5ae42cbe35 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 1f749d6bbcd55df2cd9e09624908f25a
2022-06-28 14:38:17,856 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (e796a06d1ccb2d0058ce85823ed8f4af) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,856 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (attempt #0) with attempt id e796a06d1ccb2d0058ce85823ed8f4af to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id b27121af6016a289cdd9aaf3e39a1635
2022-06-28 14:38:17,856 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (c3dd6af93e7d308b80433d49b1b48251) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,856 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (attempt #0) with attempt id c3dd6af93e7d308b80433d49b1b48251 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id b8f3efa79d8aafc31877fb817a74970f
2022-06-28 14:38:17,856 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (76ec9c0d22dff0ce2b7466f3b75be708) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,856 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (attempt #0) with attempt id 76ec9c0d22dff0ce2b7466f3b75be708 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 056e5cba4db76b1cdb94a4aa47e0d57b
2022-06-28 14:38:17,857 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (10e43b9a3d92b5d3b04e365469af82d4) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,857 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (attempt #0) with attempt id 10e43b9a3d92b5d3b04e365469af82d4 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 2aad4cea74eb0e9ceea19ac67cbafbb8
2022-06-28 14:38:17,857 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (3281386b23445b357434006aedeefd9a) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,857 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (attempt #0) with attempt id 3281386b23445b357434006aedeefd9a to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 3431c86c9aabd2a7356c6a9edf0dbe0e
2022-06-28 14:38:17,858 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (43811564f27f2e17e926f4553282d837) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,858 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (attempt #0) with attempt id 43811564f27f2e17e926f4553282d837 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 05a7832a4451ba579d3559369dca8087
2022-06-28 14:38:17,858 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (703ca70f828036577f24fe6491a57ad8) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,858 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (attempt #0) with attempt id 703ca70f828036577f24fe6491a57ad8 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 1a427f99ea706922fec97ca4df7cfca0
2022-06-28 14:38:17,858 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (dbf2a555249d5f02a7f0398ddd5e5488) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,858 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (attempt #0) with attempt id dbf2a555249d5f02a7f0398ddd5e5488 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 5a4966e2fe4c788e44a9ed79d2f86162
2022-06-28 14:38:17,858 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (cd83158945013209139193039556e00a) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,858 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (attempt #0) with attempt id cd83158945013209139193039556e00a to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 19f79675f5f8f11b63c12b27d486e370
2022-06-28 14:38:17,865 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,865 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (attempt #0) with attempt id 48e8d0a9b0ba1b358bef8421e73aa1e8 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 16ca465ad881d3871bafaca1cc217a24
2022-06-28 14:38:17,865 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (08cea9e6d486665867fc068e4bd073cb) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,865 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (attempt #0) with attempt id 08cea9e6d486665867fc068e4bd073cb to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 4dcfaf77e07ba56762b0734215849138
2022-06-28 14:38:17,865 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6571f96578a56cc4f18b269a570cb10d) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,865 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (attempt #0) with attempt id 6571f96578a56cc4f18b269a570cb10d to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 1f749d6bbcd55df2cd9e09624908f25a
2022-06-28 14:38:17,865 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (b0dc8f3f9589a6016e15d7ecbd41188c) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,865 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (attempt #0) with attempt id b0dc8f3f9589a6016e15d7ecbd41188c to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id b27121af6016a289cdd9aaf3e39a1635
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (789c52cf12ce8fb9c8c2f061cc8877c1) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (attempt #0) with attempt id 789c52cf12ce8fb9c8c2f061cc8877c1 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id b8f3efa79d8aafc31877fb817a74970f
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (02bf122c0090a0f74d4a6d2f24d97310) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (attempt #0) with attempt id 02bf122c0090a0f74d4a6d2f24d97310 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 056e5cba4db76b1cdb94a4aa47e0d57b
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (133d45f20274bc9005e56649640cff61) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (attempt #0) with attempt id 133d45f20274bc9005e56649640cff61 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 2aad4cea74eb0e9ceea19ac67cbafbb8
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (6866cd47c68fd6e054bf5406aada60ee) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (attempt #0) with attempt id 6866cd47c68fd6e054bf5406aada60ee to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 3431c86c9aabd2a7356c6a9edf0dbe0e
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (8490e61e0836c2b961838b054d93c35e) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (attempt #0) with attempt id 8490e61e0836c2b961838b054d93c35e to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 05a7832a4451ba579d3559369dca8087
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (e974e5b582c9599927ec1249fc8bbf13) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (attempt #0) with attempt id e974e5b582c9599927ec1249fc8bbf13 to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 1a427f99ea706922fec97ca4df7cfca0
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (42a375658a0b731afae537ad15bd5fbf) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:38:17,866 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (attempt #0) with attempt id 42a375658a0b731afae537ad15bd5fbf to 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1) with allocation id 5a4966e2fe4c788e44a9ed79d2f86162
2022-06-28 14:38:17,868 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory}.
2022-06-28 14:38:17,868 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2022-06-28 14:38:17,878 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:38:17,878 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:38:17,880 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:17,880 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:17,880 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398297878
2022-06-28 14:38:17,881 INFO  org.apache.kafka.clients.admin.AdminClientConfig             [] - AdminClientConfig values: 
	bootstrap.servers = [192.168.136.130:9092]
	client.dns.lookup = default
	client.id = g5-enumerator-admin-client
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-06-28 14:38:17,884 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe), deploy into slot with allocation id 19f79675f5f8f11b63c12b27d486e370.
2022-06-28 14:38:17,885 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,886 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 16ca465ad881d3871bafaca1cc217a24.
2022-06-28 14:38:17,888 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe) [DEPLOYING].
2022-06-28 14:38:17,888 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919), deploy into slot with allocation id 16ca465ad881d3871bafaca1cc217a24.
2022-06-28 14:38:17,889 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 4dcfaf77e07ba56762b0734215849138.
2022-06-28 14:38:17,889 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,889 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919) [DEPLOYING].
2022-06-28 14:38:17,891 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70), deploy into slot with allocation id 4dcfaf77e07ba56762b0734215849138.
2022-06-28 14:38:17,892 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1f749d6bbcd55df2cd9e09624908f25a.
2022-06-28 14:38:17,891 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,892 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70) [DEPLOYING].
2022-06-28 14:38:17,894 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35), deploy into slot with allocation id 1f749d6bbcd55df2cd9e09624908f25a.
2022-06-28 14:38:17,895 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b27121af6016a289cdd9aaf3e39a1635.
2022-06-28 14:38:17,895 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,895 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35) [DEPLOYING].
2022-06-28 14:38:17,897 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af), deploy into slot with allocation id b27121af6016a289cdd9aaf3e39a1635.
2022-06-28 14:38:17,897 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b8f3efa79d8aafc31877fb817a74970f.
2022-06-28 14:38:17,897 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,898 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af) [DEPLOYING].
2022-06-28 14:38:17,900 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251), deploy into slot with allocation id b8f3efa79d8aafc31877fb817a74970f.
2022-06-28 14:38:17,901 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 056e5cba4db76b1cdb94a4aa47e0d57b.
2022-06-28 14:38:17,901 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,901 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251) [DEPLOYING].
2022-06-28 14:38:17,905 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708), deploy into slot with allocation id 056e5cba4db76b1cdb94a4aa47e0d57b.
2022-06-28 14:38:17,905 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2aad4cea74eb0e9ceea19ac67cbafbb8.
2022-06-28 14:38:17,905 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,906 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708) [DEPLOYING].
2022-06-28 14:38:17,908 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4), deploy into slot with allocation id 2aad4cea74eb0e9ceea19ac67cbafbb8.
2022-06-28 14:38:17,908 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3431c86c9aabd2a7356c6a9edf0dbe0e.
2022-06-28 14:38:17,908 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,909 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4) [DEPLOYING].
2022-06-28 14:38:17,909 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'key.deserializer' was supplied but isn't a known config.
2022-06-28 14:38:17,909 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'value.deserializer' was supplied but isn't a known config.
2022-06-28 14:38:17,909 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'enable.auto.commit' was supplied but isn't a known config.
2022-06-28 14:38:17,909 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'group.id' was supplied but isn't a known config.
2022-06-28 14:38:17,909 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:38:17,909 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:38:17,910 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2022-06-28 14:38:17,910 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:17,910 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:17,910 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398297910
2022-06-28 14:38:17,910 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Starting the KafkaSourceEnumerator for consumer group g5 without periodic partition discovery.
2022-06-28 14:38:17,911 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a), deploy into slot with allocation id 3431c86c9aabd2a7356c6a9edf0dbe0e.
2022-06-28 14:38:17,912 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 05a7832a4451ba579d3559369dca8087.
2022-06-28 14:38:17,912 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,912 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a) [DEPLOYING].
2022-06-28 14:38:17,914 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837), deploy into slot with allocation id 05a7832a4451ba579d3559369dca8087.
2022-06-28 14:38:17,915 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1a427f99ea706922fec97ca4df7cfca0.
2022-06-28 14:38:17,915 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,915 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837) [DEPLOYING].
2022-06-28 14:38:17,917 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8), deploy into slot with allocation id 1a427f99ea706922fec97ca4df7cfca0.
2022-06-28 14:38:17,918 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 5a4966e2fe4c788e44a9ed79d2f86162.
2022-06-28 14:38:17,919 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,920 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8) [DEPLOYING].
2022-06-28 14:38:17,921 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488), deploy into slot with allocation id 5a4966e2fe4c788e44a9ed79d2f86162.
2022-06-28 14:38:17,922 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,923 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488) [DEPLOYING].
2022-06-28 14:38:17,922 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 19f79675f5f8f11b63c12b27d486e370.
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@11c69843
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5454bd86
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5801acb2
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@64905460
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1bbf0fad
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@afa7b86
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3c0660fa
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@b5fd88e
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4ca762cb
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@78b0898a
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4ad8063c
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@41e33036
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,932 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,931 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,931 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,932 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,932 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,932 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,935 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a), deploy into slot with allocation id 19f79675f5f8f11b63c12b27d486e370.
2022-06-28 14:38:17,935 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 16ca465ad881d3871bafaca1cc217a24.
2022-06-28 14:38:17,935 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,936 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a) [DEPLOYING].
2022-06-28 14:38:17,938 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@63174db
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8), deploy into slot with allocation id 16ca465ad881d3871bafaca1cc217a24.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,938 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,939 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 4dcfaf77e07ba56762b0734215849138.
2022-06-28 14:38:17,939 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,939 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (68fa23854e4c78da22e734bb0465ec70) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,939 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,940 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8) [DEPLOYING].
2022-06-28 14:38:17,941 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@257caf6a
2022-06-28 14:38:17,941 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,942 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,942 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,942 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (588b3654da4a8e24fc1761e7cd6ab5fe) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,942 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb), deploy into slot with allocation id 4dcfaf77e07ba56762b0734215849138.
2022-06-28 14:38:17,942 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (43811564f27f2e17e926f4553282d837) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,942 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (a4da1f9c7f7ad9fbb14e998f779b0919) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,942 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (dbf2a555249d5f02a7f0398ddd5e5488) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,943 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1f749d6bbcd55df2cd9e09624908f25a.
2022-06-28 14:38:17,943 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (703ca70f828036577f24fe6491a57ad8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,943 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (e796a06d1ccb2d0058ce85823ed8f4af) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,943 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,943 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb) [DEPLOYING].
2022-06-28 14:38:17,943 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (3281386b23445b357434006aedeefd9a) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,943 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (8633fa4b6a858b040e007a5ae42cbe35) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,943 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (76ec9c0d22dff0ce2b7466f3b75be708) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,944 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (c3dd6af93e7d308b80433d49b1b48251) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,944 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@570457a1
2022-06-28 14:38:17,944 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (10e43b9a3d92b5d3b04e365469af82d4) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,944 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,944 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,944 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (cd83158945013209139193039556e00a) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,944 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,945 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,945 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (08cea9e6d486665867fc068e4bd073cb) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,945 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d), deploy into slot with allocation id 1f749d6bbcd55df2cd9e09624908f25a.
2022-06-28 14:38:17,946 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b27121af6016a289cdd9aaf3e39a1635.
2022-06-28 14:38:17,948 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c), deploy into slot with allocation id b27121af6016a289cdd9aaf3e39a1635.
2022-06-28 14:38:17,948 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b8f3efa79d8aafc31877fb817a74970f.
2022-06-28 14:38:17,951 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1), deploy into slot with allocation id b8f3efa79d8aafc31877fb817a74970f.
2022-06-28 14:38:17,951 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 056e5cba4db76b1cdb94a4aa47e0d57b.
2022-06-28 14:38:17,953 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,953 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,953 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,953 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d) [DEPLOYING].
2022-06-28 14:38:17,953 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c) [DEPLOYING].
2022-06-28 14:38:17,953 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1) [DEPLOYING].
2022-06-28 14:38:17,954 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5310811d
2022-06-28 14:38:17,955 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6024eb71
2022-06-28 14:38:17,955 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,955 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,955 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,955 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2db87070
2022-06-28 14:38:17,956 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,957 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,957 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,957 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (789c52cf12ce8fb9c8c2f061cc8877c1) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,955 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,957 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,958 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,958 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6571f96578a56cc4f18b269a570cb10d) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,958 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (b0dc8f3f9589a6016e15d7ecbd41188c) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,958 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310), deploy into slot with allocation id 056e5cba4db76b1cdb94a4aa47e0d57b.
2022-06-28 14:38:17,959 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2aad4cea74eb0e9ceea19ac67cbafbb8.
2022-06-28 14:38:17,961 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61), deploy into slot with allocation id 2aad4cea74eb0e9ceea19ac67cbafbb8.
2022-06-28 14:38:17,962 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3431c86c9aabd2a7356c6a9edf0dbe0e.
2022-06-28 14:38:17,964 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee), deploy into slot with allocation id 3431c86c9aabd2a7356c6a9edf0dbe0e.
2022-06-28 14:38:17,965 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 05a7832a4451ba579d3559369dca8087.
2022-06-28 14:38:17,967 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (8490e61e0836c2b961838b054d93c35e), deploy into slot with allocation id 05a7832a4451ba579d3559369dca8087.
2022-06-28 14:38:17,968 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1a427f99ea706922fec97ca4df7cfca0.
2022-06-28 14:38:17,968 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,968 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61) [DEPLOYING].
2022-06-28 14:38:17,968 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,969 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee) [DEPLOYING].
2022-06-28 14:38:17,971 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13), deploy into slot with allocation id 1a427f99ea706922fec97ca4df7cfca0.
2022-06-28 14:38:17,971 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5dffbc19
2022-06-28 14:38:17,971 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,971 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@162e0487
2022-06-28 14:38:17,971 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,971 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,971 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,971 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,971 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,971 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 5a4966e2fe4c788e44a9ed79d2f86162.
2022-06-28 14:38:17,972 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (6866cd47c68fd6e054bf5406aada60ee) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,972 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (133d45f20274bc9005e56649640cff61) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,971 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,973 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310) [DEPLOYING].
2022-06-28 14:38:17,971 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (8490e61e0836c2b961838b054d93c35e) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,973 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (8490e61e0836c2b961838b054d93c35e) [DEPLOYING].
2022-06-28 14:38:17,974 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf), deploy into slot with allocation id 5a4966e2fe4c788e44a9ed79d2f86162.
2022-06-28 14:38:17,974 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,975 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13) [DEPLOYING].
2022-06-28 14:38:17,976 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3d412032
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,976 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3431c86c9aabd2a7356c6a9edf0dbe0e.
2022-06-28 14:38:17,976 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@67264f8
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b8f3efa79d8aafc31877fb817a74970f.
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 19f79675f5f8f11b63c12b27d486e370.
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 5a4966e2fe4c788e44a9ed79d2f86162.
2022-06-28 14:38:17,976 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b27121af6016a289cdd9aaf3e39a1635.
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2aad4cea74eb0e9ceea19ac67cbafbb8.
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 05a7832a4451ba579d3559369dca8087.
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1f749d6bbcd55df2cd9e09624908f25a.
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 1a427f99ea706922fec97ca4df7cfca0.
2022-06-28 14:38:17,977 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 056e5cba4db76b1cdb94a4aa47e0d57b.
2022-06-28 14:38:17,977 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 4dcfaf77e07ba56762b0734215849138.
2022-06-28 14:38:17,976 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf) switched from CREATED to DEPLOYING.
2022-06-28 14:38:17,977 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 16ca465ad881d3871bafaca1cc217a24.
2022-06-28 14:38:17,977 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf) [DEPLOYING].
2022-06-28 14:38:17,978 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@65fdb84e
2022-06-28 14:38:17,978 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,978 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (e974e5b582c9599927ec1249fc8bbf13) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,978 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,978 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@16791b5b
2022-06-28 14:38:17,978 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:38:17,979 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:38:17,978 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (8490e61e0836c2b961838b054d93c35e) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,979 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,978 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (02bf122c0090a0f74d4a6d2f24d97310) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,982 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (8490e61e0836c2b961838b054d93c35e) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,982 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (42a375658a0b731afae537ad15bd5fbf) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:38:17,992 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,992 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,992 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,993 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,993 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,993 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,994 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,994 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,994 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,993 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,994 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:17,994 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:38:18,097 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 10 @ 
2022-06-28 14:38:18,098 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 9 @ 
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,098 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 1 @ 
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (10e43b9a3d92b5d3b04e365469af82d4) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 8 @ 
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 3 @ 
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 7 @ 
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 4 @ 
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (e796a06d1ccb2d0058ce85823ed8f4af) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 11 @ 
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 0 @ 
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 2 @ 
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (c3dd6af93e7d308b80433d49b1b48251) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (703ca70f828036577f24fe6491a57ad8) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (68fa23854e4c78da22e734bb0465ec70) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,099 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 6 @ 
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (588b3654da4a8e24fc1761e7cd6ab5fe) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 5 @ 
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,100 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (76ec9c0d22dff0ce2b7466f3b75be708) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,102 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (8633fa4b6a858b040e007a5ae42cbe35) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,102 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (3281386b23445b357434006aedeefd9a) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,104 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (43811564f27f2e17e926f4553282d837) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,104 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (dbf2a555249d5f02a7f0398ddd5e5488) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,104 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (a4da1f9c7f7ad9fbb14e998f779b0919) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,112 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,111 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:38:18,149 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,149 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,150 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298149
2022-06-28 14:38:18,152 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,152 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,152 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298151
2022-06-28 14:38:18,158 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,158 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,158 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298151
2022-06-28 14:38:18,160 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,161 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,161 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298150
2022-06-28 14:38:18,162 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,162 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,162 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298150
2022-06-28 14:38:18,162 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,162 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,162 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298160
2022-06-28 14:38:18,163 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,163 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,163 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298159
2022-06-28 14:38:18,163 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,163 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,163 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298157
2022-06-28 14:38:18,164 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,164 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,164 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298156
2022-06-28 14:38:18,165 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,165 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,165 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298154
2022-06-28 14:38:18,165 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,165 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,165 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298153
2022-06-28 14:38:18,166 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,166 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,166 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298153
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,175 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:38:18,184 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,185 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,185 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,184 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,184 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,184 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,184 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,184 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,185 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,184 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,185 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,184 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:38:18,233 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,234 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,233 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,235 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (8490e61e0836c2b961838b054d93c35e) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,233 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,234 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,234 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,235 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,235 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,234 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,236 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (42a375658a0b731afae537ad15bd5fbf) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,236 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (8490e61e0836c2b961838b054d93c35e) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,236 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,236 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6571f96578a56cc4f18b269a570cb10d) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,236 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (6866cd47c68fd6e054bf5406aada60ee) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,236 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (02bf122c0090a0f74d4a6d2f24d97310) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,237 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (cd83158945013209139193039556e00a) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,237 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (133d45f20274bc9005e56649640cff61) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,237 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,237 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (e974e5b582c9599927ec1249fc8bbf13) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,237 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (08cea9e6d486665867fc068e4bd073cb) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,238 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (789c52cf12ce8fb9c8c2f061cc8877c1) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,239 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,240 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (b0dc8f3f9589a6016e15d7ecbd41188c) switched from INITIALIZING to RUNNING.
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-6] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-8] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-3] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-1] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-2] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-12] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-7] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-11] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-4] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-5] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-10] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,443 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-9] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,454 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Discovered new partitions: [flink-sql-exercise-0]
2022-06-28 14:38:18,458 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Assigning splits to readers {4=[[Partition: flink-sql-exercise-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
2022-06-28 14:38:18,463 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: flink-sql-exercise-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
2022-06-28 14:38:18,465 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 14:38:18,467 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:38:18,467 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:38:18,467 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:38:18,467 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:38:18,467 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398298467
2022-06-28 14:38:18,474 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2022-06-28 14:38:18,479 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=g5-4, groupId=g5] Subscribed to partition(s): flink-sql-exercise-0
2022-06-28 14:38:18,482 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-4, groupId=g5] Seeking to EARLIEST offset of partition flink-sql-exercise-0
2022-06-28 14:38:18,492 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=g5-4, groupId=g5] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:38:18,504 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-4, groupId=g5] Resetting offset for partition flink-sql-exercise-0 to offset 0.
2022-06-28 14:38:18,674 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,680 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (8490e61e0836c2b961838b054d93c35e) switched from RUNNING to FAILED with failure cause: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56)
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30)
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)

2022-06-28 14:38:18,680 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (8490e61e0836c2b961838b054d93c35e).
2022-06-28 14:38:18,685 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 8490e61e0836c2b961838b054d93c35e.
2022-06-28 14:38:18,687 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (8490e61e0836c2b961838b054d93c35e) switched from RUNNING to FAILED on 4991636a-9fa3-4000-99eb-87c97f8a37a1 @ kubernetes.docker.internal (dataPort=-1).
org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.4.jar:1.14.4]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-06-28 14:38:18,697 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 90bea66de1c231edf33913ecd54406c1_9.
2022-06-28 14:38:18,698 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 24 tasks should be restarted to recover the failed task 90bea66de1c231edf33913ecd54406c1_9. 
2022-06-28 14:38:18,700 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (c0a11dc3ebc0ae5a5dfddd283134dd70) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-runtime-1.14.4.jar:1.14.4]
	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[scala-library-2.12.7.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_221]
Caused by: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.4.jar:1.14.4]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-06-28 14:38:18,707 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (588b3654da4a8e24fc1761e7cd6ab5fe) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,707 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe).
2022-06-28 14:38:18,707 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,707 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe).
2022-06-28 14:38:18,708 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (a4da1f9c7f7ad9fbb14e998f779b0919) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,708 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (68fa23854e4c78da22e734bb0465ec70) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,708 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (8633fa4b6a858b040e007a5ae42cbe35) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,708 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (e796a06d1ccb2d0058ce85823ed8f4af) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,708 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (c3dd6af93e7d308b80433d49b1b48251) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (76ec9c0d22dff0ce2b7466f3b75be708) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,709 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (10e43b9a3d92b5d3b04e365469af82d4) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (3281386b23445b357434006aedeefd9a) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (43811564f27f2e17e926f4553282d837) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (703ca70f828036577f24fe6491a57ad8) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (dbf2a555249d5f02a7f0398ddd5e5488) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (588b3654da4a8e24fc1761e7cd6ab5fe).
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919).
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919).
2022-06-28 14:38:18,709 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (cd83158945013209139193039556e00a) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,710 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,715 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,715 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (08cea9e6d486665867fc068e4bd073cb) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70).
2022-06-28 14:38:18,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70).
2022-06-28 14:38:18,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (a4da1f9c7f7ad9fbb14e998f779b0919).
2022-06-28 14:38:18,715 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6571f96578a56cc4f18b269a570cb10d) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,717 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (b0dc8f3f9589a6016e15d7ecbd41188c) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,718 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,718 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35).
2022-06-28 14:38:18,718 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (789c52cf12ce8fb9c8c2f061cc8877c1) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,718 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,718 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,718 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35).
2022-06-28 14:38:18,718 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (68fa23854e4c78da22e734bb0465ec70).
2022-06-28 14:38:18,719 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (02bf122c0090a0f74d4a6d2f24d97310) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,719 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (133d45f20274bc9005e56649640cff61) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,719 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af).
2022-06-28 14:38:18,719 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (6866cd47c68fd6e054bf5406aada60ee) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,719 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,719 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,719 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af).
2022-06-28 14:38:18,720 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (e974e5b582c9599927ec1249fc8bbf13) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,720 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,720 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (8633fa4b6a858b040e007a5ae42cbe35).
2022-06-28 14:38:18,720 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (42a375658a0b731afae537ad15bd5fbf) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,720 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251).
2022-06-28 14:38:18,720 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,721 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,721 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2022-06-28 14:38:18,721 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251).
2022-06-28 14:38:18,721 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708).
2022-06-28 14:38:18,721 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,721 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,721 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708).
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (c3dd6af93e7d308b80433d49b1b48251).
2022-06-28 14:38:18,722 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4).
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4).
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (76ec9c0d22dff0ce2b7466f3b75be708).
2022-06-28 14:38:18,722 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a).
2022-06-28 14:38:18,722 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,722 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a).
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (10e43b9a3d92b5d3b04e365469af82d4).
2022-06-28 14:38:18,723 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837).
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (e796a06d1ccb2d0058ce85823ed8f4af).
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837).
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (3281386b23445b357434006aedeefd9a).
2022-06-28 14:38:18,723 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8).
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8).
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,723 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (43811564f27f2e17e926f4553282d837).
2022-06-28 14:38:18,724 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488).
2022-06-28 14:38:18,725 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488).
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (703ca70f828036577f24fe6491a57ad8).
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a).
2022-06-28 14:38:18,725 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a).
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (dbf2a555249d5f02a7f0398ddd5e5488).
2022-06-28 14:38:18,725 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 588b3654da4a8e24fc1761e7cd6ab5fe.
2022-06-28 14:38:18,725 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,726 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8).
2022-06-28 14:38:18,726 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,726 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8).
2022-06-28 14:38:18,726 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (588b3654da4a8e24fc1761e7cd6ab5fe) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,726 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb).
2022-06-28 14:38:18,726 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,726 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb).
2022-06-28 14:38:18,726 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,727 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 a4da1f9c7f7ad9fbb14e998f779b0919.
2022-06-28 14:38:18,727 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,728 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (cd83158945013209139193039556e00a).
2022-06-28 14:38:18,728 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,728 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (a4da1f9c7f7ad9fbb14e998f779b0919) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,729 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d).
2022-06-28 14:38:18,729 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,729 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,729 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d).
2022-06-28 14:38:18,729 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (48e8d0a9b0ba1b358bef8421e73aa1e8).
2022-06-28 14:38:18,730 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c).
2022-06-28 14:38:18,730 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,730 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c).
2022-06-28 14:38:18,730 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,730 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,730 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (08cea9e6d486665867fc068e4bd073cb).
2022-06-28 14:38:18,731 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1).
2022-06-28 14:38:18,731 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,731 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1).
2022-06-28 14:38:18,731 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,731 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 68fa23854e4c78da22e734bb0465ec70.
2022-06-28 14:38:18,732 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,732 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6571f96578a56cc4f18b269a570cb10d).
2022-06-28 14:38:18,732 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,732 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310).
2022-06-28 14:38:18,732 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (b0dc8f3f9589a6016e15d7ecbd41188c).
2022-06-28 14:38:18,732 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,733 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,733 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310).
2022-06-28 14:38:18,733 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (68fa23854e4c78da22e734bb0465ec70) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,733 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61).
2022-06-28 14:38:18,733 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,733 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61).
2022-06-28 14:38:18,734 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee).
2022-06-28 14:38:18,734 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,734 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee).
2022-06-28 14:38:18,734 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,734 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13).
2022-06-28 14:38:18,734 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,734 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,734 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,734 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13).
2022-06-28 14:38:18,734 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (789c52cf12ce8fb9c8c2f061cc8877c1).
2022-06-28 14:38:18,735 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,735 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 8633fa4b6a858b040e007a5ae42cbe35.
2022-06-28 14:38:18,735 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,736 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf).
2022-06-28 14:38:18,736 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf) switched from RUNNING to CANCELING.
2022-06-28 14:38:18,736 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf).
2022-06-28 14:38:18,736 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (8633fa4b6a858b040e007a5ae42cbe35) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,736 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 c3dd6af93e7d308b80433d49b1b48251.
2022-06-28 14:38:18,736 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:38:18,737 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 76ec9c0d22dff0ce2b7466f3b75be708.
2022-06-28 14:38:18,737 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (c3dd6af93e7d308b80433d49b1b48251) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,737 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 10e43b9a3d92b5d3b04e365469af82d4.
2022-06-28 14:38:18,737 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,737 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (133d45f20274bc9005e56649640cff61).
2022-06-28 14:38:18,737 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,737 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 e796a06d1ccb2d0058ce85823ed8f4af.
2022-06-28 14:38:18,737 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (6866cd47c68fd6e054bf5406aada60ee).
2022-06-28 14:38:18,737 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (76ec9c0d22dff0ce2b7466f3b75be708) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 3281386b23445b357434006aedeefd9a.
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 43811564f27f2e17e926f4553282d837.
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (02bf122c0090a0f74d4a6d2f24d97310).
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 703ca70f828036577f24fe6491a57ad8.
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (10e43b9a3d92b5d3b04e365469af82d4) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 dbf2a555249d5f02a7f0398ddd5e5488.
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (e796a06d1ccb2d0058ce85823ed8f4af) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 cd83158945013209139193039556e00a.
2022-06-28 14:38:18,738 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,739 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (42a375658a0b731afae537ad15bd5fbf).
2022-06-28 14:38:18,739 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 48e8d0a9b0ba1b358bef8421e73aa1e8.
2022-06-28 14:38:18,739 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,739 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (e974e5b582c9599927ec1249fc8bbf13).
2022-06-28 14:38:18,739 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 08cea9e6d486665867fc068e4bd073cb.
2022-06-28 14:38:18,739 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (3281386b23445b357434006aedeefd9a) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,739 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 6571f96578a56cc4f18b269a570cb10d.
2022-06-28 14:38:18,740 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 b0dc8f3f9589a6016e15d7ecbd41188c.
2022-06-28 14:38:18,740 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (43811564f27f2e17e926f4553282d837) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,740 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 789c52cf12ce8fb9c8c2f061cc8877c1.
2022-06-28 14:38:18,740 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 133d45f20274bc9005e56649640cff61.
2022-06-28 14:38:18,740 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 6866cd47c68fd6e054bf5406aada60ee.
2022-06-28 14:38:18,740 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 02bf122c0090a0f74d4a6d2f24d97310.
2022-06-28 14:38:18,741 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 42a375658a0b731afae537ad15bd5fbf.
2022-06-28 14:38:18,741 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 e974e5b582c9599927ec1249fc8bbf13.
2022-06-28 14:38:18,741 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=11}]
2022-06-28 14:38:18,742 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (703ca70f828036577f24fe6491a57ad8) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,742 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (dbf2a555249d5f02a7f0398ddd5e5488) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,743 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (cd83158945013209139193039556e00a) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,743 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=10}]
2022-06-28 14:38:18,743 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (48e8d0a9b0ba1b358bef8421e73aa1e8) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,744 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=9}]
2022-06-28 14:38:18,744 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (08cea9e6d486665867fc068e4bd073cb) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,745 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=8}]
2022-06-28 14:38:18,745 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6571f96578a56cc4f18b269a570cb10d) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,746 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=7}]
2022-06-28 14:38:18,746 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (b0dc8f3f9589a6016e15d7ecbd41188c) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,746 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
2022-06-28 14:38:18,747 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (789c52cf12ce8fb9c8c2f061cc8877c1) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,747 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
2022-06-28 14:38:18,747 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (133d45f20274bc9005e56649640cff61) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,747 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
2022-06-28 14:38:18,747 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (6866cd47c68fd6e054bf5406aada60ee) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,748 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
2022-06-28 14:38:18,748 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (02bf122c0090a0f74d4a6d2f24d97310) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,748 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
2022-06-28 14:38:18,748 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (42a375658a0b731afae537ad15bd5fbf) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,749 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c0a11dc3ebc0ae5a5dfddd283134dd70: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
2022-06-28 14:38:18,749 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (e974e5b582c9599927ec1249fc8bbf13) switched from CANCELING to CANCELED.
2022-06-28 14:38:18,750 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job c0a11dc3ebc0ae5a5dfddd283134dd70
2022-06-28 14:38:18,749 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (c0a11dc3ebc0ae5a5dfddd283134dd70) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-runtime-1.14.4.jar:1.14.4]
	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[scala-library-2.12.7.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_221]
Caused by: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.4.jar:1.14.4]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-06-28 14:38:18,754 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job c0a11dc3ebc0ae5a5dfddd283134dd70.
2022-06-28 14:38:18,765 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Shutting down Flink Mini Cluster
2022-06-28 14:38:18,765 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job c0a11dc3ebc0ae5a5dfddd283134dd70 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56)
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30)
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)
2022-06-28 14:38:18,765 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Shutting down rest endpoint.
2022-06-28 14:38:18,765 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
2022-06-28 14:38:18,765 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close ResourceManager connection 5f31f5c55f67e114e51b71f450cb06b2.
2022-06-28 14:38:18,766 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Closing TaskExecutor connection 4991636a-9fa3-4000-99eb-87c97f8a37a1 because: The TaskExecutor is shutting down.
2022-06-28 14:38:18,766 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job c0a11dc3ebc0ae5a5dfddd283134dd70.
2022-06-28 14:38:18,768 INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager [] - Shutting down TaskExecutorStateChangelogStoragesManager.
2022-06-28 14:38:18,768 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,769 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:3, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 3431c86c9aabd2a7356c6a9edf0dbe0e, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,770 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:4, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: b8f3efa79d8aafc31877fb817a74970f, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,770 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:2, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 19f79675f5f8f11b63c12b27d486e370, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,770 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise.
2022-06-28 14:38:18,770 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:8, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 5a4966e2fe4c788e44a9ed79d2f86162, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,771 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:9, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: b27121af6016a289cdd9aaf3e39a1635, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,771 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 2aad4cea74eb0e9ceea19ac67cbafbb8, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,771 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:5, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 05a7832a4451ba579d3559369dca8087, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,772 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:10, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 1f749d6bbcd55df2cd9e09624908f25a, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,772 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:11, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 1a427f99ea706922fec97ca4df7cfca0, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,772 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:1, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 056e5cba4db76b1cdb94a4aa47e0d57b, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,772 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:6, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 4dcfaf77e07ba56762b0734215849138, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,772 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:7, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 16ca465ad881d3871bafaca1cc217a24, jobId: c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,777 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [3431c86c9aabd2a7356c6a9edf0dbe0e].
2022-06-28 14:38:18,777 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise closed.
2022-06-28 14:38:18,780 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [b8f3efa79d8aafc31877fb817a74970f].
2022-06-28 14:38:18,780 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
2022-06-28 14:38:18,780 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2022-06-28 14:38:18,780 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [19f79675f5f8f11b63c12b27d486e370].
2022-06-28 14:38:18,780 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [5a4966e2fe4c788e44a9ed79d2f86162].
2022-06-28 14:38:18,780 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [b27121af6016a289cdd9aaf3e39a1635].
2022-06-28 14:38:18,781 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [2aad4cea74eb0e9ceea19ac67cbafbb8].
2022-06-28 14:38:18,780 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Removing cache directory C:\Users\BONC\AppData\Local\Temp\flink-web-ui
2022-06-28 14:38:18,781 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [05a7832a4451ba579d3559369dca8087].
2022-06-28 14:38:18,781 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [1f749d6bbcd55df2cd9e09624908f25a].
2022-06-28 14:38:18,781 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [1a427f99ea706922fec97ca4df7cfca0].
2022-06-28 14:38:18,781 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [056e5cba4db76b1cdb94a4aa47e0d57b].
2022-06-28 14:38:18,781 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [4dcfaf77e07ba56762b0734215849138].
2022-06-28 14:38:18,781 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [16ca465ad881d3871bafaca1cc217a24].
2022-06-28 14:38:18,781 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2022-06-28 14:38:18,782 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 5f31f5c55f67e114e51b71f450cb06b2: Stopping JobMaster for job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (c0a11dc3ebc0ae5a5dfddd283134dd70).
2022-06-28 14:38:18,782 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager a262b6f568c105f8c06c88cd71f94a72@akka://flink/user/rpc/jobmanager_3 for job c0a11dc3ebc0ae5a5dfddd283134dd70 from the resource manager.
2022-06-28 14:38:18,784 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Shut down complete.
2022-06-28 14:38:18,786 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
2022-06-28 14:38:18,786 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.
2022-06-28 14:38:18,786 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.
2022-06-28 14:38:18,787 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping dispatcher akka://flink/user/rpc/dispatcher_1.
2022-06-28 14:38:18,787 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.
2022-06-28 14:38:18,787 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_1.
2022-06-28 14:38:18,787 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.
2022-06-28 14:38:18,787 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.
2022-06-28 14:38:18,790 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka://flink/user/rpc/dispatcher_1.
2022-06-28 14:38:18,790 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory C:\Users\BONC\AppData\Local\Temp\flink-io-37bde3db-dc5c-400f-9394-f55a97f5f44a
2022-06-28 14:38:18,790 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Shutting down the network environment and its components.
2022-06-28 14:38:18,791 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory C:\Users\BONC\AppData\Local\Temp\flink-netty-shuffle-9c2c4326-6983-4cc6-84cf-4ee4eb3caf17
2022-06-28 14:38:18,791 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Shutting down the kvState service and its components.
2022-06-28 14:38:18,791 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
2022-06-28 14:38:18,792 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory C:\Users\BONC\AppData\Local\Temp\flink-dist-cache-d3d807e2-3e5e-44ad-8053-1668da8c14f5
2022-06-28 14:38:18,792 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
2022-06-28 14:38:18,792 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2022-06-28 14:38:18,826 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2022-06-28 14:38:18,827 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2022-06-28 14:38:18,834 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2022-06-28 14:38:18,835 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2022-06-28 14:38:18,836 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:49320
2022-06-28 14:38:18,839 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2022-06-28 14:43:59,512 WARN  org.apache.flink.connector.kafka.sink.KafkaSinkBuilder       [] - Property [transaction.timeout.ms] not specified. Setting it to PT1H
2022-06-28 14:43:59,798 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:43:59,798 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:43:59,798 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:43:59,798 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 14:43:59,798 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 14:43:59,799 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2022-06-28 14:43:59,802 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Flink Mini Cluster
2022-06-28 14:44:00,115 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Metrics Registry
2022-06-28 14:44:00,156 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
2022-06-28 14:44:00,156 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting RPC Service(s)
2022-06-28 14:44:00,166 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 14:44:00,635 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 14:44:00,714 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink
2022-06-28 14:44:00,725 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 14:44:00,732 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 14:44:00,739 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink-metrics
2022-06-28 14:44:00,747 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2022-06-28 14:44:00,759 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting high-availability services
2022-06-28 14:44:00,767 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-934a1fb0-beee-42a8-8716-cac447ac1758
2022-06-28 14:44:00,770 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:49733 - max concurrent requests: 50 - max backlog: 1000
2022-06-28 14:44:00,773 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-2cdffac6-2fc8-4ce6-ab07-61f06f35fbe8
2022-06-28 14:44:00,774 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-5cbfb911-a915-4489-b784-795da823e875
2022-06-28 14:44:00,774 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting 1 TaskManager(s)
2022-06-28 14:44:00,777 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: d277093a-f319-466a-ba38-eaf4e0d771aa
2022-06-28 14:44:00,785 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory 'C:\Users\BONC\AppData\Local\Temp': total 237 GB, usable 176 GB (74.26% usable)
2022-06-28 14:44:00,788 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-io-c67e065b-c1c3-4902-aa14-0047b7205432
2022-06-28 14:44:00,793 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-netty-shuffle-86dd76a7-38bf-41ce-9814-0f16ec0431ab
2022-06-28 14:44:00,819 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2022-06-28 14:44:00,827 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
2022-06-28 14:44:00,828 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
2022-06-28 14:44:00,836 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'akka.ask.timeout' instead of key 'taskmanager.slot.timeout'
2022-06-28 14:44:00,846 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2022-06-28 14:44:00,855 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
2022-06-28 14:44:00,857 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory C:\Users\BONC\AppData\Local\Temp\flink-dist-cache-b7f40e67-d098-4cbc-9028-d5e3d98758eb
2022-06-28 14:44:00,889 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Starting rest endpoint.
2022-06-28 14:44:00,891 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
2022-06-28 14:44:00,945 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Log file environment variable 'log.file' is not set.
2022-06-28 14:44:00,945 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
2022-06-28 14:44:01,180 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Rest endpoint listening at localhost:49789
2022-06-28 14:44:01,181 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender http://localhost:49789
2022-06-28 14:44:01,182 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - http://localhost:49789 was granted leadership with leaderSessionID=f68659d2-1d41-449a-b824-5c300ad8d61e
2022-06-28 14:44:01,183 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader http://localhost:49789 , session=f68659d2-1d41-449a-b824-5c300ad8d61e
2022-06-28 14:44:01,191 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2022-06-28 14:44:01,192 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
2022-06-28 14:44:01,192 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: ResourceManagerServiceImpl
2022-06-28 14:44:01,192 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id fa550b90-d54a-402c-8f2a-5ceb03029ada. Creating new DispatcherLeaderProcess.
2022-06-28 14:44:01,193 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 15a91282-1e2e-41c2-8120-8c957818bf9a.
2022-06-28 14:44:01,195 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Flink Mini Cluster started successfully
2022-06-28 14:44:01,196 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
2022-06-28 14:44:01,198 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.
2022-06-28 14:44:01,198 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
2022-06-28 14:44:01,203 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
2022-06-28 14:44:01,207 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_2 .
2022-06-28 14:44:01,212 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_1 , session=fa550b90-d54a-402c-8f2a-5ceb03029ada
2022-06-28 14:44:01,213 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
2022-06-28 14:44:01,219 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_2 , session=15a91282-1e2e-41c2-8120-8c957818bf9a
2022-06-28 14:44:01,220 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(81208c957818bf9a15a912821e2e41c2).
2022-06-28 14:44:01,236 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
2022-06-28 14:44:01,241 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID d277093a-f319-466a-ba38-eaf4e0d771aa (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2022-06-28 14:44:01,243 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_2 under registration id 6541dd5a0495704d06ba0649f31a581d.
2022-06-28 14:44:01,243 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (3a4d58937706269a729bf00d426e0487).
2022-06-28 14:44:01,244 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (3a4d58937706269a729bf00d426e0487).
2022-06-28 14:44:01,255 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2022-06-28 14:44:01,264 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2022-06-28 14:44:01,270 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (3a4d58937706269a729bf00d426e0487).
2022-06-28 14:44:01,285 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for insert-into_default_catalog.default_database.t_kafka_nicekcnt (3a4d58937706269a729bf00d426e0487).
2022-06-28 14:44:01,313 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job insert-into_default_catalog.default_database.t_kafka_nicekcnt (3a4d58937706269a729bf00d426e0487).
2022-06-28 14:44:01,313 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
2022-06-28 14:44:01,359 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 0 ms
2022-06-28 14:44:01,379 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@70038ea0
2022-06-28 14:44:01,380 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,381 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,396 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
2022-06-28 14:44:01,401 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@6cb83b0c for insert-into_default_catalog.default_database.t_kafka_nicekcnt (3a4d58937706269a729bf00d426e0487).
2022-06-28 14:44:01,411 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=86259c17-aef6-4a25-b3bd-f119389e7830
2022-06-28 14:44:01,413 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (3a4d58937706269a729bf00d426e0487) under job master id b3bdf119389e783086259c17aef64a25.
2022-06-28 14:44:01,415 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise.
2022-06-28 14:44:01,417 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2022-06-28 14:44:01,418 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (3a4d58937706269a729bf00d426e0487) switched from state CREATED to RUNNING.
2022-06-28 14:44:01,422 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (a90cb4a90d2ec8f48dbde140c2749708) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,422 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (7fbbc7153457fe98404aca7e8b293fd7) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,422 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (8a769820c004f86329ebbd7cdcf3a26b) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,422 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (184940351eb857cb44fc42a3400a06ab) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (a6a048a35cc4145bcb676dc0b1c1ec85) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (8b92662f4cf07debb1ae862f3aa5fb47) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (0bacd7d0c254ae11ecef8d35382b122e) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (09606db825c01cd71a6346ad184d4e3c) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (31897c91cadc4cad1c20a7e6a097194e) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cf6d410a1028c1a1136dd0cc27a1966d) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (5f2b7791a862b7cd9ac4f344b93eff63) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (a04b2c623f0d4d0fb335ccdd1774c791) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (aef607bec3e611ffd83296a426749800) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (69e59a80055d361d94618f561d2078b5) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6face129aa759c7987c9f952cb0f7ab7) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (980405b316bc33a68169df2a2b12c477) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (fcbf1da941a22b06efd9e9106d73becc) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (aba2612c0c27557f219ff8ecc4f8605a) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (1aa02148bae7ca5a913b3a6fc8628cfa) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,423 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (c3a38636d1100036e03ad4ea751cabd1) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,424 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (09526dde3e83233f7ef357797fd10a9f) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,424 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (f4302e97a7541aa074dc4368fac3a254) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,424 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (3de34239e7c4f135aa5158d6472b10e8) switched from CREATED to SCHEDULED.
2022-06-28 14:44:01,440 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(81208c957818bf9a15a912821e2e41c2)
2022-06-28 14:44:01,440 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 14:44:01,442 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
2022-06-28 14:44:01,444 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager b3bdf119389e783086259c17aef64a25@akka://flink/user/rpc/jobmanager_3 for job 3a4d58937706269a729bf00d426e0487.
2022-06-28 14:44:01,449 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager b3bdf119389e783086259c17aef64a25@akka://flink/user/rpc/jobmanager_3 for job 3a4d58937706269a729bf00d426e0487.
2022-06-28 14:44:01,453 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,457 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=12}]
2022-06-28 14:44:01,461 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request b98077ce2bd445da868d27a6f4abcebf for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,465 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for b98077ce2bd445da868d27a6f4abcebf.
2022-06-28 14:44:01,465 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 3a4d58937706269a729bf00d426e0487 for job leader monitoring.
2022-06-28 14:44:01,467 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 86259c17-aef6-4a25-b3bd-f119389e7830.
2022-06-28 14:44:01,468 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 2a8a78cf52a5cc6e1ffa5cfce6b12db1 for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,469 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 2a8a78cf52a5cc6e1ffa5cfce6b12db1.
2022-06-28 14:44:01,470 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 79d117787b25fbf484c76080d516086a for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,470 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2022-06-28 14:44:01,470 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 79d117787b25fbf484c76080d516086a.
2022-06-28 14:44:01,470 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 56895f35ec1339ddc839ae79485bd6c5 for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,471 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 56895f35ec1339ddc839ae79485bd6c5.
2022-06-28 14:44:01,471 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request d93b95d39795a65f30994d835c6574d2 for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,471 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for d93b95d39795a65f30994d835c6574d2.
2022-06-28 14:44:01,471 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request a53a1cfc8729b9acf74275b7b155a532 for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,471 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for a53a1cfc8729b9acf74275b7b155a532.
2022-06-28 14:44:01,472 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 5de667ff477b00660d08060d6f22e4a5 for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,472 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 5de667ff477b00660d08060d6f22e4a5.
2022-06-28 14:44:01,472 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 612909d08dc9ab51577d6bfc875c62e1 for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,472 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 612909d08dc9ab51577d6bfc875c62e1.
2022-06-28 14:44:01,472 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request fb4e1f5642ece6493c04e1647c0534a5 for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,473 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for fb4e1f5642ece6493c04e1647c0534a5.
2022-06-28 14:44:01,473 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 83c799e37aaf2a77c3e924add328130c for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,473 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 83c799e37aaf2a77c3e924add328130c.
2022-06-28 14:44:01,473 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 8a6b2dfe7060ffbfef980f2f86173c26 for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,473 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 8a6b2dfe7060ffbfef980f2f86173c26.
2022-06-28 14:44:01,473 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request f9d9ea68ee51e17c5085add57e7148c4 for job 3a4d58937706269a729bf00d426e0487 from resource manager with leader id 81208c957818bf9a15a912821e2e41c2.
2022-06-28 14:44:01,474 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for f9d9ea68ee51e17c5085add57e7148c4.
2022-06-28 14:44:01,475 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 3a4d58937706269a729bf00d426e0487.
2022-06-28 14:44:01,475 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 3a4d58937706269a729bf00d426e0487.
2022-06-28 14:44:01,478 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 3a4d58937706269a729bf00d426e0487.
2022-06-28 14:44:01,488 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (a90cb4a90d2ec8f48dbde140c2749708) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,489 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (attempt #0) with attempt id a90cb4a90d2ec8f48dbde140c2749708 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id d93b95d39795a65f30994d835c6574d2
2022-06-28 14:44:01,492 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (7fbbc7153457fe98404aca7e8b293fd7) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,492 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (attempt #0) with attempt id 7fbbc7153457fe98404aca7e8b293fd7 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 612909d08dc9ab51577d6bfc875c62e1
2022-06-28 14:44:01,493 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d93b95d39795a65f30994d835c6574d2.
2022-06-28 14:44:01,493 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (8a769820c004f86329ebbd7cdcf3a26b) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,493 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (attempt #0) with attempt id 8a769820c004f86329ebbd7cdcf3a26b to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id f9d9ea68ee51e17c5085add57e7148c4
2022-06-28 14:44:01,493 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (184940351eb857cb44fc42a3400a06ab) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,493 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (attempt #0) with attempt id 184940351eb857cb44fc42a3400a06ab to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 8a6b2dfe7060ffbfef980f2f86173c26
2022-06-28 14:44:01,493 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (a6a048a35cc4145bcb676dc0b1c1ec85) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,493 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (attempt #0) with attempt id a6a048a35cc4145bcb676dc0b1c1ec85 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 83c799e37aaf2a77c3e924add328130c
2022-06-28 14:44:01,494 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (8b92662f4cf07debb1ae862f3aa5fb47) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,494 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (attempt #0) with attempt id 8b92662f4cf07debb1ae862f3aa5fb47 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 56895f35ec1339ddc839ae79485bd6c5
2022-06-28 14:44:01,494 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (0bacd7d0c254ae11ecef8d35382b122e) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,494 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (attempt #0) with attempt id 0bacd7d0c254ae11ecef8d35382b122e to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id b98077ce2bd445da868d27a6f4abcebf
2022-06-28 14:44:01,495 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (09606db825c01cd71a6346ad184d4e3c) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,495 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (attempt #0) with attempt id 09606db825c01cd71a6346ad184d4e3c to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 5de667ff477b00660d08060d6f22e4a5
2022-06-28 14:44:01,495 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (31897c91cadc4cad1c20a7e6a097194e) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,495 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (attempt #0) with attempt id 31897c91cadc4cad1c20a7e6a097194e to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 79d117787b25fbf484c76080d516086a
2022-06-28 14:44:01,496 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cf6d410a1028c1a1136dd0cc27a1966d) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,496 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (attempt #0) with attempt id cf6d410a1028c1a1136dd0cc27a1966d to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id a53a1cfc8729b9acf74275b7b155a532
2022-06-28 14:44:01,496 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (5f2b7791a862b7cd9ac4f344b93eff63) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,496 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (attempt #0) with attempt id 5f2b7791a862b7cd9ac4f344b93eff63 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 2a8a78cf52a5cc6e1ffa5cfce6b12db1
2022-06-28 14:44:01,496 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (a04b2c623f0d4d0fb335ccdd1774c791) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,496 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (attempt #0) with attempt id a04b2c623f0d4d0fb335ccdd1774c791 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id fb4e1f5642ece6493c04e1647c0534a5
2022-06-28 14:44:01,496 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (aef607bec3e611ffd83296a426749800) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,496 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (attempt #0) with attempt id aef607bec3e611ffd83296a426749800 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id d93b95d39795a65f30994d835c6574d2
2022-06-28 14:44:01,502 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (69e59a80055d361d94618f561d2078b5) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,502 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (attempt #0) with attempt id 69e59a80055d361d94618f561d2078b5 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 612909d08dc9ab51577d6bfc875c62e1
2022-06-28 14:44:01,502 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,502 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (attempt #0) with attempt id c93a6ecd4fe7c34a9ffa83bcef97bb48 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id f9d9ea68ee51e17c5085add57e7148c4
2022-06-28 14:44:01,502 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6face129aa759c7987c9f952cb0f7ab7) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,502 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (attempt #0) with attempt id 6face129aa759c7987c9f952cb0f7ab7 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 8a6b2dfe7060ffbfef980f2f86173c26
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (980405b316bc33a68169df2a2b12c477) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (attempt #0) with attempt id 980405b316bc33a68169df2a2b12c477 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 83c799e37aaf2a77c3e924add328130c
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (fcbf1da941a22b06efd9e9106d73becc) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (attempt #0) with attempt id fcbf1da941a22b06efd9e9106d73becc to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 56895f35ec1339ddc839ae79485bd6c5
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (aba2612c0c27557f219ff8ecc4f8605a) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (attempt #0) with attempt id aba2612c0c27557f219ff8ecc4f8605a to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id b98077ce2bd445da868d27a6f4abcebf
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (1aa02148bae7ca5a913b3a6fc8628cfa) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (attempt #0) with attempt id 1aa02148bae7ca5a913b3a6fc8628cfa to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 5de667ff477b00660d08060d6f22e4a5
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (c3a38636d1100036e03ad4ea751cabd1) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (attempt #0) with attempt id c3a38636d1100036e03ad4ea751cabd1 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 79d117787b25fbf484c76080d516086a
2022-06-28 14:44:01,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (09526dde3e83233f7ef357797fd10a9f) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (attempt #0) with attempt id 09526dde3e83233f7ef357797fd10a9f to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id a53a1cfc8729b9acf74275b7b155a532
2022-06-28 14:44:01,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (f4302e97a7541aa074dc4368fac3a254) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (attempt #0) with attempt id f4302e97a7541aa074dc4368fac3a254 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id 2a8a78cf52a5cc6e1ffa5cfce6b12db1
2022-06-28 14:44:01,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (3de34239e7c4f135aa5158d6472b10e8) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:44:01,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (attempt #0) with attempt id 3de34239e7c4f135aa5158d6472b10e8 to d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1) with allocation id fb4e1f5642ece6493c04e1647c0534a5
2022-06-28 14:44:01,504 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory}.
2022-06-28 14:44:01,504 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2022-06-28 14:44:01,511 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:44:01,511 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:44:01,512 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,513 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,513 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641511
2022-06-28 14:44:01,514 INFO  org.apache.kafka.clients.admin.AdminClientConfig             [] - AdminClientConfig values: 
	bootstrap.servers = [192.168.136.130:9092]
	client.dns.lookup = default
	client.id = g5-enumerator-admin-client
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-06-28 14:44:01,519 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708), deploy into slot with allocation id d93b95d39795a65f30994d835c6574d2.
2022-06-28 14:44:01,520 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,521 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 612909d08dc9ab51577d6bfc875c62e1.
2022-06-28 14:44:01,523 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7), deploy into slot with allocation id 612909d08dc9ab51577d6bfc875c62e1.
2022-06-28 14:44:01,524 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708) [DEPLOYING].
2022-06-28 14:44:01,524 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f9d9ea68ee51e17c5085add57e7148c4.
2022-06-28 14:44:01,524 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,524 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7) [DEPLOYING].
2022-06-28 14:44:01,526 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b), deploy into slot with allocation id f9d9ea68ee51e17c5085add57e7148c4.
2022-06-28 14:44:01,527 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 8a6b2dfe7060ffbfef980f2f86173c26.
2022-06-28 14:44:01,527 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,527 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b) [DEPLOYING].
2022-06-28 14:44:01,529 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab), deploy into slot with allocation id 8a6b2dfe7060ffbfef980f2f86173c26.
2022-06-28 14:44:01,529 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 83c799e37aaf2a77c3e924add328130c.
2022-06-28 14:44:01,529 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,529 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab) [DEPLOYING].
2022-06-28 14:44:01,531 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85), deploy into slot with allocation id 83c799e37aaf2a77c3e924add328130c.
2022-06-28 14:44:01,532 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 56895f35ec1339ddc839ae79485bd6c5.
2022-06-28 14:44:01,532 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,532 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85) [DEPLOYING].
2022-06-28 14:44:01,535 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47), deploy into slot with allocation id 56895f35ec1339ddc839ae79485bd6c5.
2022-06-28 14:44:01,536 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b98077ce2bd445da868d27a6f4abcebf.
2022-06-28 14:44:01,536 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,536 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47) [DEPLOYING].
2022-06-28 14:44:01,538 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'key.deserializer' was supplied but isn't a known config.
2022-06-28 14:44:01,538 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'value.deserializer' was supplied but isn't a known config.
2022-06-28 14:44:01,538 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'enable.auto.commit' was supplied but isn't a known config.
2022-06-28 14:44:01,538 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'group.id' was supplied but isn't a known config.
2022-06-28 14:44:01,538 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:44:01,538 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:44:01,538 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2022-06-28 14:44:01,538 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,538 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,538 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e), deploy into slot with allocation id b98077ce2bd445da868d27a6f4abcebf.
2022-06-28 14:44:01,538 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641538
2022-06-28 14:44:01,539 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 5de667ff477b00660d08060d6f22e4a5.
2022-06-28 14:44:01,539 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,539 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Starting the KafkaSourceEnumerator for consumer group g5 without periodic partition discovery.
2022-06-28 14:44:01,539 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e) [DEPLOYING].
2022-06-28 14:44:01,541 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c), deploy into slot with allocation id 5de667ff477b00660d08060d6f22e4a5.
2022-06-28 14:44:01,542 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 79d117787b25fbf484c76080d516086a.
2022-06-28 14:44:01,541 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,542 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c) [DEPLOYING].
2022-06-28 14:44:01,544 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e), deploy into slot with allocation id 79d117787b25fbf484c76080d516086a.
2022-06-28 14:44:01,545 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a53a1cfc8729b9acf74275b7b155a532.
2022-06-28 14:44:01,545 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,546 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e) [DEPLOYING].
2022-06-28 14:44:01,548 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d), deploy into slot with allocation id a53a1cfc8729b9acf74275b7b155a532.
2022-06-28 14:44:01,549 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2a8a78cf52a5cc6e1ffa5cfce6b12db1.
2022-06-28 14:44:01,549 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,549 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d) [DEPLOYING].
2022-06-28 14:44:01,553 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63), deploy into slot with allocation id 2a8a78cf52a5cc6e1ffa5cfce6b12db1.
2022-06-28 14:44:01,553 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot fb4e1f5642ece6493c04e1647c0534a5.
2022-06-28 14:44:01,555 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791), deploy into slot with allocation id fb4e1f5642ece6493c04e1647c0534a5.
2022-06-28 14:44:01,556 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d93b95d39795a65f30994d835c6574d2.
2022-06-28 14:44:01,554 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,558 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63) [DEPLOYING].
2022-06-28 14:44:01,558 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,559 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791) [DEPLOYING].
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@47efaee
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4472192a
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@26a6c10f
2022-06-28 14:44:01,563 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,563 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,563 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@33a5da1
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,563 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@eb73dd5
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,563 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1ac2401d
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6125a247
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@658a68ab
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@48e6b791
2022-06-28 14:44:01,563 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,563 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,563 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,563 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,564 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,564 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,564 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@12e5f804
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4fb1f033
2022-06-28 14:44:01,564 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2da2503a
2022-06-28 14:44:01,564 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,563 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,564 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,564 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,564 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,564 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,564 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,568 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800), deploy into slot with allocation id d93b95d39795a65f30994d835c6574d2.
2022-06-28 14:44:01,568 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 612909d08dc9ab51577d6bfc875c62e1.
2022-06-28 14:44:01,569 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,569 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,569 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,569 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,569 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,569 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,569 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,570 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,570 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800) [DEPLOYING].
2022-06-28 14:44:01,571 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,572 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,572 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5), deploy into slot with allocation id 612909d08dc9ab51577d6bfc875c62e1.
2022-06-28 14:44:01,572 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,573 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,573 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f9d9ea68ee51e17c5085add57e7148c4.
2022-06-28 14:44:01,573 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (09606db825c01cd71a6346ad184d4e3c) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,572 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,573 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,573 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5) [DEPLOYING].
2022-06-28 14:44:01,573 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4abc86a5
2022-06-28 14:44:01,574 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,574 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1aee803d
2022-06-28 14:44:01,574 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,574 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,574 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,575 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,575 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (8b92662f4cf07debb1ae862f3aa5fb47) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,575 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,575 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (184940351eb857cb44fc42a3400a06ab) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,575 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (a90cb4a90d2ec8f48dbde140c2749708) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,575 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48), deploy into slot with allocation id f9d9ea68ee51e17c5085add57e7148c4.
2022-06-28 14:44:01,575 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (7fbbc7153457fe98404aca7e8b293fd7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,575 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (31897c91cadc4cad1c20a7e6a097194e) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,576 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (0bacd7d0c254ae11ecef8d35382b122e) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,576 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (a6a048a35cc4145bcb676dc0b1c1ec85) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,576 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 8a6b2dfe7060ffbfef980f2f86173c26.
2022-06-28 14:44:01,576 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cf6d410a1028c1a1136dd0cc27a1966d) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,576 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (a04b2c623f0d4d0fb335ccdd1774c791) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,577 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (8a769820c004f86329ebbd7cdcf3a26b) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,577 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (5f2b7791a862b7cd9ac4f344b93eff63) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,577 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (aef607bec3e611ffd83296a426749800) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,577 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (69e59a80055d361d94618f561d2078b5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,576 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,578 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48) [DEPLOYING].
2022-06-28 14:44:01,578 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7), deploy into slot with allocation id 8a6b2dfe7060ffbfef980f2f86173c26.
2022-06-28 14:44:01,579 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@60c327f4
2022-06-28 14:44:01,579 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,579 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,579 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 83c799e37aaf2a77c3e924add328130c.
2022-06-28 14:44:01,579 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,579 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7) [DEPLOYING].
2022-06-28 14:44:01,579 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,580 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@68e820ef
2022-06-28 14:44:01,580 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,580 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,580 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,580 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,581 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6face129aa759c7987c9f952cb0f7ab7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,582 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477), deploy into slot with allocation id 83c799e37aaf2a77c3e924add328130c.
2022-06-28 14:44:01,583 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 56895f35ec1339ddc839ae79485bd6c5.
2022-06-28 14:44:01,586 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc), deploy into slot with allocation id 56895f35ec1339ddc839ae79485bd6c5.
2022-06-28 14:44:01,586 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,586 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477) [DEPLOYING].
2022-06-28 14:44:01,586 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b98077ce2bd445da868d27a6f4abcebf.
2022-06-28 14:44:01,587 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@18e88a7e
2022-06-28 14:44:01,587 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,587 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,587 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,587 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc) [DEPLOYING].
2022-06-28 14:44:01,587 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,588 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (980405b316bc33a68169df2a2b12c477) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,588 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@15df1b7b
2022-06-28 14:44:01,588 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,588 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,588 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,589 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (fcbf1da941a22b06efd9e9106d73becc) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,590 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a), deploy into slot with allocation id b98077ce2bd445da868d27a6f4abcebf.
2022-06-28 14:44:01,590 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 5de667ff477b00660d08060d6f22e4a5.
2022-06-28 14:44:01,592 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa), deploy into slot with allocation id 5de667ff477b00660d08060d6f22e4a5.
2022-06-28 14:44:01,593 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 79d117787b25fbf484c76080d516086a.
2022-06-28 14:44:01,595 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1), deploy into slot with allocation id 79d117787b25fbf484c76080d516086a.
2022-06-28 14:44:01,595 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a53a1cfc8729b9acf74275b7b155a532.
2022-06-28 14:44:01,596 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,596 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a) [DEPLOYING].
2022-06-28 14:44:01,597 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,597 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa) [DEPLOYING].
2022-06-28 14:44:01,597 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@53f5261c
2022-06-28 14:44:01,597 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,598 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,598 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,598 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1) [DEPLOYING].
2022-06-28 14:44:01,599 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,599 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@72aa24bf
2022-06-28 14:44:01,599 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (09526dde3e83233f7ef357797fd10a9f), deploy into slot with allocation id a53a1cfc8729b9acf74275b7b155a532.
2022-06-28 14:44:01,599 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,599 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,600 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,600 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (aba2612c0c27557f219ff8ecc4f8605a) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,600 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@178a5157
2022-06-28 14:44:01,600 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,600 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,600 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,600 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2a8a78cf52a5cc6e1ffa5cfce6b12db1.
2022-06-28 14:44:01,600 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (09526dde3e83233f7ef357797fd10a9f) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,601 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (09526dde3e83233f7ef357797fd10a9f) [DEPLOYING].
2022-06-28 14:44:01,603 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@70ada74a
2022-06-28 14:44:01,603 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,603 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,603 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (09526dde3e83233f7ef357797fd10a9f) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,606 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (1aa02148bae7ca5a913b3a6fc8628cfa) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,606 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254), deploy into slot with allocation id 2a8a78cf52a5cc6e1ffa5cfce6b12db1.
2022-06-28 14:44:01,607 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (c3a38636d1100036e03ad4ea751cabd1) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,607 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot fb4e1f5642ece6493c04e1647c0534a5.
2022-06-28 14:44:01,607 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (09526dde3e83233f7ef357797fd10a9f) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,607 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,608 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254) [DEPLOYING].
2022-06-28 14:44:01,609 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@117f244b
2022-06-28 14:44:01,609 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,609 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,609 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8), deploy into slot with allocation id fb4e1f5642ece6493c04e1647c0534a5.
2022-06-28 14:44:01,609 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,610 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (f4302e97a7541aa074dc4368fac3a254) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,610 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a53a1cfc8729b9acf74275b7b155a532.
2022-06-28 14:44:01,610 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 79d117787b25fbf484c76080d516086a.
2022-06-28 14:44:01,610 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 5de667ff477b00660d08060d6f22e4a5.
2022-06-28 14:44:01,610 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2a8a78cf52a5cc6e1ffa5cfce6b12db1.
2022-06-28 14:44:01,610 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot f9d9ea68ee51e17c5085add57e7148c4.
2022-06-28 14:44:01,611 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 56895f35ec1339ddc839ae79485bd6c5.
2022-06-28 14:44:01,611 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 83c799e37aaf2a77c3e924add328130c.
2022-06-28 14:44:01,611 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 8a6b2dfe7060ffbfef980f2f86173c26.
2022-06-28 14:44:01,611 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 612909d08dc9ab51577d6bfc875c62e1.
2022-06-28 14:44:01,611 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot fb4e1f5642ece6493c04e1647c0534a5.
2022-06-28 14:44:01,611 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b98077ce2bd445da868d27a6f4abcebf.
2022-06-28 14:44:01,611 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d93b95d39795a65f30994d835c6574d2.
2022-06-28 14:44:01,611 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8) switched from CREATED to DEPLOYING.
2022-06-28 14:44:01,612 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8) [DEPLOYING].
2022-06-28 14:44:01,613 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2ada9add
2022-06-28 14:44:01,613 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:44:01,613 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:44:01,613 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,613 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (3de34239e7c4f135aa5158d6472b10e8) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:44:01,619 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,619 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,619 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,619 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,620 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,619 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,619 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,620 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,620 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,619 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,619 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,620 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:44:01,713 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 8 @ 
2022-06-28 14:44:01,714 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 5 @ 
2022-06-28 14:44:01,714 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 1 @ 
2022-06-28 14:44:01,714 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 7 @ 
2022-06-28 14:44:01,714 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 4 @ 
2022-06-28 14:44:01,714 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 6 @ 
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 0 @ 
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 10 @ 
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 2 @ 
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 3 @ 
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 9 @ 
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 11 @ 
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,715 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,716 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (184940351eb857cb44fc42a3400a06ab) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,716 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,718 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,716 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,718 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (31897c91cadc4cad1c20a7e6a097194e) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,718 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,718 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (a6a048a35cc4145bcb676dc0b1c1ec85) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,719 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (a04b2c623f0d4d0fb335ccdd1774c791) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (0bacd7d0c254ae11ecef8d35382b122e) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (5f2b7791a862b7cd9ac4f344b93eff63) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (8a769820c004f86329ebbd7cdcf3a26b) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (a90cb4a90d2ec8f48dbde140c2749708) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (09606db825c01cd71a6346ad184d4e3c) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (7fbbc7153457fe98404aca7e8b293fd7) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (8b92662f4cf07debb1ae862f3aa5fb47) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cf6d410a1028c1a1136dd0cc27a1966d) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,726 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,726 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,726 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,727 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,726 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,727 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,726 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,727 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,726 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,726 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,727 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,728 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:44:01,768 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,768 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,768 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641767
2022-06-28 14:44:01,771 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,771 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,771 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641770
2022-06-28 14:44:01,774 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,775 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,775 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641768
2022-06-28 14:44:01,778 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,778 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,778 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641776
2022-06-28 14:44:01,779 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,780 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,780 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641775
2022-06-28 14:44:01,782 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,782 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,782 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641774
2022-06-28 14:44:01,783 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,784 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,784 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641772
2022-06-28 14:44:01,784 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,784 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,784 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641772
2022-06-28 14:44:01,785 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,785 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,785 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641771
2022-06-28 14:44:01,786 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,786 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,786 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641781
2022-06-28 14:44:01,787 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,787 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,787 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641781
2022-06-28 14:44:01,787 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:01,787 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:01,787 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398641779
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,794 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,802 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:44:01,849 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (09526dde3e83233f7ef357797fd10a9f) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,849 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,849 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,849 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,850 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,849 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,849 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,850 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,851 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (aba2612c0c27557f219ff8ecc4f8605a) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,851 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,851 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (c3a38636d1100036e03ad4ea751cabd1) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,852 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (aef607bec3e611ffd83296a426749800) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,852 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,852 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (09526dde3e83233f7ef357797fd10a9f) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,852 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (3de34239e7c4f135aa5158d6472b10e8) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,852 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,852 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6face129aa759c7987c9f952cb0f7ab7) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,852 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,853 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (69e59a80055d361d94618f561d2078b5) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,853 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (1aa02148bae7ca5a913b3a6fc8628cfa) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,854 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (f4302e97a7541aa074dc4368fac3a254) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,854 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,854 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (fcbf1da941a22b06efd9e9106d73becc) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:01,855 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (980405b316bc33a68169df2a2b12c477) switched from INITIALIZING to RUNNING.
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-1] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-6] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-9] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-2] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-10] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-7] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-8] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-11] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-12] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-3] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-5] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,021 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-4] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,026 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Discovered new partitions: [flink-sql-exercise-demo1-0]
2022-06-28 14:44:02,028 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Assigning splits to readers {11=[[Partition: flink-sql-exercise-demo1-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
2022-06-28 14:44:02,032 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: flink-sql-exercise-demo1-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
2022-06-28 14:44:02,034 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 14:44:02,040 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:44:02,040 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:44:02,040 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:44:02,040 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:44:02,040 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656398642040
2022-06-28 14:44:02,045 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2022-06-28 14:44:02,048 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=g5-11, groupId=g5] Subscribed to partition(s): flink-sql-exercise-demo1-0
2022-06-28 14:44:02,052 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-11, groupId=g5] Seeking to EARLIEST offset of partition flink-sql-exercise-demo1-0
2022-06-28 14:44:02,063 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=g5-11, groupId=g5] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:44:02,072 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-11, groupId=g5] Resetting offset for partition flink-sql-exercise-demo1-0 to offset 0.
2022-06-28 14:45:33,150 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,153 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (09526dde3e83233f7ef357797fd10a9f) switched from RUNNING to FAILED with failure cause: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56)
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30)
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)

2022-06-28 14:45:33,153 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (09526dde3e83233f7ef357797fd10a9f).
2022-06-28 14:45:33,156 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 09526dde3e83233f7ef357797fd10a9f.
2022-06-28 14:45:33,158 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (09526dde3e83233f7ef357797fd10a9f) switched from RUNNING to FAILED on d277093a-f319-466a-ba38-eaf4e0d771aa @ kubernetes.docker.internal (dataPort=-1).
org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.4.jar:1.14.4]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-06-28 14:45:33,165 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 90bea66de1c231edf33913ecd54406c1_9.
2022-06-28 14:45:33,166 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 24 tasks should be restarted to recover the failed task 90bea66de1c231edf33913ecd54406c1_9. 
2022-06-28 14:45:33,168 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (3a4d58937706269a729bf00d426e0487) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-runtime-1.14.4.jar:1.14.4]
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[scala-library-2.12.7.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_221]
Caused by: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.4.jar:1.14.4]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-06-28 14:45:33,172 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (a90cb4a90d2ec8f48dbde140c2749708) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,173 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708).
2022-06-28 14:45:33,173 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,173 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708).
2022-06-28 14:45:33,173 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (7fbbc7153457fe98404aca7e8b293fd7) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (8a769820c004f86329ebbd7cdcf3a26b) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (184940351eb857cb44fc42a3400a06ab) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (a6a048a35cc4145bcb676dc0b1c1ec85) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (8b92662f4cf07debb1ae862f3aa5fb47) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (0bacd7d0c254ae11ecef8d35382b122e) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (09606db825c01cd71a6346ad184d4e3c) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (31897c91cadc4cad1c20a7e6a097194e) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,174 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cf6d410a1028c1a1136dd0cc27a1966d) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7).
2022-06-28 14:45:33,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (5f2b7791a862b7cd9ac4f344b93eff63) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,175 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,175 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7).
2022-06-28 14:45:33,175 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,175 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (a04b2c623f0d4d0fb335ccdd1774c791) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,175 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (a90cb4a90d2ec8f48dbde140c2749708).
2022-06-28 14:45:33,175 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (aef607bec3e611ffd83296a426749800) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,176 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b).
2022-06-28 14:45:33,177 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,177 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,178 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b).
2022-06-28 14:45:33,178 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (69e59a80055d361d94618f561d2078b5) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,178 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,178 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,178 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (7fbbc7153457fe98404aca7e8b293fd7).
2022-06-28 14:45:33,179 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6face129aa759c7987c9f952cb0f7ab7) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,179 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab).
2022-06-28 14:45:33,179 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,180 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (980405b316bc33a68169df2a2b12c477) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,180 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,180 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab).
2022-06-28 14:45:33,180 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (fcbf1da941a22b06efd9e9106d73becc) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,180 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,180 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (8a769820c004f86329ebbd7cdcf3a26b).
2022-06-28 14:45:33,180 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (aba2612c0c27557f219ff8ecc4f8605a) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,180 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85).
2022-06-28 14:45:33,180 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (1aa02148bae7ca5a913b3a6fc8628cfa) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,181 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,181 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,181 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85).
2022-06-28 14:45:33,181 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (c3a38636d1100036e03ad4ea751cabd1) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,181 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,181 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (f4302e97a7541aa074dc4368fac3a254) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,181 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (184940351eb857cb44fc42a3400a06ab).
2022-06-28 14:45:33,181 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47).
2022-06-28 14:45:33,181 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (3de34239e7c4f135aa5158d6472b10e8) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,181 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,181 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,182 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47).
2022-06-28 14:45:33,182 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,182 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (a6a048a35cc4145bcb676dc0b1c1ec85).
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e).
2022-06-28 14:45:33,183 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e).
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (8b92662f4cf07debb1ae862f3aa5fb47).
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c).
2022-06-28 14:45:33,183 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c).
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,183 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (0bacd7d0c254ae11ecef8d35382b122e).
2022-06-28 14:45:33,184 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e).
2022-06-28 14:45:33,184 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,184 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,184 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e).
2022-06-28 14:45:33,184 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,184 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (09606db825c01cd71a6346ad184d4e3c).
2022-06-28 14:45:33,184 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d).
2022-06-28 14:45:33,185 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,185 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,185 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d).
2022-06-28 14:45:33,185 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,185 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (31897c91cadc4cad1c20a7e6a097194e).
2022-06-28 14:45:33,185 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63).
2022-06-28 14:45:33,185 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,185 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63).
2022-06-28 14:45:33,185 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,186 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,186 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cf6d410a1028c1a1136dd0cc27a1966d).
2022-06-28 14:45:33,186 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791).
2022-06-28 14:45:33,186 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,186 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,186 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791).
2022-06-28 14:45:33,186 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,187 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (5f2b7791a862b7cd9ac4f344b93eff63).
2022-06-28 14:45:33,187 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-06-28 14:45:33,187 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 a90cb4a90d2ec8f48dbde140c2749708.
2022-06-28 14:45:33,187 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
2022-06-28 14:45:33,187 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800).
2022-06-28 14:45:33,187 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,187 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800).
2022-06-28 14:45:33,187 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (a90cb4a90d2ec8f48dbde140c2749708) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,188 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5).
2022-06-28 14:45:33,188 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,188 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5).
2022-06-28 14:45:33,188 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,188 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48).
2022-06-28 14:45:33,188 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,188 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48).
2022-06-28 14:45:33,188 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,189 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 7fbbc7153457fe98404aca7e8b293fd7.
2022-06-28 14:45:33,189 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,189 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (7fbbc7153457fe98404aca7e8b293fd7) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,189 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7).
2022-06-28 14:45:33,189 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,189 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7).
2022-06-28 14:45:33,190 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477).
2022-06-28 14:45:33,190 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,190 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477).
2022-06-28 14:45:33,190 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,190 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc).
2022-06-28 14:45:33,190 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,190 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc).
2022-06-28 14:45:33,190 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,190 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 8a769820c004f86329ebbd7cdcf3a26b.
2022-06-28 14:45:33,191 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,191 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a).
2022-06-28 14:45:33,191 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,191 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a).
2022-06-28 14:45:33,191 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (8a769820c004f86329ebbd7cdcf3a26b) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,191 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa).
2022-06-28 14:45:33,191 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,191 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa).
2022-06-28 14:45:33,191 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,192 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1).
2022-06-28 14:45:33,192 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,192 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1).
2022-06-28 14:45:33,192 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,192 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 184940351eb857cb44fc42a3400a06ab.
2022-06-28 14:45:33,192 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254).
2022-06-28 14:45:33,192 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,192 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254).
2022-06-28 14:45:33,193 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (184940351eb857cb44fc42a3400a06ab) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,193 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
2022-06-28 14:45:33,193 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,193 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8).
2022-06-28 14:45:33,193 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8) switched from RUNNING to CANCELING.
2022-06-28 14:45:33,193 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8).
2022-06-28 14:45:33,193 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,193 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (69e59a80055d361d94618f561d2078b5).
2022-06-28 14:45:33,193 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,193 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 a6a048a35cc4145bcb676dc0b1c1ec85.
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (a04b2c623f0d4d0fb335ccdd1774c791).
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 8b92662f4cf07debb1ae862f3aa5fb47.
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (a6a048a35cc4145bcb676dc0b1c1ec85) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,194 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 3600000 ms.
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 0bacd7d0c254ae11ecef8d35382b122e.
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 09606db825c01cd71a6346ad184d4e3c.
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (8b92662f4cf07debb1ae862f3aa5fb47) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 31897c91cadc4cad1c20a7e6a097194e.
2022-06-28 14:45:33,194 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (aef607bec3e611ffd83296a426749800).
2022-06-28 14:45:33,195 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 cf6d410a1028c1a1136dd0cc27a1966d.
2022-06-28 14:45:33,195 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (0bacd7d0c254ae11ecef8d35382b122e) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,195 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 5f2b7791a862b7cd9ac4f344b93eff63.
2022-06-28 14:45:33,196 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,196 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (c93a6ecd4fe7c34a9ffa83bcef97bb48).
2022-06-28 14:45:33,196 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 69e59a80055d361d94618f561d2078b5.
2022-06-28 14:45:33,196 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 a04b2c623f0d4d0fb335ccdd1774c791.
2022-06-28 14:45:33,196 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (09606db825c01cd71a6346ad184d4e3c) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,196 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 aef607bec3e611ffd83296a426749800.
2022-06-28 14:45:33,196 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 c93a6ecd4fe7c34a9ffa83bcef97bb48.
2022-06-28 14:45:33,196 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (31897c91cadc4cad1c20a7e6a097194e) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,196 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cf6d410a1028c1a1136dd0cc27a1966d) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (aba2612c0c27557f219ff8ecc4f8605a).
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 aba2612c0c27557f219ff8ecc4f8605a.
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (980405b316bc33a68169df2a2b12c477).
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 980405b316bc33a68169df2a2b12c477.
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=11}]
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (fcbf1da941a22b06efd9e9106d73becc).
2022-06-28 14:45:33,198 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (5f2b7791a862b7cd9ac4f344b93eff63) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (69e59a80055d361d94618f561d2078b5) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 fcbf1da941a22b06efd9e9106d73becc.
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (f4302e97a7541aa074dc4368fac3a254).
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (6face129aa759c7987c9f952cb0f7ab7).
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 f4302e97a7541aa074dc4368fac3a254.
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (1aa02148bae7ca5a913b3a6fc8628cfa).
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (3de34239e7c4f135aa5158d6472b10e8).
2022-06-28 14:45:33,199 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (a04b2c623f0d4d0fb335ccdd1774c791) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,200 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=10}]
2022-06-28 14:45:33,200 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 6face129aa759c7987c9f952cb0f7ab7.
2022-06-28 14:45:33,200 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,200 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (c3a38636d1100036e03ad4ea751cabd1).
2022-06-28 14:45:33,200 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 1aa02148bae7ca5a913b3a6fc8628cfa.
2022-06-28 14:45:33,200 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (aef607bec3e611ffd83296a426749800) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,200 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 3de34239e7c4f135aa5158d6472b10e8.
2022-06-28 14:45:33,200 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 c3a38636d1100036e03ad4ea751cabd1.
2022-06-28 14:45:33,200 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=9}]
2022-06-28 14:45:33,201 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (c93a6ecd4fe7c34a9ffa83bcef97bb48) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,201 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=8}]
2022-06-28 14:45:33,201 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (aba2612c0c27557f219ff8ecc4f8605a) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,201 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=7}]
2022-06-28 14:45:33,201 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (980405b316bc33a68169df2a2b12c477) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,202 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
2022-06-28 14:45:33,202 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (fcbf1da941a22b06efd9e9106d73becc) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,202 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
2022-06-28 14:45:33,202 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (f4302e97a7541aa074dc4368fac3a254) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,202 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
2022-06-28 14:45:33,203 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (6face129aa759c7987c9f952cb0f7ab7) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,203 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
2022-06-28 14:45:33,203 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (1aa02148bae7ca5a913b3a6fc8628cfa) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,203 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
2022-06-28 14:45:33,203 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (3de34239e7c4f135aa5158d6472b10e8) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,204 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 3a4d58937706269a729bf00d426e0487: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
2022-06-28 14:45:33,204 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (c3a38636d1100036e03ad4ea751cabd1) switched from CANCELING to CANCELED.
2022-06-28 14:45:33,204 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 3a4d58937706269a729bf00d426e0487
2022-06-28 14:45:33,204 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (3a4d58937706269a729bf00d426e0487) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-runtime-1.14.4.jar:1.14.4]
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[scala-library-2.12.7.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_221]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_221]
Caused by: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-streaming-java_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.4.jar:1.14.4]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.4.jar:1.14.4]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-06-28 14:45:33,207 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 3a4d58937706269a729bf00d426e0487.
2022-06-28 14:45:33,215 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Shutting down Flink Mini Cluster
2022-06-28 14:45:33,215 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 3a4d58937706269a729bf00d426e0487 reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.table.api.TableException: Column 'nick' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:56)
	at org.apache.flink.table.runtime.operators.sink.SinkNotNullEnforcer.filter(SinkNotNullEnforcer.java:30)
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:38)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:194)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)
2022-06-28 14:45:33,216 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
2022-06-28 14:45:33,216 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Shutting down rest endpoint.
2022-06-28 14:45:33,216 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close ResourceManager connection aad3f9231f6f6a81e9efeafad06fe1ee.
2022-06-28 14:45:33,216 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Closing TaskExecutor connection d277093a-f319-466a-ba38-eaf4e0d771aa because: The TaskExecutor is shutting down.
2022-06-28 14:45:33,217 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 3a4d58937706269a729bf00d426e0487.
2022-06-28 14:45:33,217 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,218 INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager [] - Shutting down TaskExecutorStateChangelogStoragesManager.
2022-06-28 14:45:33,218 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:5, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: a53a1cfc8729b9acf74275b7b155a532, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,219 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise.
2022-06-28 14:45:33,219 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:2, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 79d117787b25fbf484c76080d516086a, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,220 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:6, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 5de667ff477b00660d08060d6f22e4a5, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,220 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:1, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 2a8a78cf52a5cc6e1ffa5cfce6b12db1, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,220 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:11, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: f9d9ea68ee51e17c5085add57e7148c4, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,220 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:3, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 56895f35ec1339ddc839ae79485bd6c5, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,220 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:9, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 83c799e37aaf2a77c3e924add328130c, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,220 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:10, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 8a6b2dfe7060ffbfef980f2f86173c26, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,220 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:7, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: 612909d08dc9ab51577d6bfc875c62e1, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,221 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:8, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: fb4e1f5642ece6493c04e1647c0534a5, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,221 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: b98077ce2bd445da868d27a6f4abcebf, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,221 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:4, state:ALLOCATED, resource profile: ResourceProfile{taskHeapMemory=85.333gb (91625968981 bytes), taskOffHeapMemory=85.333gb (91625968981 bytes), managedMemory=10.667mb (11184810 bytes), networkMemory=5.333mb (5592405 bytes)}, allocationId: d93b95d39795a65f30994d835c6574d2, jobId: 3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,223 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise closed.
2022-06-28 14:45:33,223 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [a53a1cfc8729b9acf74275b7b155a532].
2022-06-28 14:45:33,225 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [79d117787b25fbf484c76080d516086a].
2022-06-28 14:45:33,226 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [5de667ff477b00660d08060d6f22e4a5].
2022-06-28 14:45:33,226 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
2022-06-28 14:45:33,226 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2022-06-28 14:45:33,226 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [2a8a78cf52a5cc6e1ffa5cfce6b12db1].
2022-06-28 14:45:33,227 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [f9d9ea68ee51e17c5085add57e7148c4].
2022-06-28 14:45:33,227 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [56895f35ec1339ddc839ae79485bd6c5].
2022-06-28 14:45:33,228 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [83c799e37aaf2a77c3e924add328130c].
2022-06-28 14:45:33,228 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [8a6b2dfe7060ffbfef980f2f86173c26].
2022-06-28 14:45:33,229 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [612909d08dc9ab51577d6bfc875c62e1].
2022-06-28 14:45:33,230 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [fb4e1f5642ece6493c04e1647c0534a5].
2022-06-28 14:45:33,231 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [b98077ce2bd445da868d27a6f4abcebf].
2022-06-28 14:45:33,231 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [d93b95d39795a65f30994d835c6574d2].
2022-06-28 14:45:33,231 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2022-06-28 14:45:33,231 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection aad3f9231f6f6a81e9efeafad06fe1ee: Stopping JobMaster for job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (3a4d58937706269a729bf00d426e0487).
2022-06-28 14:45:33,231 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Removing cache directory C:\Users\BONC\AppData\Local\Temp\flink-web-ui
2022-06-28 14:45:33,232 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager b3bdf119389e783086259c17aef64a25@akka://flink/user/rpc/jobmanager_3 for job 3a4d58937706269a729bf00d426e0487 from the resource manager.
2022-06-28 14:45:33,233 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Shut down complete.
2022-06-28 14:45:33,234 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
2022-06-28 14:45:33,234 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.
2022-06-28 14:45:33,235 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.
2022-06-28 14:45:33,235 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping dispatcher akka://flink/user/rpc/dispatcher_1.
2022-06-28 14:45:33,235 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.
2022-06-28 14:45:33,235 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_1.
2022-06-28 14:45:33,235 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.
2022-06-28 14:45:33,235 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.
2022-06-28 14:45:33,239 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory C:\Users\BONC\AppData\Local\Temp\flink-io-c67e065b-c1c3-4902-aa14-0047b7205432
2022-06-28 14:45:33,239 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Shutting down the network environment and its components.
2022-06-28 14:45:33,239 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka://flink/user/rpc/dispatcher_1.
2022-06-28 14:45:33,240 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory C:\Users\BONC\AppData\Local\Temp\flink-netty-shuffle-86dd76a7-38bf-41ce-9814-0f16ec0431ab
2022-06-28 14:45:33,240 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Shutting down the kvState service and its components.
2022-06-28 14:45:33,240 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.
2022-06-28 14:45:33,241 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory C:\Users\BONC\AppData\Local\Temp\flink-dist-cache-b7f40e67-d098-4cbc-9028-d5e3d98758eb
2022-06-28 14:45:33,241 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
2022-06-28 14:45:33,241 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2022-06-28 14:45:33,278 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2022-06-28 14:45:33,279 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2022-06-28 14:45:33,291 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2022-06-28 14:45:33,292 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2022-06-28 14:45:33,293 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:49733
2022-06-28 14:45:33,297 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2022-06-28 14:54:00,302 WARN  org.apache.flink.connector.kafka.sink.KafkaSinkBuilder       [] - Property [transaction.timeout.ms] not specified. Setting it to PT1H
2022-06-28 14:54:00,596 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:54:00,596 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:54:00,596 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 14:54:00,597 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 14:54:00,597 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 14:54:00,597 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2022-06-28 14:54:00,601 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Flink Mini Cluster
2022-06-28 14:54:00,910 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Metrics Registry
2022-06-28 14:54:00,953 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
2022-06-28 14:54:00,953 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting RPC Service(s)
2022-06-28 14:54:00,963 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 14:54:01,434 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 14:54:01,512 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink
2022-06-28 14:54:01,522 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 14:54:01,530 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 14:54:01,536 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink-metrics
2022-06-28 14:54:01,545 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2022-06-28 14:54:01,556 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting high-availability services
2022-06-28 14:54:01,565 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-a4016752-1abd-4074-9f8b-163181e22135
2022-06-28 14:54:01,568 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:50388 - max concurrent requests: 50 - max backlog: 1000
2022-06-28 14:54:01,571 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-3204cf55-0d29-4711-960e-5dfabdae1bcd
2022-06-28 14:54:01,572 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-3c531763-e6ee-4063-a3a1-efe5308d7b4e
2022-06-28 14:54:01,572 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting 1 TaskManager(s)
2022-06-28 14:54:01,575 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 6bef4ac8-cce5-436b-a24c-1441a6dae713
2022-06-28 14:54:01,583 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory 'C:\Users\BONC\AppData\Local\Temp': total 237 GB, usable 176 GB (74.26% usable)
2022-06-28 14:54:01,586 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-io-fc45401d-029f-436d-879d-cb40093f6ea0
2022-06-28 14:54:01,592 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-netty-shuffle-b2a21eb6-81d4-46ff-8fa4-6eff67655485
2022-06-28 14:54:01,618 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2022-06-28 14:54:01,625 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
2022-06-28 14:54:01,627 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
2022-06-28 14:54:01,635 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'akka.ask.timeout' instead of key 'taskmanager.slot.timeout'
2022-06-28 14:54:01,644 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2022-06-28 14:54:01,653 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
2022-06-28 14:54:01,655 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory C:\Users\BONC\AppData\Local\Temp\flink-dist-cache-c0ea6165-7e97-4884-94fd-1e7b12793ed8
2022-06-28 14:54:01,687 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Starting rest endpoint.
2022-06-28 14:54:01,689 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
2022-06-28 14:54:01,744 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Log file environment variable 'log.file' is not set.
2022-06-28 14:54:01,744 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
2022-06-28 14:54:01,975 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Rest endpoint listening at localhost:50439
2022-06-28 14:54:01,976 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender http://localhost:50439
2022-06-28 14:54:01,977 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - http://localhost:50439 was granted leadership with leaderSessionID=c5ddb4d4-7383-4fee-b2bc-41eb5ee299f3
2022-06-28 14:54:01,978 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader http://localhost:50439 , session=c5ddb4d4-7383-4fee-b2bc-41eb5ee299f3
2022-06-28 14:54:01,987 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2022-06-28 14:54:01,987 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
2022-06-28 14:54:01,987 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: ResourceManagerServiceImpl
2022-06-28 14:54:01,988 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id 28951cad-f397-48b4-9100-8db2b1312dc3. Creating new DispatcherLeaderProcess.
2022-06-28 14:54:01,988 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 3a4c9178-654f-4098-aede-4fbaff4ecc80.
2022-06-28 14:54:01,990 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Flink Mini Cluster started successfully
2022-06-28 14:54:01,994 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
2022-06-28 14:54:01,995 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.
2022-06-28 14:54:01,995 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
2022-06-28 14:54:02,000 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
2022-06-28 14:54:02,003 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_2 .
2022-06-28 14:54:02,008 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_1 , session=28951cad-f397-48b4-9100-8db2b1312dc3
2022-06-28 14:54:02,009 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
2022-06-28 14:54:02,015 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_2 , session=3a4c9178-654f-4098-aede-4fbaff4ecc80
2022-06-28 14:54:02,029 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(aede4fbaff4ecc803a4c9178654f4098).
2022-06-28 14:54:02,035 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
2022-06-28 14:54:02,038 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (0b896af55e19c2378341ac5cf051545e).
2022-06-28 14:54:02,039 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (0b896af55e19c2378341ac5cf051545e).
2022-06-28 14:54:02,039 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 6bef4ac8-cce5-436b-a24c-1441a6dae713 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2022-06-28 14:54:02,040 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_2 under registration id 25a5d50674fbe31ccf01cd330c3d815f.
2022-06-28 14:54:02,050 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2022-06-28 14:54:02,060 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2022-06-28 14:54:02,066 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (0b896af55e19c2378341ac5cf051545e).
2022-06-28 14:54:02,083 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for insert-into_default_catalog.default_database.t_kafka_nicekcnt (0b896af55e19c2378341ac5cf051545e).
2022-06-28 14:54:02,113 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job insert-into_default_catalog.default_database.t_kafka_nicekcnt (0b896af55e19c2378341ac5cf051545e).
2022-06-28 14:54:02,113 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
2022-06-28 14:54:02,147 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 0 ms
2022-06-28 14:54:02,184 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@208c7448
2022-06-28 14:54:02,184 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,186 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,201 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
2022-06-28 14:54:02,206 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@7abf77f4 for insert-into_default_catalog.default_database.t_kafka_nicekcnt (0b896af55e19c2378341ac5cf051545e).
2022-06-28 14:54:02,214 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=d45b16ea-8744-43f1-accb-cdeaee8fbb51
2022-06-28 14:54:02,216 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'insert-into_default_catalog.default_database.t_kafka_nicekcnt' (0b896af55e19c2378341ac5cf051545e) under job master id accbcdeaee8fbb51d45b16ea874443f1.
2022-06-28 14:54:02,217 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise.
2022-06-28 14:54:02,220 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2022-06-28 14:54:02,221 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.t_kafka_nicekcnt (0b896af55e19c2378341ac5cf051545e) switched from state CREATED to RUNNING.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (8200d1540087947c6fb9db4d58c335f5) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (29156cd79b62bd6c5c4339b225104fd6) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (2339568ea67fbf4dea6ec1043e3c0277) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (300829c0bac2e1c5f5d96f5aacafb1a7) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (bfe887ede2739beeadbc0260ae95b7f6) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (4f765c11f89a504943a136348a2305d3) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (14378e2980e953f1f2068e9279f31dd5) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (b9e9eaf5e0d2b51365de0e2fec370258) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (cf6143ff468f749de4698068c06ce84c) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cbd90bdb45f42735134e391fd45fc349) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (2a0cf2af403a316c8d4a6edd958afee5) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (99a046f47aa4133348e3933c90eff94a) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (61941e7e7b56768dc9f74d8916301302) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (ac088bb9f326f586898efba2fc976814) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,225 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (0485633771bccb637aefdd27cef18655) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,226 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (0acb55671e6ee51a07e04cd306c2b4f7) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,226 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (74f660a5270e0504dabcac1f5615a165) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,226 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (70c6df78ed80edd0c97ce53021b68553) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,226 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (4576b71ad103c8bacb75a6f369b21a10) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,226 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (89d46a34cf3b3afd12aebf83347fed1e) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,226 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (1db613ece75ede86cb5a38cdb7a4ea03) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,226 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (66728f21ddc611e92e8e1b7798142d61) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,226 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (a42f4df3a35d53982b1cfbfeb2617544) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,226 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (e5b5a5899a399c51f687b61f7839c2bb) switched from CREATED to SCHEDULED.
2022-06-28 14:54:02,248 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(aede4fbaff4ecc803a4c9178654f4098)
2022-06-28 14:54:02,249 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 14:54:02,250 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
2022-06-28 14:54:02,251 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager accbcdeaee8fbb51d45b16ea874443f1@akka://flink/user/rpc/jobmanager_3 for job 0b896af55e19c2378341ac5cf051545e.
2022-06-28 14:54:02,258 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager accbcdeaee8fbb51d45b16ea874443f1@akka://flink/user/rpc/jobmanager_3 for job 0b896af55e19c2378341ac5cf051545e.
2022-06-28 14:54:02,259 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,261 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 0b896af55e19c2378341ac5cf051545e: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=12}]
2022-06-28 14:54:02,264 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request b04999814d51b31ca4e869fe5e3a65b3 for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,268 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for b04999814d51b31ca4e869fe5e3a65b3.
2022-06-28 14:54:02,270 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 0b896af55e19c2378341ac5cf051545e for job leader monitoring.
2022-06-28 14:54:02,272 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id d45b16ea-8744-43f1-accb-cdeaee8fbb51.
2022-06-28 14:54:02,272 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request a3fc914220acef1afb40973b4c11f643 for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,273 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for a3fc914220acef1afb40973b4c11f643.
2022-06-28 14:54:02,273 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request cdc967622f689394cb9bcbff140e74be for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,273 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for cdc967622f689394cb9bcbff140e74be.
2022-06-28 14:54:02,273 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 492877804668f73c8b4550127355de76 for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,273 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 492877804668f73c8b4550127355de76.
2022-06-28 14:54:02,274 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2022-06-28 14:54:02,274 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 0b7d71e39c6fa5e64e74b6baed145caf for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,274 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 0b7d71e39c6fa5e64e74b6baed145caf.
2022-06-28 14:54:02,274 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 59e215becb75c77fad3aeca8c41606b9 for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,274 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 59e215becb75c77fad3aeca8c41606b9.
2022-06-28 14:54:02,274 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 766bcbb709d1823d4e97c01744723140 for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,274 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 766bcbb709d1823d4e97c01744723140.
2022-06-28 14:54:02,274 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request e0d1e4a2e212cc17449d4729e8cbdee2 for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,275 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for e0d1e4a2e212cc17449d4729e8cbdee2.
2022-06-28 14:54:02,275 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 3872809346575f7762f8c89ce2d02602 for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,275 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 3872809346575f7762f8c89ce2d02602.
2022-06-28 14:54:02,275 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request a1106211bb9cc72e048f0a9f1fd49fdb for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,275 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for a1106211bb9cc72e048f0a9f1fd49fdb.
2022-06-28 14:54:02,275 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request d08431ae2692b5a8b7542bfac60534d8 for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,275 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for d08431ae2692b5a8b7542bfac60534d8.
2022-06-28 14:54:02,275 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request b63ff5b79cdf1b60cd656b78adb1da21 for job 0b896af55e19c2378341ac5cf051545e from resource manager with leader id aede4fbaff4ecc803a4c9178654f4098.
2022-06-28 14:54:02,276 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for b63ff5b79cdf1b60cd656b78adb1da21.
2022-06-28 14:54:02,277 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 0b896af55e19c2378341ac5cf051545e.
2022-06-28 14:54:02,277 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 0b896af55e19c2378341ac5cf051545e.
2022-06-28 14:54:02,280 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 0b896af55e19c2378341ac5cf051545e.
2022-06-28 14:54:02,289 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (8200d1540087947c6fb9db4d58c335f5) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,289 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (attempt #0) with attempt id 8200d1540087947c6fb9db4d58c335f5 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id cdc967622f689394cb9bcbff140e74be
2022-06-28 14:54:02,292 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (29156cd79b62bd6c5c4339b225104fd6) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (attempt #0) with attempt id 29156cd79b62bd6c5c4339b225104fd6 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 59e215becb75c77fad3aeca8c41606b9
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (2339568ea67fbf4dea6ec1043e3c0277) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot cdc967622f689394cb9bcbff140e74be.
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (attempt #0) with attempt id 2339568ea67fbf4dea6ec1043e3c0277 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 3872809346575f7762f8c89ce2d02602
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (300829c0bac2e1c5f5d96f5aacafb1a7) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (attempt #0) with attempt id 300829c0bac2e1c5f5d96f5aacafb1a7 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id b63ff5b79cdf1b60cd656b78adb1da21
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (bfe887ede2739beeadbc0260ae95b7f6) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (attempt #0) with attempt id bfe887ede2739beeadbc0260ae95b7f6 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 0b7d71e39c6fa5e64e74b6baed145caf
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (4f765c11f89a504943a136348a2305d3) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,293 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (attempt #0) with attempt id 4f765c11f89a504943a136348a2305d3 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id a1106211bb9cc72e048f0a9f1fd49fdb
2022-06-28 14:54:02,294 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (14378e2980e953f1f2068e9279f31dd5) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,294 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (attempt #0) with attempt id 14378e2980e953f1f2068e9279f31dd5 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id d08431ae2692b5a8b7542bfac60534d8
2022-06-28 14:54:02,294 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (b9e9eaf5e0d2b51365de0e2fec370258) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,294 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (attempt #0) with attempt id b9e9eaf5e0d2b51365de0e2fec370258 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id e0d1e4a2e212cc17449d4729e8cbdee2
2022-06-28 14:54:02,294 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (cf6143ff468f749de4698068c06ce84c) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,294 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (attempt #0) with attempt id cf6143ff468f749de4698068c06ce84c to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 766bcbb709d1823d4e97c01744723140
2022-06-28 14:54:02,294 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cbd90bdb45f42735134e391fd45fc349) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,295 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (attempt #0) with attempt id cbd90bdb45f42735134e391fd45fc349 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id b04999814d51b31ca4e869fe5e3a65b3
2022-06-28 14:54:02,295 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (2a0cf2af403a316c8d4a6edd958afee5) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,295 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (attempt #0) with attempt id 2a0cf2af403a316c8d4a6edd958afee5 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id a3fc914220acef1afb40973b4c11f643
2022-06-28 14:54:02,295 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (99a046f47aa4133348e3933c90eff94a) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,295 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (attempt #0) with attempt id 99a046f47aa4133348e3933c90eff94a to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 492877804668f73c8b4550127355de76
2022-06-28 14:54:02,295 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (61941e7e7b56768dc9f74d8916301302) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,295 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (attempt #0) with attempt id 61941e7e7b56768dc9f74d8916301302 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id cdc967622f689394cb9bcbff140e74be
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (ac088bb9f326f586898efba2fc976814) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (attempt #0) with attempt id ac088bb9f326f586898efba2fc976814 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 59e215becb75c77fad3aeca8c41606b9
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (0485633771bccb637aefdd27cef18655) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (attempt #0) with attempt id 0485633771bccb637aefdd27cef18655 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 3872809346575f7762f8c89ce2d02602
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (0acb55671e6ee51a07e04cd306c2b4f7) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (attempt #0) with attempt id 0acb55671e6ee51a07e04cd306c2b4f7 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id b63ff5b79cdf1b60cd656b78adb1da21
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (74f660a5270e0504dabcac1f5615a165) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (attempt #0) with attempt id 74f660a5270e0504dabcac1f5615a165 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 0b7d71e39c6fa5e64e74b6baed145caf
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (70c6df78ed80edd0c97ce53021b68553) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,301 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (attempt #0) with attempt id 70c6df78ed80edd0c97ce53021b68553 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id a1106211bb9cc72e048f0a9f1fd49fdb
2022-06-28 14:54:02,302 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (4576b71ad103c8bacb75a6f369b21a10) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,302 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (attempt #0) with attempt id 4576b71ad103c8bacb75a6f369b21a10 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id d08431ae2692b5a8b7542bfac60534d8
2022-06-28 14:54:02,302 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (89d46a34cf3b3afd12aebf83347fed1e) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,302 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (attempt #0) with attempt id 89d46a34cf3b3afd12aebf83347fed1e to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id e0d1e4a2e212cc17449d4729e8cbdee2
2022-06-28 14:54:02,302 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (1db613ece75ede86cb5a38cdb7a4ea03) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,302 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (attempt #0) with attempt id 1db613ece75ede86cb5a38cdb7a4ea03 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 766bcbb709d1823d4e97c01744723140
2022-06-28 14:54:02,302 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory}.
2022-06-28 14:54:02,302 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (66728f21ddc611e92e8e1b7798142d61) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,303 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (attempt #0) with attempt id 66728f21ddc611e92e8e1b7798142d61 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id b04999814d51b31ca4e869fe5e3a65b3
2022-06-28 14:54:02,303 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2022-06-28 14:54:02,303 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (a42f4df3a35d53982b1cfbfeb2617544) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,303 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (attempt #0) with attempt id a42f4df3a35d53982b1cfbfeb2617544 to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id a3fc914220acef1afb40973b4c11f643
2022-06-28 14:54:02,303 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (e5b5a5899a399c51f687b61f7839c2bb) switched from SCHEDULED to DEPLOYING.
2022-06-28 14:54:02,303 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (attempt #0) with attempt id e5b5a5899a399c51f687b61f7839c2bb to 6bef4ac8-cce5-436b-a24c-1441a6dae713 @ kubernetes.docker.internal (dataPort=-1) with allocation id 492877804668f73c8b4550127355de76
2022-06-28 14:54:02,313 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:54:02,314 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:54:02,315 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,315 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,315 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242314
2022-06-28 14:54:02,317 INFO  org.apache.kafka.clients.admin.AdminClientConfig             [] - AdminClientConfig values: 
	bootstrap.servers = [192.168.136.130:9092]
	client.dns.lookup = default
	client.id = g5-enumerator-admin-client
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-06-28 14:54:02,318 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (8200d1540087947c6fb9db4d58c335f5), deploy into slot with allocation id cdc967622f689394cb9bcbff140e74be.
2022-06-28 14:54:02,318 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (8200d1540087947c6fb9db4d58c335f5) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,319 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 59e215becb75c77fad3aeca8c41606b9.
2022-06-28 14:54:02,322 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (29156cd79b62bd6c5c4339b225104fd6), deploy into slot with allocation id 59e215becb75c77fad3aeca8c41606b9.
2022-06-28 14:54:02,322 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (8200d1540087947c6fb9db4d58c335f5) [DEPLOYING].
2022-06-28 14:54:02,322 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3872809346575f7762f8c89ce2d02602.
2022-06-28 14:54:02,322 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (29156cd79b62bd6c5c4339b225104fd6) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,323 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (29156cd79b62bd6c5c4339b225104fd6) [DEPLOYING].
2022-06-28 14:54:02,324 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (2339568ea67fbf4dea6ec1043e3c0277), deploy into slot with allocation id 3872809346575f7762f8c89ce2d02602.
2022-06-28 14:54:02,325 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b63ff5b79cdf1b60cd656b78adb1da21.
2022-06-28 14:54:02,325 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (2339568ea67fbf4dea6ec1043e3c0277) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,325 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (2339568ea67fbf4dea6ec1043e3c0277) [DEPLOYING].
2022-06-28 14:54:02,327 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (300829c0bac2e1c5f5d96f5aacafb1a7), deploy into slot with allocation id b63ff5b79cdf1b60cd656b78adb1da21.
2022-06-28 14:54:02,327 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 0b7d71e39c6fa5e64e74b6baed145caf.
2022-06-28 14:54:02,327 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (300829c0bac2e1c5f5d96f5aacafb1a7) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,327 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (300829c0bac2e1c5f5d96f5aacafb1a7) [DEPLOYING].
2022-06-28 14:54:02,329 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (bfe887ede2739beeadbc0260ae95b7f6), deploy into slot with allocation id 0b7d71e39c6fa5e64e74b6baed145caf.
2022-06-28 14:54:02,329 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a1106211bb9cc72e048f0a9f1fd49fdb.
2022-06-28 14:54:02,329 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (bfe887ede2739beeadbc0260ae95b7f6) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,329 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (bfe887ede2739beeadbc0260ae95b7f6) [DEPLOYING].
2022-06-28 14:54:02,331 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (4f765c11f89a504943a136348a2305d3), deploy into slot with allocation id a1106211bb9cc72e048f0a9f1fd49fdb.
2022-06-28 14:54:02,331 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d08431ae2692b5a8b7542bfac60534d8.
2022-06-28 14:54:02,331 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (4f765c11f89a504943a136348a2305d3) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,332 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (4f765c11f89a504943a136348a2305d3) [DEPLOYING].
2022-06-28 14:54:02,333 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (14378e2980e953f1f2068e9279f31dd5), deploy into slot with allocation id d08431ae2692b5a8b7542bfac60534d8.
2022-06-28 14:54:02,334 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot e0d1e4a2e212cc17449d4729e8cbdee2.
2022-06-28 14:54:02,334 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (14378e2980e953f1f2068e9279f31dd5) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,334 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (14378e2980e953f1f2068e9279f31dd5) [DEPLOYING].
2022-06-28 14:54:02,336 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (b9e9eaf5e0d2b51365de0e2fec370258), deploy into slot with allocation id e0d1e4a2e212cc17449d4729e8cbdee2.
2022-06-28 14:54:02,337 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 766bcbb709d1823d4e97c01744723140.
2022-06-28 14:54:02,337 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (b9e9eaf5e0d2b51365de0e2fec370258) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,337 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (b9e9eaf5e0d2b51365de0e2fec370258) [DEPLOYING].
2022-06-28 14:54:02,339 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (cf6143ff468f749de4698068c06ce84c), deploy into slot with allocation id 766bcbb709d1823d4e97c01744723140.
2022-06-28 14:54:02,339 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'key.deserializer' was supplied but isn't a known config.
2022-06-28 14:54:02,339 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'value.deserializer' was supplied but isn't a known config.
2022-06-28 14:54:02,339 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'enable.auto.commit' was supplied but isn't a known config.
2022-06-28 14:54:02,340 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'group.id' was supplied but isn't a known config.
2022-06-28 14:54:02,340 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:54:02,340 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:54:02,340 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b04999814d51b31ca4e869fe5e3a65b3.
2022-06-28 14:54:02,340 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2022-06-28 14:54:02,340 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,340 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,340 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242340
2022-06-28 14:54:02,340 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (cf6143ff468f749de4698068c06ce84c) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,340 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (cf6143ff468f749de4698068c06ce84c) [DEPLOYING].
2022-06-28 14:54:02,340 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Starting the KafkaSourceEnumerator for consumer group g5 without periodic partition discovery.
2022-06-28 14:54:02,342 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cbd90bdb45f42735134e391fd45fc349), deploy into slot with allocation id b04999814d51b31ca4e869fe5e3a65b3.
2022-06-28 14:54:02,342 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a3fc914220acef1afb40973b4c11f643.
2022-06-28 14:54:02,344 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cbd90bdb45f42735134e391fd45fc349) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,345 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cbd90bdb45f42735134e391fd45fc349) [DEPLOYING].
2022-06-28 14:54:02,345 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (2a0cf2af403a316c8d4a6edd958afee5), deploy into slot with allocation id a3fc914220acef1afb40973b4c11f643.
2022-06-28 14:54:02,345 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 492877804668f73c8b4550127355de76.
2022-06-28 14:54:02,345 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (2a0cf2af403a316c8d4a6edd958afee5) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,346 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (2a0cf2af403a316c8d4a6edd958afee5) [DEPLOYING].
2022-06-28 14:54:02,348 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (99a046f47aa4133348e3933c90eff94a), deploy into slot with allocation id 492877804668f73c8b4550127355de76.
2022-06-28 14:54:02,349 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot cdc967622f689394cb9bcbff140e74be.
2022-06-28 14:54:02,349 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (99a046f47aa4133348e3933c90eff94a) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,349 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (99a046f47aa4133348e3933c90eff94a) [DEPLOYING].
2022-06-28 14:54:02,355 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@15606b17
2022-06-28 14:54:02,355 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2facb8e4
2022-06-28 14:54:02,355 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@622dbcab
2022-06-28 14:54:02,355 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7202ad79
2022-06-28 14:54:02,355 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@49d3ac11
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@407bb583
2022-06-28 14:54:02,355 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6f2e8716
2022-06-28 14:54:02,355 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7224ceb4
2022-06-28 14:54:02,355 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5ea2c203
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@486053d7
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@76365baf
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@67ea87e9
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,355 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,356 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (61941e7e7b56768dc9f74d8916301302), deploy into slot with allocation id cdc967622f689394cb9bcbff140e74be.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (29156cd79b62bd6c5c4339b225104fd6) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (b9e9eaf5e0d2b51365de0e2fec370258) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (99a046f47aa4133348e3933c90eff94a) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (300829c0bac2e1c5f5d96f5aacafb1a7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (2a0cf2af403a316c8d4a6edd958afee5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cbd90bdb45f42735134e391fd45fc349) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (4f765c11f89a504943a136348a2305d3) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (cf6143ff468f749de4698068c06ce84c) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (8200d1540087947c6fb9db4d58c335f5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (14378e2980e953f1f2068e9279f31dd5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (2339568ea67fbf4dea6ec1043e3c0277) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 59e215becb75c77fad3aeca8c41606b9.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (bfe887ede2739beeadbc0260ae95b7f6) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,361 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (61941e7e7b56768dc9f74d8916301302) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,362 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (61941e7e7b56768dc9f74d8916301302) [DEPLOYING].
2022-06-28 14:54:02,362 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (300829c0bac2e1c5f5d96f5aacafb1a7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,364 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@28b46a5b
2022-06-28 14:54:02,364 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,364 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,364 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (b9e9eaf5e0d2b51365de0e2fec370258) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,364 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (2a0cf2af403a316c8d4a6edd958afee5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,364 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (61941e7e7b56768dc9f74d8916301302) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,364 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (ac088bb9f326f586898efba2fc976814), deploy into slot with allocation id 59e215becb75c77fad3aeca8c41606b9.
2022-06-28 14:54:02,364 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cbd90bdb45f42735134e391fd45fc349) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,365 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (29156cd79b62bd6c5c4339b225104fd6) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,365 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3872809346575f7762f8c89ce2d02602.
2022-06-28 14:54:02,365 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (4f765c11f89a504943a136348a2305d3) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,365 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (cf6143ff468f749de4698068c06ce84c) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,365 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (ac088bb9f326f586898efba2fc976814) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,365 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (ac088bb9f326f586898efba2fc976814) [DEPLOYING].
2022-06-28 14:54:02,365 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (99a046f47aa4133348e3933c90eff94a) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,365 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (8200d1540087947c6fb9db4d58c335f5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,366 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (14378e2980e953f1f2068e9279f31dd5) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,366 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (2339568ea67fbf4dea6ec1043e3c0277) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,366 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (bfe887ede2739beeadbc0260ae95b7f6) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,367 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (61941e7e7b56768dc9f74d8916301302) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,367 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@76a8fa78
2022-06-28 14:54:02,367 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (0485633771bccb637aefdd27cef18655), deploy into slot with allocation id 3872809346575f7762f8c89ce2d02602.
2022-06-28 14:54:02,367 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,367 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,367 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (ac088bb9f326f586898efba2fc976814) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,368 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b63ff5b79cdf1b60cd656b78adb1da21.
2022-06-28 14:54:02,368 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (ac088bb9f326f586898efba2fc976814) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,369 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (0485633771bccb637aefdd27cef18655) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,371 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (0485633771bccb637aefdd27cef18655) [DEPLOYING].
2022-06-28 14:54:02,371 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (0acb55671e6ee51a07e04cd306c2b4f7), deploy into slot with allocation id b63ff5b79cdf1b60cd656b78adb1da21.
2022-06-28 14:54:02,372 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5b1c6f0c
2022-06-28 14:54:02,372 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,372 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (0acb55671e6ee51a07e04cd306c2b4f7) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,372 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,372 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (0acb55671e6ee51a07e04cd306c2b4f7) [DEPLOYING].
2022-06-28 14:54:02,372 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (0485633771bccb637aefdd27cef18655) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,373 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (0485633771bccb637aefdd27cef18655) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,373 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 0b7d71e39c6fa5e64e74b6baed145caf.
2022-06-28 14:54:02,373 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@bdc000b
2022-06-28 14:54:02,373 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,373 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,373 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (0acb55671e6ee51a07e04cd306c2b4f7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,375 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (74f660a5270e0504dabcac1f5615a165), deploy into slot with allocation id 0b7d71e39c6fa5e64e74b6baed145caf.
2022-06-28 14:54:02,375 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a1106211bb9cc72e048f0a9f1fd49fdb.
2022-06-28 14:54:02,377 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (0acb55671e6ee51a07e04cd306c2b4f7) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,377 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (70c6df78ed80edd0c97ce53021b68553), deploy into slot with allocation id a1106211bb9cc72e048f0a9f1fd49fdb.
2022-06-28 14:54:02,378 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d08431ae2692b5a8b7542bfac60534d8.
2022-06-28 14:54:02,384 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (70c6df78ed80edd0c97ce53021b68553) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,384 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (70c6df78ed80edd0c97ce53021b68553) [DEPLOYING].
2022-06-28 14:54:02,384 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (4576b71ad103c8bacb75a6f369b21a10), deploy into slot with allocation id d08431ae2692b5a8b7542bfac60534d8.
2022-06-28 14:54:02,385 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1c01f2d1
2022-06-28 14:54:02,385 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,385 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,385 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot e0d1e4a2e212cc17449d4729e8cbdee2.
2022-06-28 14:54:02,385 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (70c6df78ed80edd0c97ce53021b68553) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,386 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (70c6df78ed80edd0c97ce53021b68553) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,386 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (74f660a5270e0504dabcac1f5615a165) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,386 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (74f660a5270e0504dabcac1f5615a165) [DEPLOYING].
2022-06-28 14:54:02,385 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (4576b71ad103c8bacb75a6f369b21a10) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,387 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6fdfc6de
2022-06-28 14:54:02,387 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,387 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,387 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (4576b71ad103c8bacb75a6f369b21a10) [DEPLOYING].
2022-06-28 14:54:02,387 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (74f660a5270e0504dabcac1f5615a165) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,388 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (74f660a5270e0504dabcac1f5615a165) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,388 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4ead2a5f
2022-06-28 14:54:02,388 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,388 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,388 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (4576b71ad103c8bacb75a6f369b21a10) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,389 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (4576b71ad103c8bacb75a6f369b21a10) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,389 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (89d46a34cf3b3afd12aebf83347fed1e), deploy into slot with allocation id e0d1e4a2e212cc17449d4729e8cbdee2.
2022-06-28 14:54:02,390 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 766bcbb709d1823d4e97c01744723140.
2022-06-28 14:54:02,392 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (1db613ece75ede86cb5a38cdb7a4ea03), deploy into slot with allocation id 766bcbb709d1823d4e97c01744723140.
2022-06-28 14:54:02,392 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b04999814d51b31ca4e869fe5e3a65b3.
2022-06-28 14:54:02,393 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (1db613ece75ede86cb5a38cdb7a4ea03) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,394 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (1db613ece75ede86cb5a38cdb7a4ea03) [DEPLOYING].
2022-06-28 14:54:02,393 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (89d46a34cf3b3afd12aebf83347fed1e) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,394 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (89d46a34cf3b3afd12aebf83347fed1e) [DEPLOYING].
2022-06-28 14:54:02,395 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2eb889de
2022-06-28 14:54:02,395 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@31af0699
2022-06-28 14:54:02,395 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,395 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,395 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,396 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (66728f21ddc611e92e8e1b7798142d61), deploy into slot with allocation id b04999814d51b31ca4e869fe5e3a65b3.
2022-06-28 14:54:02,396 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (1db613ece75ede86cb5a38cdb7a4ea03) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,396 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a3fc914220acef1afb40973b4c11f643.
2022-06-28 14:54:02,395 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,397 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (89d46a34cf3b3afd12aebf83347fed1e) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,397 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (1db613ece75ede86cb5a38cdb7a4ea03) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,397 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (89d46a34cf3b3afd12aebf83347fed1e) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,397 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (a42f4df3a35d53982b1cfbfeb2617544), deploy into slot with allocation id a3fc914220acef1afb40973b4c11f643.
2022-06-28 14:54:02,398 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 492877804668f73c8b4550127355de76.
2022-06-28 14:54:02,398 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (a42f4df3a35d53982b1cfbfeb2617544) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,399 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (a42f4df3a35d53982b1cfbfeb2617544) [DEPLOYING].
2022-06-28 14:54:02,398 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (66728f21ddc611e92e8e1b7798142d61) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,400 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2a007422
2022-06-28 14:54:02,400 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (66728f21ddc611e92e8e1b7798142d61) [DEPLOYING].
2022-06-28 14:54:02,400 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,400 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,400 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (a42f4df3a35d53982b1cfbfeb2617544) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (e5b5a5899a399c51f687b61f7839c2bb), deploy into slot with allocation id 492877804668f73c8b4550127355de76.
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (a42f4df3a35d53982b1cfbfeb2617544) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,401 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@40774548
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,401 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (66728f21ddc611e92e8e1b7798142d61) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (66728f21ddc611e92e8e1b7798142d61) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a3fc914220acef1afb40973b4c11f643.
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 0b7d71e39c6fa5e64e74b6baed145caf.
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 766bcbb709d1823d4e97c01744723140.
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot cdc967622f689394cb9bcbff140e74be.
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d08431ae2692b5a8b7542bfac60534d8.
2022-06-28 14:54:02,402 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3872809346575f7762f8c89ce2d02602.
2022-06-28 14:54:02,402 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 492877804668f73c8b4550127355de76.
2022-06-28 14:54:02,402 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b04999814d51b31ca4e869fe5e3a65b3.
2022-06-28 14:54:02,402 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot b63ff5b79cdf1b60cd656b78adb1da21.
2022-06-28 14:54:02,401 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (e5b5a5899a399c51f687b61f7839c2bb) switched from CREATED to DEPLOYING.
2022-06-28 14:54:02,402 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 59e215becb75c77fad3aeca8c41606b9.
2022-06-28 14:54:02,402 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot e0d1e4a2e212cc17449d4729e8cbdee2.
2022-06-28 14:54:02,402 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot a1106211bb9cc72e048f0a9f1fd49fdb.
2022-06-28 14:54:02,402 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (e5b5a5899a399c51f687b61f7839c2bb) [DEPLOYING].
2022-06-28 14:54:02,403 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@8021007
2022-06-28 14:54:02,403 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 14:54:02,403 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 14:54:02,403 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (e5b5a5899a399c51f687b61f7839c2bb) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,404 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (e5b5a5899a399c51f687b61f7839c2bb) switched from DEPLOYING to INITIALIZING.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,408 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,409 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) exceeded the 80 characters length limit and was truncated.
2022-06-28 14:54:02,500 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 3 @ 
2022-06-28 14:54:02,501 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 2 @ 
2022-06-28 14:54:02,501 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 10 @ 
2022-06-28 14:54:02,501 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 4 @ 
2022-06-28 14:54:02,501 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 11 @ 
2022-06-28 14:54:02,501 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 8 @ 
2022-06-28 14:54:02,501 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 0 @ 
2022-06-28 14:54:02,501 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 7 @ 
2022-06-28 14:54:02,501 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 9 @ 
2022-06-28 14:54:02,502 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 5 @ 
2022-06-28 14:54:02,502 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 1 @ 
2022-06-28 14:54:02,502 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 6 @ 
2022-06-28 14:54:02,502 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12)#0 (99a046f47aa4133348e3933c90eff94a) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,502 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12)#0 (300829c0bac2e1c5f5d96f5aacafb1a7) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,502 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12)#0 (b9e9eaf5e0d2b51365de0e2fec370258) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,502 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12)#0 (29156cd79b62bd6c5c4339b225104fd6) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,502 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12)#0 (8200d1540087947c6fb9db4d58c335f5) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,502 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12)#0 (bfe887ede2739beeadbc0260ae95b7f6) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (12/12) (99a046f47aa4133348e3933c90eff94a) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,503 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12)#0 (14378e2980e953f1f2068e9279f31dd5) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,503 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12)#0 (4f765c11f89a504943a136348a2305d3) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,503 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12)#0 (2a0cf2af403a316c8d4a6edd958afee5) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,503 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12)#0 (cbd90bdb45f42735134e391fd45fc349) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,503 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (4/12) (300829c0bac2e1c5f5d96f5aacafb1a7) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,503 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12)#0 (2339568ea67fbf4dea6ec1043e3c0277) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,503 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12)#0 (cf6143ff468f749de4698068c06ce84c) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (8/12) (b9e9eaf5e0d2b51365de0e2fec370258) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,504 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (2/12) (29156cd79b62bd6c5c4339b225104fd6) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,508 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (1/12) (8200d1540087947c6fb9db4d58c335f5) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,509 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (5/12) (bfe887ede2739beeadbc0260ae95b7f6) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,509 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (7/12) (14378e2980e953f1f2068e9279f31dd5) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,509 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (6/12) (4f765c11f89a504943a136348a2305d3) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,509 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (11/12) (2a0cf2af403a316c8d4a6edd958afee5) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,510 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (10/12) (cbd90bdb45f42735134e391fd45fc349) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,510 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (3/12) (2339568ea67fbf4dea6ec1043e3c0277) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,510 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[nick, id]) (9/12) (cf6143ff468f749de4698068c06ce84c) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,513 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,513 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,513 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,513 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,513 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,514 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,513 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,514 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,514 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,513 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,514 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,513 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:54:02,550 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,550 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,550 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242549
2022-06-28 14:54:02,552 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,552 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,552 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242549
2022-06-28 14:54:02,557 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,557 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,557 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242555
2022-06-28 14:54:02,561 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,562 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,562 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242553
2022-06-28 14:54:02,562 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,563 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,563 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242561
2022-06-28 14:54:02,565 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,565 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,566 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242561
2022-06-28 14:54:02,568 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,569 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,569 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242559
2022-06-28 14:54:02,569 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,569 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,569 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242558
2022-06-28 14:54:02,569 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,569 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,570 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242558
2022-06-28 14:54:02,570 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,570 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,570 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242567
2022-06-28 14:54:02,570 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,570 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,570 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242565
2022-06-28 14:54:02,571 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,571 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,571 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242562
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,579 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,579 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,578 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder [] - Finished to build heap keyed state-backend.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,585 INFO  org.apache.flink.runtime.state.heap.HeapKeyedStateBackend    [] - Initializing heap keyed state backend with stream factory.
2022-06-28 14:54:02,629 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12)#0 (61941e7e7b56768dc9f74d8916301302) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,629 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12)#0 (1db613ece75ede86cb5a38cdb7a4ea03) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,630 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12)#0 (74f660a5270e0504dabcac1f5615a165) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,630 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12)#0 (70c6df78ed80edd0c97ce53021b68553) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,630 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12)#0 (e5b5a5899a399c51f687b61f7839c2bb) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,629 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12)#0 (a42f4df3a35d53982b1cfbfeb2617544) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,630 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (1/12) (61941e7e7b56768dc9f74d8916301302) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,629 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12)#0 (4576b71ad103c8bacb75a6f369b21a10) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,631 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12)#0 (0acb55671e6ee51a07e04cd306c2b4f7) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,629 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12)#0 (ac088bb9f326f586898efba2fc976814) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,629 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12)#0 (66728f21ddc611e92e8e1b7798142d61) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,633 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (9/12) (1db613ece75ede86cb5a38cdb7a4ea03) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,633 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12)#0 (0485633771bccb637aefdd27cef18655) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,633 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (5/12) (74f660a5270e0504dabcac1f5615a165) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,633 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12)#0 (89d46a34cf3b3afd12aebf83347fed1e) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,634 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (6/12) (70c6df78ed80edd0c97ce53021b68553) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,634 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (12/12) (e5b5a5899a399c51f687b61f7839c2bb) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,634 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (11/12) (a42f4df3a35d53982b1cfbfeb2617544) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,634 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (7/12) (4576b71ad103c8bacb75a6f369b21a10) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,635 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (4/12) (0acb55671e6ee51a07e04cd306c2b4f7) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,635 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (2/12) (ac088bb9f326f586898efba2fc976814) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,635 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (10/12) (66728f21ddc611e92e8e1b7798142d61) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,637 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (3/12) (0485633771bccb637aefdd27cef18655) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,745 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - GroupAggregate(groupBy=[nick], select=[nick, COUNT(DISTINCT id) AS user_cnt]) -> NotNullEnforcer(fields=[nick]) -> Sink Sink(table=[default_catalog.default_database.t_kafka_nicekcnt], fields=[nick, user_cnt]) (8/12) (89d46a34cf3b3afd12aebf83347fed1e) switched from INITIALIZING to RUNNING.
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-5] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-2] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-9] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-10] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-7] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-1] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-11] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-3] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-6] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-12] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-4] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,812 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-8] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,816 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Discovered new partitions: [flink-sql-exercise-demo2-0]
2022-06-28 14:54:02,818 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Assigning splits to readers {6=[[Partition: flink-sql-exercise-demo2-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
2022-06-28 14:54:02,821 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: flink-sql-exercise-demo2-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
2022-06-28 14:54:02,824 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 14:54:02,826 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 14:54:02,826 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 14:54:02,826 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:54:02,826 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:54:02,826 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399242826
2022-06-28 14:54:02,831 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2022-06-28 14:54:02,834 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=g5-6, groupId=g5] Subscribed to partition(s): flink-sql-exercise-demo2-0
2022-06-28 14:54:02,837 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-6, groupId=g5] Seeking to EARLIEST offset of partition flink-sql-exercise-demo2-0
2022-06-28 14:54:02,849 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=g5-6, groupId=g5] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:54:02,861 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-6, groupId=g5] Resetting offset for partition flink-sql-exercise-demo2-0 to offset 0.
2022-06-28 14:59:06,529 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 14:59:06,531 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 14:59:06,531 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 14:59:06,531 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399546531
2022-06-28 14:59:06,534 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-13] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 14:59:06,535 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-06-28 15:03:40,447 INFO  org.apache.kafka.clients.producer.ProducerConfig             [] - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.136.130:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-06-28 15:03:40,449 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 15:03:40,449 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 15:03:40,449 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656399820449
2022-06-28 15:03:40,454 INFO  org.apache.kafka.clients.Metadata                            [] - [Producer clientId=producer-14] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 15:03:40,454 INFO  org.apache.kafka.clients.producer.KafkaProducer              [] - [Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-06-28 16:00:11,481 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2022-06-28 16:00:11,481 INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager [] - Shutting down TaskExecutorStateChangelogStoragesManager.
2022-06-28 16:00:11,482 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2022-06-28 16:00:11,481 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2022-06-28 16:24:30,022 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2022-06-28 16:24:30,025 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 16:24:30,025 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2022-06-28 16:24:30,025 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 16:24:30,025 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2022-06-28 16:24:30,025 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2022-06-28 16:24:30,028 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Flink Mini Cluster
2022-06-28 16:24:30,432 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Metrics Registry
2022-06-28 16:24:30,473 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
2022-06-28 16:24:30,473 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting RPC Service(s)
2022-06-28 16:24:30,483 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 16:24:30,949 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 16:24:31,030 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink
2022-06-28 16:24:31,040 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system
2022-06-28 16:24:31,048 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-06-28 16:24:31,054 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka://flink-metrics
2022-06-28 16:24:31,062 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2022-06-28 16:24:31,074 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting high-availability services
2022-06-28 16:24:31,083 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-83e0537b-a230-4ec8-890d-36b2145c940e
2022-06-28 16:24:31,086 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:53557 - max concurrent requests: 50 - max backlog: 1000
2022-06-28 16:24:31,089 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-e3309b27-60b5-44e5-bd2c-132e6a33beea
2022-06-28 16:24:31,090 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory C:\Users\BONC\AppData\Local\Temp\blobStore-d4cca96b-fc70-468d-972b-212bee73a9f0
2022-06-28 16:24:31,090 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting 1 TaskManager(s)
2022-06-28 16:24:31,093 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 69dab3f1-a75a-440e-9b76-1d34b4e90ec4
2022-06-28 16:24:31,101 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory 'C:\Users\BONC\AppData\Local\Temp': total 237 GB, usable 176 GB (74.26% usable)
2022-06-28 16:24:31,104 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-io-7ce7edd2-cd3a-424e-a218-e818a8887527
2022-06-28 16:24:31,110 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	C:\Users\BONC\AppData\Local\Temp\flink-netty-shuffle-251ad9f5-77eb-4927-97f1-49ba130a573c
2022-06-28 16:24:31,138 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2022-06-28 16:24:31,146 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
2022-06-28 16:24:31,147 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
2022-06-28 16:24:31,155 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'akka.ask.timeout' instead of key 'taskmanager.slot.timeout'
2022-06-28 16:24:31,165 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2022-06-28 16:24:31,175 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
2022-06-28 16:24:31,177 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory C:\Users\BONC\AppData\Local\Temp\flink-dist-cache-bba013ce-8f40-4058-babb-da30da354774
2022-06-28 16:24:31,211 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Starting rest endpoint.
2022-06-28 16:24:31,212 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
2022-06-28 16:24:31,268 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Log file environment variable 'log.file' is not set.
2022-06-28 16:24:31,268 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
2022-06-28 16:24:31,497 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Rest endpoint listening at localhost:53608
2022-06-28 16:24:31,498 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender http://localhost:53608
2022-06-28 16:24:31,500 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - http://localhost:53608 was granted leadership with leaderSessionID=3eb34814-f6a2-48ca-9dad-a84542a12428
2022-06-28 16:24:31,501 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader http://localhost:53608 , session=3eb34814-f6a2-48ca-9dad-a84542a12428
2022-06-28 16:24:31,509 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2022-06-28 16:24:31,509 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
2022-06-28 16:24:31,509 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: ResourceManagerServiceImpl
2022-06-28 16:24:31,510 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id 5837e157-1fa1-4f33-b823-634111ac0402. Creating new DispatcherLeaderProcess.
2022-06-28 16:24:31,510 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 6e4883f0-a2d5-4e8d-b6d7-2f99d9fe4a49.
2022-06-28 16:24:31,512 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Flink Mini Cluster started successfully
2022-06-28 16:24:31,513 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
2022-06-28 16:24:31,514 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.
2022-06-28 16:24:31,514 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
2022-06-28 16:24:31,519 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
2022-06-28 16:24:31,523 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_2 .
2022-06-28 16:24:31,526 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_1 , session=5837e157-1fa1-4f33-b823-634111ac0402
2022-06-28 16:24:31,529 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
2022-06-28 16:24:31,535 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_2 , session=6e4883f0-a2d5-4e8d-b6d7-2f99d9fe4a49
2022-06-28 16:24:31,535 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(b6d72f99d9fe4a496e4883f0a2d54e8d).
2022-06-28 16:24:31,547 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
2022-06-28 16:24:31,551 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2022-06-28 16:24:31,553 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_2 under registration id f8ee00904d685c7f8d4d8838075588a7.
2022-06-28 16:24:31,554 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'collect' (931bc733a6969347a3fdb771cea453da).
2022-06-28 16:24:31,555 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'collect' (931bc733a6969347a3fdb771cea453da).
2022-06-28 16:24:31,566 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2022-06-28 16:24:31,576 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2022-06-28 16:24:31,582 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'collect' (931bc733a6969347a3fdb771cea453da).
2022-06-28 16:24:31,598 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for collect (931bc733a6969347a3fdb771cea453da).
2022-06-28 16:24:31,630 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job collect (931bc733a6969347a3fdb771cea453da).
2022-06-28 16:24:31,630 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
2022-06-28 16:24:31,665 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 0 ms
2022-06-28 16:24:31,686 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@35d2f276
2022-06-28 16:24:31,686 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,687 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,699 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
2022-06-28 16:24:31,705 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@5c21d36b for collect (931bc733a6969347a3fdb771cea453da).
2022-06-28 16:24:31,713 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=72e9b629-df6f-4dc6-9ea1-a364c22c7c35
2022-06-28 16:24:31,715 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'collect' (931bc733a6969347a3fdb771cea453da) under job master id 9ea1a364c22c7c3572e9b629df6f4dc6.
2022-06-28 16:24:31,716 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Starting split enumerator for source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise.
2022-06-28 16:24:31,718 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2022-06-28 16:24:31,718 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job collect (931bc733a6969347a3fdb771cea453da) switched from state CREATED to RUNNING.
2022-06-28 16:24:31,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12) (64ef5ee9cfd181e16e4ca54910c4ca7e) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12) (4197ea5df276dbab325581874ddd596e) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12) (49db214fc16fe3b61149b0f9e1d925c3) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12) (25e5646394dc050bfe8cc1aa1ab4efb1) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12) (dc5dc3be7a7f9dce75c73fb88bcc3e1a) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12) (c8fc8ec5f9b94db439c554c7fe071420) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12) (ad949c2e86d937b27f9579995ab4ebfa) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,721 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12) (821a8e33843e2af6f3ae1df52f9c0b0b) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12) (4f2597239d67e71ae1eb6acfe3a18330) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12) (9c1982217f6cef9722a482a9d4df7559) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12) (f6e204231635a2774929e436d844695a) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12) (432399431dc5864ed0f4d341792fcf03) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,722 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Collect table sink (1/1) (c2467235cdf7036ab8e9ebccc22cfc6d) switched from CREATED to SCHEDULED.
2022-06-28 16:24:31,736 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(b6d72f99d9fe4a496e4883f0a2d54e8d)
2022-06-28 16:24:31,738 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
2022-06-28 16:24:31,738 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 16:24:31,740 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 9ea1a364c22c7c3572e9b629df6f4dc6@akka://flink/user/rpc/jobmanager_3 for job 931bc733a6969347a3fdb771cea453da.
2022-06-28 16:24:31,749 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 9ea1a364c22c7c3572e9b629df6f4dc6@akka://flink/user/rpc/jobmanager_3 for job 931bc733a6969347a3fdb771cea453da.
2022-06-28 16:24:31,750 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,751 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 931bc733a6969347a3fdb771cea453da: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=12}]
2022-06-28 16:24:31,755 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 227c5886f37850089b70e1aeff4b3a2f for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,779 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 227c5886f37850089b70e1aeff4b3a2f.
2022-06-28 16:24:31,780 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 931bc733a6969347a3fdb771cea453da for job leader monitoring.
2022-06-28 16:24:31,782 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 72e9b629-df6f-4dc6-9ea1-a364c22c7c35.
2022-06-28 16:24:31,783 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 487c4704c07dbffd23f9c0fbe0135f10 for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,783 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 487c4704c07dbffd23f9c0fbe0135f10.
2022-06-28 16:24:31,783 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 6ddfe7213a7402ac97afc9d0375c70a4 for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,783 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 6ddfe7213a7402ac97afc9d0375c70a4.
2022-06-28 16:24:31,783 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
2022-06-28 16:24:31,784 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 21e3aac63be84eb5a84a56b3faef9b43 for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,784 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 21e3aac63be84eb5a84a56b3faef9b43.
2022-06-28 16:24:31,784 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 801a92362500bfcd66a896e45576c0a8 for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,784 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 801a92362500bfcd66a896e45576c0a8.
2022-06-28 16:24:31,784 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 5194d150041bccae279fff3155ff610b for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,784 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 5194d150041bccae279fff3155ff610b.
2022-06-28 16:24:31,784 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 9b750511271ca84fb4c66527879a816e for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,785 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 9b750511271ca84fb4c66527879a816e.
2022-06-28 16:24:31,785 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 0464bd5bfa800255f0202873fbe42337 for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,785 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 0464bd5bfa800255f0202873fbe42337.
2022-06-28 16:24:31,785 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 9a215efc8d17dac9ed3d0e7f9cd220fe for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,785 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 9a215efc8d17dac9ed3d0e7f9cd220fe.
2022-06-28 16:24:31,786 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request ec66a1eb12045d0a4dd4b45fdfd384a8 for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,786 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for ec66a1eb12045d0a4dd4b45fdfd384a8.
2022-06-28 16:24:31,786 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 2b0c2fcce96af604059dbaf16346aab4 for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,786 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 2b0c2fcce96af604059dbaf16346aab4.
2022-06-28 16:24:31,786 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request be8026bf22cc19a18729113e58ddc6d4 for job 931bc733a6969347a3fdb771cea453da from resource manager with leader id b6d72f99d9fe4a496e4883f0a2d54e8d.
2022-06-28 16:24:31,786 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for be8026bf22cc19a18729113e58ddc6d4.
2022-06-28 16:24:31,788 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 931bc733a6969347a3fdb771cea453da.
2022-06-28 16:24:31,788 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 931bc733a6969347a3fdb771cea453da.
2022-06-28 16:24:31,791 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 931bc733a6969347a3fdb771cea453da.
2022-06-28 16:24:31,800 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12) (64ef5ee9cfd181e16e4ca54910c4ca7e) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,801 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12) (attempt #0) with attempt id 64ef5ee9cfd181e16e4ca54910c4ca7e to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 21e3aac63be84eb5a84a56b3faef9b43
2022-06-28 16:24:31,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12) (4197ea5df276dbab325581874ddd596e) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12) (attempt #0) with attempt id 4197ea5df276dbab325581874ddd596e to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 9b750511271ca84fb4c66527879a816e
2022-06-28 16:24:31,805 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 21e3aac63be84eb5a84a56b3faef9b43.
2022-06-28 16:24:31,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12) (49db214fc16fe3b61149b0f9e1d925c3) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12) (attempt #0) with attempt id 49db214fc16fe3b61149b0f9e1d925c3 to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 2b0c2fcce96af604059dbaf16346aab4
2022-06-28 16:24:31,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12) (25e5646394dc050bfe8cc1aa1ab4efb1) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12) (attempt #0) with attempt id 25e5646394dc050bfe8cc1aa1ab4efb1 to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 6ddfe7213a7402ac97afc9d0375c70a4
2022-06-28 16:24:31,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12) (dc5dc3be7a7f9dce75c73fb88bcc3e1a) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,805 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12) (attempt #0) with attempt id dc5dc3be7a7f9dce75c73fb88bcc3e1a to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 5194d150041bccae279fff3155ff610b
2022-06-28 16:24:31,806 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12) (c8fc8ec5f9b94db439c554c7fe071420) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,806 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12) (attempt #0) with attempt id c8fc8ec5f9b94db439c554c7fe071420 to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 0464bd5bfa800255f0202873fbe42337
2022-06-28 16:24:31,806 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12) (ad949c2e86d937b27f9579995ab4ebfa) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,806 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12) (attempt #0) with attempt id ad949c2e86d937b27f9579995ab4ebfa to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id be8026bf22cc19a18729113e58ddc6d4
2022-06-28 16:24:31,806 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12) (821a8e33843e2af6f3ae1df52f9c0b0b) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,806 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12) (attempt #0) with attempt id 821a8e33843e2af6f3ae1df52f9c0b0b to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 801a92362500bfcd66a896e45576c0a8
2022-06-28 16:24:31,806 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12) (4f2597239d67e71ae1eb6acfe3a18330) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,807 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12) (attempt #0) with attempt id 4f2597239d67e71ae1eb6acfe3a18330 to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 9a215efc8d17dac9ed3d0e7f9cd220fe
2022-06-28 16:24:31,807 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12) (9c1982217f6cef9722a482a9d4df7559) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,807 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12) (attempt #0) with attempt id 9c1982217f6cef9722a482a9d4df7559 to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 227c5886f37850089b70e1aeff4b3a2f
2022-06-28 16:24:31,807 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12) (f6e204231635a2774929e436d844695a) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,807 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12) (attempt #0) with attempt id f6e204231635a2774929e436d844695a to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 487c4704c07dbffd23f9c0fbe0135f10
2022-06-28 16:24:31,807 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12) (432399431dc5864ed0f4d341792fcf03) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,807 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12) (attempt #0) with attempt id 432399431dc5864ed0f4d341792fcf03 to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id ec66a1eb12045d0a4dd4b45fdfd384a8
2022-06-28 16:24:31,807 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Collect table sink (1/1) (c2467235cdf7036ab8e9ebccc22cfc6d) switched from SCHEDULED to DEPLOYING.
2022-06-28 16:24:31,807 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Sink: Collect table sink (1/1) (attempt #0) with attempt id c2467235cdf7036ab8e9ebccc22cfc6d to 69dab3f1-a75a-440e-9b76-1d34b4e90ec4 @ kubernetes.docker.internal (dataPort=-1) with allocation id 21e3aac63be84eb5a84a56b3faef9b43
2022-06-28 16:24:31,814 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory}.
2022-06-28 16:24:31,814 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
2022-06-28 16:24:31,822 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 16:24:31,822 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 16:24:31,823 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 16:24:31,823 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 16:24:31,823 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656404671822
2022-06-28 16:24:31,825 INFO  org.apache.kafka.clients.admin.AdminClientConfig             [] - AdminClientConfig values: 
	bootstrap.servers = [192.168.136.130:9092]
	client.dns.lookup = default
	client.id = g5-enumerator-admin-client
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2022-06-28 16:24:31,828 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12)#0 (64ef5ee9cfd181e16e4ca54910c4ca7e), deploy into slot with allocation id 21e3aac63be84eb5a84a56b3faef9b43.
2022-06-28 16:24:31,829 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12)#0 (64ef5ee9cfd181e16e4ca54910c4ca7e) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,830 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 9b750511271ca84fb4c66527879a816e.
2022-06-28 16:24:31,832 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12)#0 (4197ea5df276dbab325581874ddd596e), deploy into slot with allocation id 9b750511271ca84fb4c66527879a816e.
2022-06-28 16:24:31,832 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2b0c2fcce96af604059dbaf16346aab4.
2022-06-28 16:24:31,832 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12)#0 (64ef5ee9cfd181e16e4ca54910c4ca7e) [DEPLOYING].
2022-06-28 16:24:31,832 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12)#0 (4197ea5df276dbab325581874ddd596e) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,833 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12)#0 (4197ea5df276dbab325581874ddd596e) [DEPLOYING].
2022-06-28 16:24:31,834 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12)#0 (49db214fc16fe3b61149b0f9e1d925c3), deploy into slot with allocation id 2b0c2fcce96af604059dbaf16346aab4.
2022-06-28 16:24:31,835 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 6ddfe7213a7402ac97afc9d0375c70a4.
2022-06-28 16:24:31,835 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12)#0 (49db214fc16fe3b61149b0f9e1d925c3) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,835 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12)#0 (49db214fc16fe3b61149b0f9e1d925c3) [DEPLOYING].
2022-06-28 16:24:31,837 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12)#0 (25e5646394dc050bfe8cc1aa1ab4efb1), deploy into slot with allocation id 6ddfe7213a7402ac97afc9d0375c70a4.
2022-06-28 16:24:31,837 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 5194d150041bccae279fff3155ff610b.
2022-06-28 16:24:31,837 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12)#0 (25e5646394dc050bfe8cc1aa1ab4efb1) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,837 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12)#0 (25e5646394dc050bfe8cc1aa1ab4efb1) [DEPLOYING].
2022-06-28 16:24:31,839 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12)#0 (dc5dc3be7a7f9dce75c73fb88bcc3e1a), deploy into slot with allocation id 5194d150041bccae279fff3155ff610b.
2022-06-28 16:24:31,839 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 0464bd5bfa800255f0202873fbe42337.
2022-06-28 16:24:31,839 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12)#0 (dc5dc3be7a7f9dce75c73fb88bcc3e1a) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,839 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12)#0 (dc5dc3be7a7f9dce75c73fb88bcc3e1a) [DEPLOYING].
2022-06-28 16:24:31,842 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12)#0 (c8fc8ec5f9b94db439c554c7fe071420), deploy into slot with allocation id 0464bd5bfa800255f0202873fbe42337.
2022-06-28 16:24:31,842 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot be8026bf22cc19a18729113e58ddc6d4.
2022-06-28 16:24:31,842 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12)#0 (c8fc8ec5f9b94db439c554c7fe071420) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,843 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12)#0 (c8fc8ec5f9b94db439c554c7fe071420) [DEPLOYING].
2022-06-28 16:24:31,844 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12)#0 (ad949c2e86d937b27f9579995ab4ebfa), deploy into slot with allocation id be8026bf22cc19a18729113e58ddc6d4.
2022-06-28 16:24:31,844 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 801a92362500bfcd66a896e45576c0a8.
2022-06-28 16:24:31,844 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12)#0 (ad949c2e86d937b27f9579995ab4ebfa) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,844 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12)#0 (ad949c2e86d937b27f9579995ab4ebfa) [DEPLOYING].
2022-06-28 16:24:31,846 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12)#0 (821a8e33843e2af6f3ae1df52f9c0b0b), deploy into slot with allocation id 801a92362500bfcd66a896e45576c0a8.
2022-06-28 16:24:31,846 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 9a215efc8d17dac9ed3d0e7f9cd220fe.
2022-06-28 16:24:31,847 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'key.deserializer' was supplied but isn't a known config.
2022-06-28 16:24:31,846 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12)#0 (821a8e33843e2af6f3ae1df52f9c0b0b) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,847 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'value.deserializer' was supplied but isn't a known config.
2022-06-28 16:24:31,847 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'enable.auto.commit' was supplied but isn't a known config.
2022-06-28 16:24:31,847 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12)#0 (821a8e33843e2af6f3ae1df52f9c0b0b) [DEPLOYING].
2022-06-28 16:24:31,847 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'group.id' was supplied but isn't a known config.
2022-06-28 16:24:31,847 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 16:24:31,847 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 16:24:31,847 WARN  org.apache.kafka.clients.admin.AdminClientConfig             [] - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2022-06-28 16:24:31,847 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 16:24:31,847 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 16:24:31,847 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656404671847
2022-06-28 16:24:31,848 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Starting the KafkaSourceEnumerator for consumer group g5 without periodic partition discovery.
2022-06-28 16:24:31,849 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12)#0 (4f2597239d67e71ae1eb6acfe3a18330), deploy into slot with allocation id 9a215efc8d17dac9ed3d0e7f9cd220fe.
2022-06-28 16:24:31,849 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 227c5886f37850089b70e1aeff4b3a2f.
2022-06-28 16:24:31,849 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12)#0 (4f2597239d67e71ae1eb6acfe3a18330) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,849 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12)#0 (4f2597239d67e71ae1eb6acfe3a18330) [DEPLOYING].
2022-06-28 16:24:31,851 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12)#0 (9c1982217f6cef9722a482a9d4df7559), deploy into slot with allocation id 227c5886f37850089b70e1aeff4b3a2f.
2022-06-28 16:24:31,852 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 487c4704c07dbffd23f9c0fbe0135f10.
2022-06-28 16:24:31,854 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12)#0 (9c1982217f6cef9722a482a9d4df7559) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,854 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12)#0 (9c1982217f6cef9722a482a9d4df7559) [DEPLOYING].
2022-06-28 16:24:31,854 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12)#0 (f6e204231635a2774929e436d844695a), deploy into slot with allocation id 487c4704c07dbffd23f9c0fbe0135f10.
2022-06-28 16:24:31,855 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ec66a1eb12045d0a4dd4b45fdfd384a8.
2022-06-28 16:24:31,855 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12)#0 (f6e204231635a2774929e436d844695a) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,856 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12)#0 (f6e204231635a2774929e436d844695a) [DEPLOYING].
2022-06-28 16:24:31,857 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12)#0 (432399431dc5864ed0f4d341792fcf03), deploy into slot with allocation id ec66a1eb12045d0a4dd4b45fdfd384a8.
2022-06-28 16:24:31,858 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12)#0 (432399431dc5864ed0f4d341792fcf03) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,858 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12)#0 (432399431dc5864ed0f4d341792fcf03) [DEPLOYING].
2022-06-28 16:24:31,858 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 21e3aac63be84eb5a84a56b3faef9b43.
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7759a7b6
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@186d18fb
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5bd3e74
2022-06-28 16:24:31,861 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,861 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@66a8e818
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@bcc7353
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4be6ed3
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,861 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,861 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@24892405
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@22a63047
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3db02fed
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,862 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,862 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,862 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,862 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,862 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,862 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@3cbb5785
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@b2d6042
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,862 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,861 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,862 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,861 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,862 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,862 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,861 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@46e4ce07
2022-06-28 16:24:31,862 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,862 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,862 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,862 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12)#0 (821a8e33843e2af6f3ae1df52f9c0b0b) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12)#0 (432399431dc5864ed0f4d341792fcf03) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12)#0 (c8fc8ec5f9b94db439c554c7fe071420) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12)#0 (4197ea5df276dbab325581874ddd596e) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12)#0 (4f2597239d67e71ae1eb6acfe3a18330) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12)#0 (dc5dc3be7a7f9dce75c73fb88bcc3e1a) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12)#0 (9c1982217f6cef9722a482a9d4df7559) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12)#0 (49db214fc16fe3b61149b0f9e1d925c3) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12)#0 (25e5646394dc050bfe8cc1aa1ab4efb1) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12)#0 (64ef5ee9cfd181e16e4ca54910c4ca7e) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12)#0 (f6e204231635a2774929e436d844695a) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,868 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12)#0 (ad949c2e86d937b27f9579995ab4ebfa) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,869 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12) (4f2597239d67e71ae1eb6acfe3a18330) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,870 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Sink: Collect table sink (1/1)#0 (c2467235cdf7036ab8e9ebccc22cfc6d), deploy into slot with allocation id 21e3aac63be84eb5a84a56b3faef9b43.
2022-06-28 16:24:31,870 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12) (821a8e33843e2af6f3ae1df52f9c0b0b) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,870 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12) (c8fc8ec5f9b94db439c554c7fe071420) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,870 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 227c5886f37850089b70e1aeff4b3a2f.
2022-06-28 16:24:31,870 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Sink: Collect table sink (1/1)#0 (c2467235cdf7036ab8e9ebccc22cfc6d) switched from CREATED to DEPLOYING.
2022-06-28 16:24:31,870 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 2b0c2fcce96af604059dbaf16346aab4.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12) (4197ea5df276dbab325581874ddd596e) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 6ddfe7213a7402ac97afc9d0375c70a4.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 9b750511271ca84fb4c66527879a816e.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Sink: Collect table sink (1/1)#0 (c2467235cdf7036ab8e9ebccc22cfc6d) [DEPLOYING].
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12) (432399431dc5864ed0f4d341792fcf03) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 801a92362500bfcd66a896e45576c0a8.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot ec66a1eb12045d0a4dd4b45fdfd384a8.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 487c4704c07dbffd23f9c0fbe0135f10.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12) (dc5dc3be7a7f9dce75c73fb88bcc3e1a) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 0464bd5bfa800255f0202873fbe42337.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 5194d150041bccae279fff3155ff610b.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot be8026bf22cc19a18729113e58ddc6d4.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12) (9c1982217f6cef9722a482a9d4df7559) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 21e3aac63be84eb5a84a56b3faef9b43.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 9a215efc8d17dac9ed3d0e7f9cd220fe.
2022-06-28 16:24:31,871 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12) (49db214fc16fe3b61149b0f9e1d925c3) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,872 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12) (25e5646394dc050bfe8cc1aa1ab4efb1) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,872 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12) (64ef5ee9cfd181e16e4ca54910c4ca7e) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,872 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12) (f6e204231635a2774929e436d844695a) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,872 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12) (ad949c2e86d937b27f9579995ab4ebfa) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,872 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@119e03f2
2022-06-28 16:24:31,873 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-06-28 16:24:31,873 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
2022-06-28 16:24:31,873 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Sink: Collect table sink (1/1)#0 (c2467235cdf7036ab8e9ebccc22cfc6d) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,873 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Collect table sink (1/1) (c2467235cdf7036ab8e9ebccc22cfc6d) switched from DEPLOYING to INITIALIZING.
2022-06-28 16:24:31,927 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,926 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,927 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,927 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,926 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,927 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,926 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,926 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,927 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,927 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,926 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,926 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) exceeded the 80 characters length limit and was truncated.
2022-06-28 16:24:31,977 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 2 @ 
2022-06-28 16:24:31,980 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 1 @ 
2022-06-28 16:24:31,980 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 7 @ 
2022-06-28 16:24:31,980 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 6 @ 
2022-06-28 16:24:31,980 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 4 @ 
2022-06-28 16:24:31,980 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 9 @ 
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 0 @ 
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 5 @ 
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 3 @ 
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 11 @ 
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 10 @ 
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: KafkaSource-default_catalog.default_database.t_kafka_exercise registering reader for parallel task 8 @ 
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12)#0 (ad949c2e86d937b27f9579995ab4ebfa) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12)#0 (49db214fc16fe3b61149b0f9e1d925c3) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12)#0 (432399431dc5864ed0f4d341792fcf03) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12)#0 (25e5646394dc050bfe8cc1aa1ab4efb1) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12)#0 (821a8e33843e2af6f3ae1df52f9c0b0b) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12)#0 (64ef5ee9cfd181e16e4ca54910c4ca7e) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12)#0 (f6e204231635a2774929e436d844695a) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12)#0 (4f2597239d67e71ae1eb6acfe3a18330) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12)#0 (c8fc8ec5f9b94db439c554c7fe071420) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,982 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12)#0 (4197ea5df276dbab325581874ddd596e) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,981 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12)#0 (9c1982217f6cef9722a482a9d4df7559) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,982 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12)#0 (dc5dc3be7a7f9dce75c73fb88bcc3e1a) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,982 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (7/12) (ad949c2e86d937b27f9579995ab4ebfa) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,982 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (12/12) (432399431dc5864ed0f4d341792fcf03) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,983 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (4/12) (25e5646394dc050bfe8cc1aa1ab4efb1) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,983 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (3/12) (49db214fc16fe3b61149b0f9e1d925c3) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,983 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (8/12) (821a8e33843e2af6f3ae1df52f9c0b0b) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,984 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (1/12) (64ef5ee9cfd181e16e4ca54910c4ca7e) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,985 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (11/12) (f6e204231635a2774929e436d844695a) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,986 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (9/12) (4f2597239d67e71ae1eb6acfe3a18330) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,987 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (6/12) (c8fc8ec5f9b94db439c554c7fe071420) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,987 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (2/12) (4197ea5df276dbab325581874ddd596e) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,987 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (10/12) (9c1982217f6cef9722a482a9d4df7559) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,987 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: KafkaSource-default_catalog.default_database.t_kafka_exercise -> Calc(select=[id, name, nick, age, id AS guid, (age + 10) AS big_age, CAST(offset) AS xxxoffset, CAST(timestamp) AS ts, gender]) (5/12) (dc5dc3be7a7f9dce75c73fb88bcc3e1a) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:31,991 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkFunction [] - Initializing collect sink state with offset = 0, buffered results bytes = 0
2022-06-28 16:24:31,993 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkFunction [] - Collect sink server established, address = localhost/127.0.0.1:53614
2022-06-28 16:24:31,997 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator [] - Received sink socket server address: localhost/127.0.0.1:53614
2022-06-28 16:24:32,002 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Sink: Collect table sink (1/1)#0 (c2467235cdf7036ab8e9ebccc22cfc6d) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:32,002 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Collect table sink (1/1) (c2467235cdf7036ab8e9ebccc22cfc6d) switched from INITIALIZING to RUNNING.
2022-06-28 16:24:32,064 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkFunction [] - Coordinator connection received
2022-06-28 16:24:32,064 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator [] - Sink connection established
2022-06-28 16:24:32,066 INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkFunction [] - Invalid request. Received version = , offset = 0, while expected version = 244728ea-d5f1-4fd7-b306-aa1fa4e8a36a, offset = 0
2022-06-28 16:24:32,140 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Discovered new partitions: [flink-sql-exercise-demo2-0]
2022-06-28 16:24:32,143 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Assigning splits to readers {6=[[Partition: flink-sql-exercise-demo2-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
2022-06-28 16:24:32,146 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [[Partition: flink-sql-exercise-demo2-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
2022-06-28 16:24:32,148 INFO  org.apache.kafka.clients.consumer.ConsumerConfig             [] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.136.130:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = g5-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = g5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2022-06-28 16:24:32,153 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
2022-06-28 16:24:32,153 WARN  org.apache.kafka.clients.consumer.ConsumerConfig             [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
2022-06-28 16:24:32,154 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka version: 2.4.1
2022-06-28 16:24:32,154 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka commitId: c57222ae8cd7866b
2022-06-28 16:24:32,154 INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - Kafka startTimeMs: 1656404672153
2022-06-28 16:24:32,161 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
2022-06-28 16:24:32,167 INFO  org.apache.kafka.clients.consumer.KafkaConsumer              [] - [Consumer clientId=g5-6, groupId=g5] Subscribed to partition(s): flink-sql-exercise-demo2-0
2022-06-28 16:24:32,171 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-6, groupId=g5] Seeking to EARLIEST offset of partition flink-sql-exercise-demo2-0
2022-06-28 16:24:32,189 INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=g5-6, groupId=g5] Cluster ID: e4SJu7z7Q3akarjUt4ikvg
2022-06-28 16:24:32,201 INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=g5-6, groupId=g5] Resetting offset for partition flink-sql-exercise-demo2-0 to offset 0.
2022-06-28 16:27:21,683 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2022-06-28 16:27:21,682 INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager [] - Shutting down TaskExecutorStateChangelogStoragesManager.
2022-06-28 16:27:21,682 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2022-06-28 16:27:21,682 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2022-06-28 17:52:44,160 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: java.io.IOException: Failed to deserialize JSON '{"id":10,"name":{"nick":"tiedan1","formal":"doit edu1},"age":18,"gender":"male"}'.
	at org.apache.flink.formats.json.JsonRowDataDeserializationSchema.deserialize(JsonRowDataDeserializationSchema.java:112) ~[flink-json-1.14.4.jar:1.14.4]
	at org.apache.flink.formats.json.JsonRowDataDeserializationSchema.deserialize(JsonRowDataDeserializationSchema.java:50) ~[flink-json-1.14.4.jar:1.14.4]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-core-1.14.4.jar:1.14.4]
	at org.apache.flink.table.filesystem.DeserializationSchemaAdapter$LineBytesInputFormat.readRecord(DeserializationSchemaAdapter.java:277) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.filesystem.DeserializationSchemaAdapter$LineBytesInputFormat.nextRecord(DeserializationSchemaAdapter.java:302) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.filesystem.DeserializationSchemaAdapter$Reader.readBatch(DeserializationSchemaAdapter.java:187) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67) ~[flink-connector-files-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParseException: Unexpected character ('a' (code 97)): was expecting comma to separate Object entries
 at [Source: (byte[])"{"id":10,"name":{"nick":"tiedan1","formal":"doit edu1},"age":18,"gender":"male"}"; line: 1, column: 58]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2337) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:710) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:635) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextFieldName(UTF8StreamJsonParser.java:1024) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:269) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:277) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:69) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:16) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._readTreeAndClose(ObjectMapper.java:4635) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:3056) ~[flink-shaded-jackson-2.12.4-14.0.jar:2.12.4-14.0]
	at org.apache.flink.formats.json.JsonRowDataDeserializationSchema.deserializeToJsonNode(JsonRowDataDeserializationSchema.java:117) ~[flink-json-1.14.4.jar:1.14.4]
	at org.apache.flink.formats.json.JsonRowDataDeserializationSchema.deserialize(JsonRowDataDeserializationSchema.java:106) ~[flink-json-1.14.4.jar:1.14.4]
	at org.apache.flink.formats.json.JsonRowDataDeserializationSchema.deserialize(JsonRowDataDeserializationSchema.java:50) ~[flink-json-1.14.4.jar:1.14.4]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82) ~[flink-core-1.14.4.jar:1.14.4]
	at org.apache.flink.table.filesystem.DeserializationSchemaAdapter$LineBytesInputFormat.readRecord(DeserializationSchemaAdapter.java:277) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.filesystem.DeserializationSchemaAdapter$LineBytesInputFormat.nextRecord(DeserializationSchemaAdapter.java:302) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.filesystem.DeserializationSchemaAdapter$Reader.readBatch(DeserializationSchemaAdapter.java:187) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67) ~[flink-connector-files-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
2022-07-04 09:36:36,597 ERROR org.apache.flink.table.factories.FactoryUtil                 [] - Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.formats.raw.RawFormatFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_221]
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_221]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_221]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_221]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.lookupExecutor(StreamTableEnvironmentImpl.java:176) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:148) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:128) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:96) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at cn.flinkSql.train.FlinkSqlDemo_MsqlcdcConnector.main(FlinkSqlDemo_MsqlcdcConnector.java:14) ~[classes/:?]
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/collect/Sets
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.Sets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_221]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_221]
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
2022-07-04 09:39:18,184 ERROR org.apache.flink.table.factories.FactoryUtil                 [] - Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.formats.raw.RawFormatFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_221]
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_221]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_221]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_221]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.lookupExecutor(StreamTableEnvironmentImpl.java:176) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:148) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:128) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:96) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at cn.flinkSql.train.FlinkSqlDemo_MsqlcdcConnector.main(FlinkSqlDemo_MsqlcdcConnector.java:14) ~[classes/:?]
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/collect/Sets
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.Sets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_221]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_221]
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
2022-07-04 09:41:38,158 ERROR org.apache.flink.table.factories.FactoryUtil                 [] - Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.formats.raw.RawFormatFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_221]
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_221]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_221]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_221]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.lookupExecutor(StreamTableEnvironmentImpl.java:176) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:148) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:128) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:96) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at cn.flinkSql.train.FlinkSqlDemo_MsqlcdcConnector.main(FlinkSqlDemo_MsqlcdcConnector.java:14) ~[classes/:?]
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/collect/Sets
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.Sets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_221]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_221]
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
2022-07-04 09:43:07,610 ERROR org.apache.flink.table.factories.FactoryUtil                 [] - Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.formats.raw.RawFormatFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_221]
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_221]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_221]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_221]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.lookupExecutor(StreamTableEnvironmentImpl.java:176) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:148) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:128) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:96) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at cn.flinkSql.train.FlinkSqlDemo_MsqlcdcConnector.main(FlinkSqlDemo_MsqlcdcConnector.java:14) ~[classes/:?]
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/collect/Sets
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.Sets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_221]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_221]
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
2022-07-04 09:43:38,607 ERROR org.apache.flink.table.factories.FactoryUtil                 [] - Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.formats.raw.RawFormatFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_221]
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_221]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_221]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_221]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.lookupExecutor(StreamTableEnvironmentImpl.java:176) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:148) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:128) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:96) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at cn.flinkSql.train.FlinkSqlDemo_JsonFormat.main(FlinkSqlDemo_JsonFormat.java:14) ~[classes/:?]
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/collect/Sets
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.Sets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_221]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_221]
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
2022-07-04 09:46:55,041 ERROR org.apache.flink.table.factories.FactoryUtil                 [] - Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.formats.raw.RawFormatFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_221]
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_221]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_221]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_221]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.lookupExecutor(StreamTableEnvironmentImpl.java:176) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:148) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:128) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:96) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at cn.flinkSql.train.FlinkSqlDemo_JDBCConnector.main(FlinkSqlDemo_JDBCConnector.java:14) ~[classes/:?]
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/collect/Sets
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.Sets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_221]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_221]
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
2022-07-04 09:50:34,552 ERROR org.apache.flink.table.factories.FactoryUtil                 [] - Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.formats.raw.RawFormatFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_221]
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_221]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_221]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_221]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.lookupExecutor(StreamTableEnvironmentImpl.java:176) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:148) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:128) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:96) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at cn.flinkSql.train.FlinkSqlDemo_JsonFormat.main(FlinkSqlDemo_JsonFormat.java:14) ~[classes/:?]
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/collect/Sets
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.Sets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_221]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_221]
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
2022-07-04 09:57:04,488 ERROR org.apache.flink.table.factories.FactoryUtil                 [] - Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.formats.raw.RawFormatFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_221]
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_221]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_221]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_221]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.lookupExecutor(StreamTableEnvironmentImpl.java:176) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:148) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:128) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:96) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at cn.flinkSql.train.FlinkSqlDemo_MsqlcdcConnector.main(FlinkSqlDemo_MsqlcdcConnector.java:14) ~[classes/:?]
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/collect/Sets
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.Sets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_221]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_221]
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
2022-07-04 10:20:23,701 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	... 6 more
2022-07-04 10:20:24,672 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-bonc.jar:2.3-bonc]
	... 5 more
2022-07-04 10:34:24,277 ERROR org.apache.flink.table.factories.FactoryUtil                 [] - Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.formats.raw.RawFormatFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_221]
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_221]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_221]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_221]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378) ~[flink-table-common-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.lookupExecutor(StreamTableEnvironmentImpl.java:176) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:148) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:128) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:96) ~[flink-table-api-java-bridge_2.12-1.14.4.jar:1.14.4]
	at cn.flinkSql.train.FlinkSqlDemo_MsqlcdcConnector.main(FlinkSqlDemo_MsqlcdcConnector.java:14) ~[classes/:?]
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/collect/Sets
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.Sets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_221]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_221]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_221]
	at org.apache.flink.formats.raw.RawFormatFactory.<clinit>(RawFormatFactory.java:130) ~[flink-table-runtime_2.12-1.14.4.jar:1.14.4]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_221]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_221]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_221]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_221]
	at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_221]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_221]
	... 10 more
2022-07-04 10:49:45,963 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:49:46,935 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:26,386 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:27,354 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:28,459 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:29,459 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:30,514 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:31,013 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:32,061 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:32,560 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:33,598 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:34,600 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:35,632 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:36,131 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:37,179 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:37,681 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:38,740 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:39,741 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:40,774 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:41,775 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:42,804 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:43,303 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:44,350 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:45,349 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:46,383 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:46,885 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:47,918 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:48,919 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:49,948 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:50,449 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:51,483 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:51,983 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:53,015 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:54,027 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:55,053 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:55,556 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:56,589 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:57,090 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 10:58:58,156 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 10:58:58,657 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 11:00:47,353 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 11:00:48,326 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 11:00:49,421 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 11:00:49,920 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 11:00:50,966 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 11:00:51,468 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 11:00:52,508 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 11:00:53,510 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 11:03:16,576 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 11:03:17,539 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 11:03:18,624 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 11:10:19,519 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
2022-07-04 11:10:20,492 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Read split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} error due to org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured.
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.checkReadException(SnapshotSplitReader.java:258) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.pollSplitRecords(SnapshotSplitReader.java:235) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:78) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) ~[flink-connector-base-1.14.4.jar:1.14.4]
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-base-1.14.4.jar:1.14.4]
	... 6 more
Caused by: io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 5 more
2022-07-04 11:10:21,588 ERROR com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader [] - Execute snapshot read task for mysql split MySqlSnapshotSplit{tableId=flink.scorce, splitId='flink.scorce:0', splitKeyType=[`id` INT NOT NULL], splitStart=null, splitEnd=null, highWatermark=null} fail
io.debezium.DebeziumException: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:119) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$0(SnapshotSplitReader.java:116) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_221]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot read the binlog filename and position via 'SHOW MASTER STATUS'. Make sure your server is correctly configured
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
Caused by: java.sql.SQLSyntaxErrorException: Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1206) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:648) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at io.debezium.jdbc.JdbcConnection.queryAndMap(JdbcConnection.java:517) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.currentBinlogOffset(DebeziumUtils.java:104) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.doExecute(MySqlSnapshotSplitReadTask.java:142) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.execute(MySqlSnapshotSplitReadTask.java:114) ~[flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar:2.3-SNAPSHOT]
	... 6 more
